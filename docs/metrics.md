# Metrics

On this page, we'll describe the various metrics that you can calculate using Velour. We'd love to hear your suggestions on new metrics that we should support: please [write us a GitHub Issue ticket](https://github.com/Striveworks/velour/issues) if we're missing any important metrics for your particular use case.

## Classification Metrics
| Name 	| Description 	|
|---	|---	|
| Precision 	| The number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives). 	|
| Recall 	| The number of true positives divided by the total count of the class of interest (i.e., the number of true positives plus the number of true negatives). 	|
| F1 	| A weighted average of precision and recall, calculated as `(2 * Precision * Recall)/(Precision + Recall)`. 	|
| Accuracy 	| The number of true positives divided by the total number of predictions. 	|
| ROC AUC 	| The area under the Receiver Operating Characteristic curve (ROC) for the predictions generated by a given model. 	|

## Object Detection & Instance Segmentation Metrics

| Name 	| Description 	|
|---	|---	|
| Average Precision (AP) 	| The weighted mean of precisions achieved at several different recall thresholds for a single IOU*, grouped by class. |
| AP Averaged Over Intersection over Unions (IOUs) 	| The average of several AP metrics calculated at various IOUs, grouped by class. 	|
| Mean Average Precision (mAP) 	| The mean of several AP scores calculated over various classes. 	|
| mAP Averaged Over Intersection over Unions (IOUs) 	| The mean of several averaged AP scores calculated over various classes. 	|

## Semantic Segmentation Metrics

| Name 	| Description 	|
|---	|---	|
| Intersection over Union (IOU) 	| The overlap between the groundtruth and predicted regions of an image, measured as a percentage, grouped by class. IOUs are calculated by a) fetching the groundtruth and prediction rasters for a particular image and class, b) counting the true positive pixels (e.g., the number of pixels that were selected in both the groundtruth masks and prediction masks), and c) dividing the sum of true positives by the total number pixels in both the groundtruth and prediction masks. |
| Mean IOU 	| The average of IOUs calculated over several different classes. 	|


\*Note that, when calculating IOUs for object detection metrics, Velour handles the necessary conversion between different types of image annotations. For example: if your model prediction is a polygon and your groundtruth is a raster, then the raster will be converted to a polygon prior to calculating the IOU.