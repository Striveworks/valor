# Metrics

Let's look at the various metrics you can calculate using Valor.

If we're missing an important metric for your particular use case, please [write us a GitHub Issue ticket](https://github.com/Striveworks/valor/issues). We love hearing your suggestions.

## Classification Metrics

| Name | Description | Equation |
|:- | :- | :- |
| Precision | The number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives). | $$\dfrac{\|TP\|}{\|TP\|+\|FP\|}$$ |
| Recall | The number of true positives divided by the total count of the class of interest (i.e., the number of true positives plus the number of true negatives). | $$\dfrac{\|TP\|}{\|TP\|+\|FN\|}$$ |
| F1 | A weighted average of precision and recall. | $$\frac{2 * Precision * Recall}{Precision + Recall}$$ |
| Accuracy | The number of true predictions divided by the total number of predictions. | $$\dfrac{\|TP\|+\|TN\|}{\|TP\|+\|TN\|+\|FP\|+\|FN\|}$$ |
| ROC AUC | The area under the Receiver Operating Characteristic (ROC) curve for the predictions generated by a given model. | See [ROCAUC methods](methods.md#roc-auc-for-classification). |

## Object Detection and Instance Segmentation Metrics

| Name | Description | Equation |
| :- | :- | :- |
| Average Precision (AP) | The weighted mean of precisions achieved at several different recall thresholds for a single Intersection over Union (IoU)*, grouped by class. | See [AP methods](methods.md#average-precision-ap-for-object-detection). |
| AP Averaged Over IoUs | The average of several AP metrics, calculated at various IoUs, grouped by class. | $$\dfrac{1}{\|thresholds\|} \sum\limits_{iou \in thresholds} AP_{iou}$$ |
| Mean Average Precision (mAP) 	| The mean of several AP scores, calculated over various classes. | $$\dfrac{1}{\|classes\|} \sum\limits_{c \in classes} AP_{c}$$ |
| mAP Averaged Over IoUs | The mean of several averaged AP scores, calculated over various classes. | $$\dfrac{1}{\|thresholds\|} \sum\limits_{iou \in thresholds} mAP_{iou}$$ |

## Semantic Segmentation Metrics

| Name | Description | Equation |
| :- | :- | :- |
| Intersection Over Union (IoU) | A ratio between the groundtruth and predicted regions of an image, measured as a percentage, grouped by class. |$$\dfrac{area( prediction \cap groundtruth )}{area( prediction \cup groundtruth )}$$ |
| Mean IoU 	| The average of IoUs, calculated over several different classes. | $$\dfrac{1}{\|classes\|} \sum\limits_{c \in classes} IoU_{c}$$ |
