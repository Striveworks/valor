# Metrics

On this page, we'll describe the various metrics that you can calculate using Velour. We'd love to hear your suggestions on new metrics that we should support: please [write us a GitHub Issue ticket](https://github.com/Striveworks/velour/issues) if we're missing any important metrics for your particular use case.

## Classification Metrics
| Name 	| Description 	|
|---	|---	|
| Precision 	| The number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives). 	|
| Recall 	| The number of true positives divided by the total count of the class of interest (i.e., the number of true positives plus the number of true negatives). 	|
| F1 	| A weighted average of precision and recall, calculated as `(2 * Precision * Recall)/(Precision + Recall)`. 	|
| Accuracy 	| The number of true positives divided by the total number of predictions. 	|
| ROC AUC 	| The area under the Receiver Operating Characteristic curve (ROC) for the predictions generated by a given model. 	|

## Detection Metrics

| Name 	| Description 	|
|---	|---	|
| Average Precision (AP) 	| The weighted mean of precisions achieved at several different recall thresholds for a single IOU. 	|
| AP Averaged Over Intersection over Unions (IOUs) 	| The average of several AP metrics calculated at various IOUs. 	|
| Mean Average Precision (mAP) 	| The mean of several AP scores calculated over various classes. 	|
| mAP Averaged Over Intersection over Unions (IOUs) 	| The mean of several averaged AP scores calculated over various classes. 	|

## Segmentation Metrics

| Name 	| Description 	|
|---	|---	|
| Intersection over Union (IOU) 	| The overlap between the groundtruth and predicted regions of an image, measured as a percentage, for a given label.  	|
| Mean IOU 	| The average of IOUs calculated over several different classes. 	|