# Metrics

Let's look at the various metrics you can calculate using Valor.

If we're missing an important metric for your particular use case, please [write us a GitHub Issue ticket](https://github.com/Striveworks/valor/issues). We love hearing your suggestions.


## Classification Metrics
| Name 	| Description 	|
|---	|---	|
| Precision 	| The number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives). 	|
| Recall 	| The number of true positives divided by the total count of the class of interest (i.e., the number of true positives plus the number of true negatives). 	|
| F1 	| A weighted average of precision and recall, calculated as `(2 * Precision * Recall)/(Precision + Recall)`. 	|
| Accuracy 	| The number of true positives divided by the total number of predictions. 	|
| ROC AUC 	| The area under the Receiver Operating Characteristic (ROC) curve for the predictions generated by a given model. 	|

## Object Detection and Instance Segmentation Metrics

| Name 	| Description 	|
|---	|---	|
| Average Precision (AP) 	| The weighted mean of precisions achieved at several different recall thresholds for a single Intersection over Union (IOU)*, grouped by class. |
| AP Averaged Over IOUs 	| The average of several AP metrics, calculated at various IOUs, grouped by class. 	|
| Mean Average Precision (mAP) 	| The mean of several AP scores, calculated over various classes.	|
| mAP Averaged Over IOUs 	| The mean of several averaged AP scores, calculated over various classes. 	|

## Semantic Segmentation Metrics

| Name 	| Description 	|
|---	|---	|
| Intersection Over Union (IOU) 	| The overlap between the ground truth and predicted regions of an image, measured as a percentage, grouped by class. IOUs are calculated by a) fetching the ground truth and prediction rasters for a particular image and class, b) counting the true positive pixels (e.g., the number of pixels that were selected in both the ground truth masks and prediction masks), and c) dividing the sum of true positives by the total number of pixels in both the ground truth and prediction masks. |
| Mean IOU 	| The average of IOUs, calculated over several different classes. 	|

\*When calculating IOUs for object detection metrics, Valor handles the necessary conversion between different types of image annotations. For example, if your model prediction is a polygon and your ground truth is a raster, then the raster will be converted to a polygon prior to calculating the IOU.