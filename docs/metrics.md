# Metrics

Let's look at the various metrics you can calculate using Valor.

If we're missing an important metric for your particular use case, please [write us a GitHub Issue ticket](https://github.com/Striveworks/valor/issues). We love hearing your suggestions.

## Classification Metrics

| Name | Description | Equation |
|:- | :- | :- |
| Precision | The number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives). | $$\dfrac{\|TP\|}{\|TP\|+\|FP\|}$$ |
| Recall | The number of true positives divided by the total count of the class of interest (i.e., the number of true positives plus the number of true negatives). | $$\dfrac{\|TP\|}{\|TP\|+\|FN\|}$$ |
| F1 | A weighted average of precision and recall. | $$\frac{2 * Precision * Recall}{Precision + Recall}$$ |
| Accuracy | The number of true predictions divided by the total number of predictions. | $$\dfrac{\|TP\|+\|TN\|}{\|TP\|+\|TN\|+\|FP\|+\|FN\|}$$ |
| ROC AUC | The area under the Receiver Operating Characteristic (ROC) curve for the predictions generated by a given model. | See [ROCAUC methods](#binary-roc-auc). |

## Object Detection and Instance Segmentation Metrics<sup>[1](#notes)</sup>

| Name | Description | Equation |
| :- | :- | :- |
| Average Precision (AP) | The weighted mean of precisions achieved at several different recall thresholds for a single Intersection over Union (IoU), grouped by class. | See [AP methods](#average-precision-ap). |
| AP Averaged Over IoUs | The average of several AP metrics, calculated at various IoUs, grouped by class. | $$\dfrac{1}{\|thresholds\|} \sum\limits_{iou \in thresholds} AP_{iou}$$ |
| Mean Average Precision (mAP) 	| The mean of several AP scores, calculated over various classes. | $$\dfrac{1}{\|classes\|} \sum\limits_{c \in classes} AP_{c}$$ |
| mAP Averaged Over IoUs | The mean of several averaged AP scores, calculated over various classes. | $$\dfrac{1}{\|thresholds\|} \sum\limits_{iou \in thresholds} mAP_{iou}$$ |

## Semantic Segmentation Metrics

| Name | Description | Equation |
| :- | :- | :- |
| Intersection Over Union (IoU) | A ratio between the groundtruth and predicted regions of an image, measured as a percentage, grouped by class. |$$\dfrac{area( prediction \cap groundtruth )}{area( prediction \cup groundtruth )}$$ |
| Mean IoU 	| The average of IoUs, calculated over several different classes. | $$\dfrac{1}{\|classes\|} \sum\limits_{c \in classes} IoU_{c}$$ |

# Notes
1. When calculating IoUs for object detection metrics, Valor handles the necessary conversion between different types of geometric annotations. For example, if your model prediction is a polygon and your groundtruth is a raster, then the raster will be converted to a polygon prior to calculating the IoU.

# Appendix: Metric Calculations

## Binary ROC AUC

### Determining the rate of correct predictions.

| Element | Description |
| ------- | ------------ |
| True Positive (TP) | Prediction confidence score >= threshold and is correct. |
| False Positive (FP) | Prediction confidence score >= threshold and is incorrect. |
| True Negative (TN) | Prediction confidence score < threshold and is correct. |
| False Negative (FN) | Prediction confidence score < threshold and is incorrect. |

- $\text{True Positive Rate (TPR)} = \dfrac{|TP|}{|TP| + |FN|}$

- $\text{False Positive Rate (FPR)} = \dfrac{|FP|}{|FP| + |TN|}$

These rates vary depending on a threshold so we can add a threshold parameter to be more explicit.

$$
TPR(threshold) = \dfrac{|TP(threshold)|}{|TP(threshold)| + |FN(threshold)|}
$$

$$
FPR(threshold) = \dfrac{|FP(threshold)|}{|FP(threshold)| + |TN(threshold)|}
$$

### Receiver Operating Characteristic (ROC)

An ROC curve plots TPR vs. FPR at different confidence thresholds.

In Valor, we use the confidence scores sorted in decreasing order as our thresholds.

$$
Point(score) = (FPR(score), \ TPR(score))
$$

### Area under the ROC curve (ROC AUC)

From the ROC curve we calculate the ROC AUC metric by approximating the integral using the trapezoidal rule formula.

$$
ROC AUC =  \sum_{i=1}^{|scores|} \frac{  \lVert Point(score_{i-1}) - Point(score_i) \rVert }{2}
$$

### References
- [Classification: ROC Curve and AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

## Average Precision (AP)

For object-detection and instance segmentation tasks, average-precision is calculated from the intersection-over-union (IoU) of geometric annotations.

### Multiclass Precision and Recall

Geometric annotations use the ratio intersection-over-union (IoU) to calculate precision and recall. IoU is the ratio of intersecting area over the joint area spanned by the two geometries and is defined in the following equation.

$$Intersection \ over \ Union \ (IoU) = \dfrac{Area( prediction \cap groundtruth )}{Area( prediction \cup groundtruth )}$$

By setting a threshold we can use IoU to determine whether we count a prediction-groundtruth pairing based on their overlap.

| Case | Description |
| :- | :- |
| True Positive (TP) | Prediction-GroundTruth pair exists with IoU >= threshold. |
| False Positive (FP) | Prediction-GroundTruth pair exists with IoU < threshold. |
| True Negative (TN) | Unused in multi-class evaluation.
| False Negative (FN) | No Prediction exists for the GroundTruth. |

- $Precision = \dfrac{|TP|}{|TP| + |FP|} = \dfrac{\text{Number of True Predictions}}{|\text{Predictions}|}$

- $Recall = \dfrac{|TP|}{|TP| + |FN|} = \dfrac{\text{Number of True Predictions}}{|\text{Groundtruths}|}$

### Finding the best prediction for a groundtruth.

To properly evaluate a detection, we must first find the best matches of predictions to ground truths. We start by iterating over our predictions, ordering them by highest scores first. We pair each prediction with the ground truth that has the highest calculated IOU. Both the prediction and ground truth are now considered paired and removed from the pool of choices.

Note: For simplicity, the following algorithm assumes operation over the same datum and label.

$$
\begin{aligned}
&\underline{\large\textbf{Algorithm 1} \hspace{0.5em} \text{Rank IoU's}} \\
&\textbf{Data: }\text{Lists of groundtruths and predictions sharing the same label for a datum.} \\
&\textbf{Results: }\text{Ranked list of IoU's.} \\
&\textbf{Note: }\text{Assume }argsort\text{ algorithm sorts in descending order.} \\
\\
&1 \hspace{1.5em} groundtruths \gets \text{list of geometries} \\
&2 \hspace{1.5em} predictions \gets \text{list of geometries} \\
&3 \hspace{1.5em} scores \gets \text{list of prediction scores} \\
\\
&4 \hspace{1.5em} k \gets \text{length of } groundtruths\\
\\
&5 \hspace{1.5em} visited \gets \text{new empty set} \\
&6 \hspace{1.5em} ious \gets \text{new list of size }k\\
&7 \hspace{1.5em} ranked\_ious \gets \text{new empty list} \\
\\
&8 \hspace{1.5em} I \gets \textbf{argsort}(scores) \\
&9 \hspace{1.5em} \textbf{foreach }i\text{ in }I\textbf{ do} \\
&10 \hspace{1em} | \quad p \gets predictions[i] \\
&11 \hspace{1em} | \quad \textbf{for }j=1\text{ to }k\textbf{ do} \\
&12 \hspace{1em} | \quad | \quad g \gets groundtruths[j] \\
&13 \hspace{1em} | \quad | \quad ious[j] \gets IoU(p, g) \\
&14 \hspace{1em} | \quad \textbf{end} \\
&15 \hspace{1em} | \quad J \gets \textbf{argsort}(ious) \\
&16 \hspace{1em} | \quad \textbf{foreach }j\text{ in }J\textbf{ do} \\
&17 \hspace{1em} | \quad | \quad \textbf{if }j\text{ not in }visited\_groundtruths \textbf{ then} \\
&18 \hspace{1em} | \quad | \quad | \quad visited.add(j) \\
&19 \hspace{1em} | \quad | \quad | \quad ranked\_ious.append(ious[j]) \\
&20 \hspace{1em} | \quad | \quad | \quad \textbf{break} \\
&21 \hspace{1em} | \quad | \quad \textbf{end} \\
&22 \hspace{1em} | \quad \textbf{end} \\
&23 \hspace{1em} \textbf{end} \\
\end{aligned}
$$

### Precision-Recall Curve

We can now compute the precision-recall curve using our previously ranked IoU's. We do this by iterating through the ranked IoU's and creating points cumulatively using recall and precision.

$$
\begin{aligned}
&\underline{\large\textbf{Algorithm 2} \hspace{0.5em} \text{Precision-Recall Curve}} \\
&\textbf{Data: }\text{Ranked list of IoU's for a label and a IoU threshold between 0 and 1.} \\
&\textbf{Results: }\text{List of points on a curve.} \\
\\
&1 \hspace{1.5em} ranked\_ious \gets \text{list of IoU's} \\
&2 \hspace{1.5em} threshold \gets \text{IoU Threshold} \\
\\
&3 \hspace{1.5em} n \gets \text{total number of groundtruths.} \\
&4 \hspace{1.5em} count\_tp \gets 0 \\
\\
&5 \hspace{1.5em} curve \gets \text{new empty list} \\
\\
&6 \hspace{1.5em} \textbf{for }i=0\text{ to }n-1\textbf{ do} \\
&7 \hspace{1.5em} | \quad \textbf{if } ranked\_ious[i] \ge threshold \textbf{ then} \\
&8 \hspace{1.5em} | \quad | \quad count\_tp \gets count\_tp + 1 \\
&9 \hspace{1.5em} | \quad \textbf{end} \\
&10 \hspace{1em} | \quad precision \gets count\_tp \mathrel{{/}} (i+1) \\
&11 \hspace{1em} | \quad recall \gets count\_tp \mathrel{{/}} n \\
&12 \hspace{1em} | \quad curve.append((recall, precision)) \\
&13 \hspace{1em} \textbf{end} \\
\end{aligned}
$$

### Calculating Average Precision

Average precision is defined as the integration of the precision-recall curve. However, due to the varying nature of datasets it has been shown that interpolating this curve with a fixed set of points helps to reduce inconsistencies between dataset splits. The defacto standard has been to use a 101-point interpolation of the precision-recall curve to compute this integral.

$$
AP = \frac{1}{101} \sum\limits_{r\in\{ 0, 0.01, \ldots , 1 \}}\rho_{interp}(r)
$$

$$
\rho_{interp} = \underset{\tilde{r}:\tilde{r} \ge r}{max \ \rho (\tilde{r})}
$$

### References
- [MS COCO Detection Evaluation](https://cocodataset.org/#detection-eval)
- [Mean Average Precision (mAP) Using the COCO Evaluator](https://pyimagesearch.com/2022/05/02/mean-average-precision-map-using-the-coco-evaluator/)
