{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Valor is a collection of evaluation methods that make it easy to measure, explore, and rank machine learning model performance. Valor empowers data scientists and engineers to evaluate the performance of their machine learning pipelines and use those evaluations to make better modeling decisions in the future. To skip this textual introduction and dive right in, first go here for basic installation instructions, and then checkout the example notebooks.</p> <p>Valor is maintained by Striveworks, a cutting-edge machine learning operations (MLOps) company based out of Austin, Texas. We'd love to learn more about your interest in Valor and answer any questions you may have; please don't hesitate to reach out to us on Slack or GitHub.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#pypi","title":"PyPi","text":"<pre><code>pip install valor-lite\n</code></pre>"},{"location":"#source","title":"Source","text":"<pre><code>git clone https://github.com/Striveworks/valor.git\ncd valor\nmake install\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Documentation<ul> <li>Classification<ul> <li>Documentation</li> <li>Metrics</li> </ul> </li> <li>Object Detection<ul> <li>Documentation</li> <li>Metrics</li> </ul> </li> <li>Semantic Segmentation<ul> <li>Documentation</li> <li>Metrics</li> </ul> </li> <li>Text Generation<ul> <li>Documentation</li> <li>Metrics</li> </ul> </li> </ul> </li> <li>Example Notebooks: Collection of descriptive Jupyter notebooks giving examples of how to evaluate model performance using Valor.</li> <li>Contributing and Development: Explains how you can build on and contribute to Valor.</li> </ul>"},{"location":"contributing/","title":"Contibuting &amp; Development","text":""},{"location":"contributing/#contributing-to-valor","title":"Contributing to Valor","text":"<p>We welcome all contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas aimed at improving Valor. This doc describes the high-level process for how to contribute to this repository. If you have any questions or comments about this process, please feel free to reach out to us on Slack.</p>"},{"location":"contributing/#on-github","title":"On GitHub","text":"<p>We use Git on GitHub to manage this repo, which means you will need to sign up for a free GitHub account to submit issues, ideas, and pull requests. We use Git for version control to allow contributors from all over the world to work together on this project.</p> <p>If you are new to Git, these official resources can help bring you up to speed:</p> <ul> <li>GitHub documentation for forking a repo</li> <li>GitHub documentation for collaborating with pull requests</li> <li>GitHub documentation for working with forks</li> </ul>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":"<p>Generally, the high-level workflow for contributing to this repo includes:</p> <ol> <li>Submitting an issue or enhancement request using the appropriate template on GitHub Issues.</li> <li>Gathering feedback from devs and the broader community in your issue before starting to code.</li> <li>Forking the Valor repo, making your proposed changes, and submitting a pull request (PR). When submitting a PR, please be sure to:<ol> <li>Update the README.md and/or any relevant docstrings with details of your change.</li> <li>Add tests where necessary.</li> <li>Run <code>pre-commit install</code> on your local repo before your last commit to ensure your changes follow our formatting guidelines.</li> <li>Double-check that your code passes all of the tests that are automated via GitHub Actions.</li> <li>Ping us on Slack to ensure timely review.</li> </ol> </li> <li>Working with repo maintainers to review and improve your PR before it is merged into the official repo.</li> </ol> <p>For questions or comments on this process, please reach out to us at any time on Slack.</p>"},{"location":"contributing/#development","title":"Development","text":""},{"location":"contributing/#setting-up-your-environment","title":"Setting Up Your Environment","text":"<p>Creating a Valor-specific Python environment at the start of development can help you avoid dependency and versioning issues later on. To start, we'd recommend activating a new Python environment:</p> <pre><code># venv\npython3 -m venv .env-valor\nsource .env-valor/bin/activate\n\n# conda\nconda create --name valor python=3.10\nconda activate valor\n</code></pre> <p>Install the <code>valor-lite</code> module from source: <pre><code>make install\n</code></pre></p>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>All of our tests are run automatically via GitHub Actions on every push, so it's important to double-check that your code passes all local tests before committing your code.</p> <p>For linting and code formatting use: <pre><code>make pre-commit\n</code></pre></p> <p>Run all tests using: <pre><code>make test\n</code></pre></p>"},{"location":"classification/documentation/","title":"Documentation","text":"<p>Documentation</p>"},{"location":"classification/documentation/#valor_lite.classification.Classification","title":"<code>valor_lite.classification.Classification</code>  <code>dataclass</code>","text":"<p>Classification data structure containing a ground truth label and a list of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique identifier for the instance.</p> required <code>groundtruth</code> <code>str</code> <p>The true label for the instance.</p> required <code>predictions</code> <code>list of str</code> <p>List of predicted labels.</p> required <code>scores</code> <code>list of float</code> <p>Confidence scores corresponding to each predicted label.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; classification = Classification(\n...     uid='123',\n...     groundtruth='cat',\n...     predictions=['cat', 'dog', 'bird'],\n...     scores=[0.9, 0.05, 0.05]\n... )\n</code></pre> Source code in <code>valor_lite/classification/annotation.py</code> <pre><code>@dataclass\nclass Classification:\n    \"\"\"\n    Classification data structure containing a ground truth label and a list of predictions.\n\n    Parameters\n    ----------\n    uid : str\n        Unique identifier for the instance.\n    groundtruth : str\n        The true label for the instance.\n    predictions : list of str\n        List of predicted labels.\n    scores : list of float\n        Confidence scores corresponding to each predicted label.\n\n    Examples\n    --------\n    &gt;&gt;&gt; classification = Classification(\n    ...     uid='123',\n    ...     groundtruth='cat',\n    ...     predictions=['cat', 'dog', 'bird'],\n    ...     scores=[0.9, 0.05, 0.05]\n    ... )\n    \"\"\"\n\n    uid: str\n    groundtruth: str\n    predictions: list[str]\n    scores: list[float]\n\n    def __post_init__(self):\n        if not isinstance(self.groundtruth, str):\n            raise ValueError(\n                \"A classification must contain a single groundtruth.\"\n            )\n        if len(self.predictions) != len(self.scores):\n            raise ValueError(\"There must be a score per prediction label.\")\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.DataLoader","title":"<code>valor_lite.classification.DataLoader</code>","text":"<p>               Bases: <code>Evaluator</code></p> <p>Used for backwards compatibility as the Evaluator now handles ingestion.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>class DataLoader(Evaluator):\n    \"\"\"\n    Used for backwards compatibility as the Evaluator now handles ingestion.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator","title":"<code>valor_lite.classification.Evaluator</code>","text":"<p>Classification Evaluator</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>class Evaluator:\n    \"\"\"\n    Classification Evaluator\n    \"\"\"\n\n    def __init__(self):\n        # external references\n        self.datum_id_to_index: dict[str, int] = {}\n        self.label_to_index: dict[str, int] = {}\n\n        self.index_to_datum_id: list[str] = []\n        self.index_to_label: list[str] = []\n\n        # internal caches\n        self._detailed_pairs = np.array([])\n        self._label_metadata = np.array([], dtype=np.int32)\n        self._metadata = Metadata()\n\n    @property\n    def metadata(self) -&gt; Metadata:\n        return self._metadata\n\n    @property\n    def ignored_prediction_labels(self) -&gt; list[str]:\n        \"\"\"\n        Prediction labels that are not present in the ground truth set.\n        \"\"\"\n        glabels = set(np.where(self._label_metadata[:, 0] &gt; 0)[0])\n        plabels = set(np.where(self._label_metadata[:, 1] &gt; 0)[0])\n        return [\n            self.index_to_label[label_id] for label_id in (plabels - glabels)\n        ]\n\n    @property\n    def missing_prediction_labels(self) -&gt; list[str]:\n        \"\"\"\n        Ground truth labels that are not present in the prediction set.\n        \"\"\"\n        glabels = set(np.where(self._label_metadata[:, 0] &gt; 0)[0])\n        plabels = set(np.where(self._label_metadata[:, 1] &gt; 0)[0])\n        return [\n            self.index_to_label[label_id] for label_id in (glabels - plabels)\n        ]\n\n    def create_filter(\n        self,\n        datum_ids: list[str] | None = None,\n        labels: list[str] | None = None,\n    ) -&gt; Filter:\n        \"\"\"\n        Creates a filter object.\n\n        Parameters\n        ----------\n        datum_uids : list[str], optional\n            An optional list of string uids representing datums.\n        labels : list[str], optional\n            An optional list of labels.\n\n        Returns\n        -------\n        Filter\n            The filter object representing the input parameters.\n        \"\"\"\n        # create datum mask\n        n_pairs = self._detailed_pairs.shape[0]\n        datum_mask = np.ones(n_pairs, dtype=np.bool_)\n        if datum_ids is not None:\n            if not datum_ids:\n                warnings.warn(\"no valid filtered pairs\")\n                return Filter(\n                    datum_mask=np.zeros_like(datum_mask),\n                    valid_label_indices=None,\n                    metadata=Metadata(),\n                )\n            valid_datum_indices = np.array(\n                [self.datum_id_to_index[uid] for uid in datum_ids],\n                dtype=np.int32,\n            )\n            datum_mask = np.isin(\n                self._detailed_pairs[:, 0], valid_datum_indices\n            )\n\n        # collect valid label indices\n        valid_label_indices = None\n        if labels is not None:\n            if not labels:\n                warnings.warn(\"no valid filtered pairs\")\n                return Filter(\n                    datum_mask=datum_mask,\n                    valid_label_indices=np.array([], dtype=np.int32),\n                    metadata=Metadata(),\n                )\n            valid_label_indices = np.array(\n                [self.label_to_index[label] for label in labels] + [-1]\n            )\n\n        filtered_detailed_pairs, _ = filter_cache(\n            detailed_pairs=self._detailed_pairs,\n            datum_mask=datum_mask,\n            valid_label_indices=valid_label_indices,\n            n_labels=self.metadata.number_of_labels,\n        )\n\n        number_of_datums = (\n            len(datum_ids)\n            if datum_ids is not None\n            else self.metadata.number_of_datums\n        )\n\n        return Filter(\n            datum_mask=datum_mask,\n            valid_label_indices=valid_label_indices,\n            metadata=Metadata.create(\n                detailed_pairs=filtered_detailed_pairs,\n                number_of_datums=number_of_datums,\n                number_of_labels=self.metadata.number_of_labels,\n            ),\n        )\n\n    def filter(\n        self, filter_: Filter\n    ) -&gt; tuple[NDArray[np.float64], NDArray[np.int32]]:\n        \"\"\"\n        Performs filtering over the internal cache.\n\n        Parameters\n        ----------\n        filter_ : Filter\n            The filter object representation.\n\n        Returns\n        -------\n        NDArray[float64]\n            The filtered detailed pairs.\n        NDArray[int32]\n            The filtered label metadata.\n        \"\"\"\n        empty_datum_mask = not filter_.datum_mask.any()\n        empty_label_mask = (\n            filter_.valid_label_indices.size == 0\n            if filter_.valid_label_indices is not None\n            else False\n        )\n        if empty_datum_mask or empty_label_mask:\n            if empty_datum_mask:\n                warnings.warn(\"filter removes all datums\")\n            if empty_label_mask:\n                warnings.warn(\"filter removes all labels\")\n            return (\n                np.array([], dtype=np.float64),\n                np.zeros((self.metadata.number_of_labels, 2), dtype=np.int32),\n            )\n        return filter_cache(\n            detailed_pairs=self._detailed_pairs,\n            datum_mask=filter_.datum_mask,\n            valid_label_indices=filter_.valid_label_indices,\n            n_labels=self.metadata.number_of_labels,\n        )\n\n    def compute_precision_recall_rocauc(\n        self,\n        score_thresholds: list[float] = [0.0],\n        hardmax: bool = True,\n        filter_: Filter | None = None,\n    ) -&gt; dict[MetricType, list]:\n        \"\"\"\n        Performs an evaluation and returns metrics.\n\n        Parameters\n        ----------\n        score_thresholds : list[float]\n            A list of score thresholds to compute metrics over.\n        hardmax : bool\n            Toggles whether a hardmax is applied to predictions.\n        filter_ : Filter, optional\n            Applies a filter to the internal cache.\n\n        Returns\n        -------\n        dict[MetricType, list]\n            A dictionary mapping MetricType enumerations to lists of computed metrics.\n        \"\"\"\n        # apply filters\n        if filter_ is not None:\n            detailed_pairs, label_metadata = self.filter(filter_=filter_)\n            n_datums = filter_.metadata.number_of_datums\n        else:\n            detailed_pairs = self._detailed_pairs\n            label_metadata = self._label_metadata\n            n_datums = self.metadata.number_of_datums\n\n        results = compute_precision_recall_rocauc(\n            detailed_pairs=detailed_pairs,\n            label_metadata=label_metadata,\n            score_thresholds=np.array(score_thresholds),\n            hardmax=hardmax,\n            n_datums=n_datums,\n        )\n        return unpack_precision_recall_rocauc_into_metric_lists(\n            results=results,\n            score_thresholds=score_thresholds,\n            hardmax=hardmax,\n            label_metadata=label_metadata,\n            index_to_label=self.index_to_label,\n        )\n\n    def compute_confusion_matrix(\n        self,\n        score_thresholds: list[float] = [0.0],\n        hardmax: bool = True,\n        number_of_examples: int = 0,\n        filter_: Filter | None = None,\n    ) -&gt; list[Metric]:\n        \"\"\"\n        Computes a detailed confusion matrix..\n\n        Parameters\n        ----------\n        score_thresholds : list[float]\n            A list of score thresholds to compute metrics over.\n        hardmax : bool\n            Toggles whether a hardmax is applied to predictions.\n        number_of_examples : int, default=0\n            The number of examples to return per count.\n        filter_ : Filter, optional\n            Applies a filter to the internal cache.\n\n        Returns\n        -------\n        list[Metric]\n            A list of confusion matrices.\n        \"\"\"\n        # apply filters\n        if filter_ is not None:\n            detailed_pairs, label_metadata = self.filter(filter_=filter_)\n        else:\n            detailed_pairs = self._detailed_pairs\n            label_metadata = self._label_metadata\n\n        if detailed_pairs.size == 0:\n            return list()\n\n        results = compute_confusion_matrix(\n            detailed_pairs=detailed_pairs,\n            label_metadata=label_metadata,\n            score_thresholds=np.array(score_thresholds),\n            hardmax=hardmax,\n            n_examples=number_of_examples,\n        )\n        return unpack_confusion_matrix_into_metric_list(\n            results=results,\n            score_thresholds=score_thresholds,\n            number_of_examples=number_of_examples,\n            index_to_datum_id=self.index_to_datum_id,\n            index_to_label=self.index_to_label,\n        )\n\n    def evaluate(\n        self,\n        score_thresholds: list[float] = [0.0],\n        hardmax: bool = True,\n        number_of_examples: int = 0,\n        filter_: Filter | None = None,\n    ) -&gt; dict[MetricType, list[Metric]]:\n        \"\"\"\n        Computes a detailed confusion matrix..\n\n        Parameters\n        ----------\n        score_thresholds : list[float]\n            A list of score thresholds to compute metrics over.\n        hardmax : bool\n            Toggles whether a hardmax is applied to predictions.\n        number_of_examples : int, default=0\n            The number of examples to return per count.\n        filter_ : Filter, optional\n            Applies a filter to the internal cache.\n\n        Returns\n        -------\n        dict[MetricType, list[Metric]]\n            Lists of metrics organized by metric type.\n        \"\"\"\n        metrics = self.compute_precision_recall_rocauc(\n            score_thresholds=score_thresholds,\n            hardmax=hardmax,\n            filter_=filter_,\n        )\n        metrics[MetricType.ConfusionMatrix] = self.compute_confusion_matrix(\n            score_thresholds=score_thresholds,\n            hardmax=hardmax,\n            number_of_examples=number_of_examples,\n            filter_=filter_,\n        )\n        return metrics\n\n    def _add_datum(self, uid: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a datum to the cache.\n\n        Parameters\n        ----------\n        uid : str\n            The datum uid.\n\n        Returns\n        -------\n        int\n            The datum index.\n        \"\"\"\n        if uid not in self.datum_id_to_index:\n            index = len(self.datum_id_to_index)\n            self.datum_id_to_index[uid] = index\n            self.index_to_datum_id.append(uid)\n        return self.datum_id_to_index[uid]\n\n    def _add_label(self, label: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a label to the cache.\n\n        Parameters\n        ----------\n        label : str\n            A string representing a label.\n\n        Returns\n        -------\n        int\n            Label index.\n        \"\"\"\n        label_id = len(self.index_to_label)\n        if label not in self.label_to_index:\n            self.label_to_index[label] = label_id\n            self.index_to_label.append(label)\n            label_id += 1\n        return self.label_to_index[label]\n\n    def add_data(\n        self,\n        classifications: list[Classification],\n        show_progress: bool = False,\n    ):\n        \"\"\"\n        Adds classifications to the cache.\n\n        Parameters\n        ----------\n        classifications : list[Classification]\n            A list of Classification objects.\n        show_progress : bool, default=False\n            Toggle for tqdm progress bar.\n        \"\"\"\n\n        disable_tqdm = not show_progress\n        for classification in tqdm(classifications, disable=disable_tqdm):\n\n            if len(classification.predictions) == 0:\n                raise ValueError(\n                    \"Classifications must contain at least one prediction.\"\n                )\n\n            # update datum uid index\n            uid_index = self._add_datum(uid=classification.uid)\n\n            # cache labels and annotations\n            groundtruth = self._add_label(classification.groundtruth)\n\n            predictions = list()\n            for plabel, pscore in zip(\n                classification.predictions, classification.scores\n            ):\n                label_idx = self._add_label(plabel)\n                predictions.append(\n                    (\n                        label_idx,\n                        pscore,\n                    )\n                )\n\n            pairs = list()\n            scores = np.array([score for _, score in predictions])\n            max_score_idx = np.argmax(scores)\n\n            for idx, (plabel, score) in enumerate(predictions):\n                pairs.append(\n                    (\n                        float(uid_index),\n                        float(groundtruth),\n                        float(plabel),\n                        float(score),\n                        float(max_score_idx == idx),\n                    )\n                )\n\n            if self._detailed_pairs.size == 0:\n                self._detailed_pairs = np.array(pairs)\n            else:\n                self._detailed_pairs = np.concatenate(\n                    [\n                        self._detailed_pairs,\n                        np.array(pairs),\n                    ],\n                    axis=0,\n                )\n\n    def finalize(self):\n        \"\"\"\n        Performs data finalization and some preprocessing steps.\n\n        Returns\n        -------\n        Evaluator\n            A ready-to-use evaluator object.\n        \"\"\"\n        if self._detailed_pairs.size == 0:\n            self._label_metadata = np.array([], dtype=np.int32)\n            warnings.warn(\"evaluator is empty\")\n            return self\n\n        self._label_metadata = compute_label_metadata(\n            ids=self._detailed_pairs[:, :3].astype(np.int32),\n            n_labels=len(self.index_to_label),\n        )\n        indices = np.lexsort(\n            (\n                self._detailed_pairs[:, 1],  # ground truth\n                self._detailed_pairs[:, 2],  # prediction\n                -self._detailed_pairs[:, 3],  # score\n            )\n        )\n        self._detailed_pairs = self._detailed_pairs[indices]\n        self._metadata = Metadata.create(\n            detailed_pairs=self._detailed_pairs,\n            number_of_datums=len(self.index_to_datum_id),\n            number_of_labels=len(self.index_to_label),\n        )\n        return self\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.ignored_prediction_labels","title":"<code>ignored_prediction_labels</code>  <code>property</code>","text":"<p>Prediction labels that are not present in the ground truth set.</p>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.missing_prediction_labels","title":"<code>missing_prediction_labels</code>  <code>property</code>","text":"<p>Ground truth labels that are not present in the prediction set.</p>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.add_data","title":"<code>add_data(classifications, show_progress=False)</code>","text":"<p>Adds classifications to the cache.</p> <p>Parameters:</p> Name Type Description Default <code>classifications</code> <code>list[Classification]</code> <p>A list of Classification objects.</p> required <code>show_progress</code> <code>bool</code> <p>Toggle for tqdm progress bar.</p> <code>False</code> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def add_data(\n    self,\n    classifications: list[Classification],\n    show_progress: bool = False,\n):\n    \"\"\"\n    Adds classifications to the cache.\n\n    Parameters\n    ----------\n    classifications : list[Classification]\n        A list of Classification objects.\n    show_progress : bool, default=False\n        Toggle for tqdm progress bar.\n    \"\"\"\n\n    disable_tqdm = not show_progress\n    for classification in tqdm(classifications, disable=disable_tqdm):\n\n        if len(classification.predictions) == 0:\n            raise ValueError(\n                \"Classifications must contain at least one prediction.\"\n            )\n\n        # update datum uid index\n        uid_index = self._add_datum(uid=classification.uid)\n\n        # cache labels and annotations\n        groundtruth = self._add_label(classification.groundtruth)\n\n        predictions = list()\n        for plabel, pscore in zip(\n            classification.predictions, classification.scores\n        ):\n            label_idx = self._add_label(plabel)\n            predictions.append(\n                (\n                    label_idx,\n                    pscore,\n                )\n            )\n\n        pairs = list()\n        scores = np.array([score for _, score in predictions])\n        max_score_idx = np.argmax(scores)\n\n        for idx, (plabel, score) in enumerate(predictions):\n            pairs.append(\n                (\n                    float(uid_index),\n                    float(groundtruth),\n                    float(plabel),\n                    float(score),\n                    float(max_score_idx == idx),\n                )\n            )\n\n        if self._detailed_pairs.size == 0:\n            self._detailed_pairs = np.array(pairs)\n        else:\n            self._detailed_pairs = np.concatenate(\n                [\n                    self._detailed_pairs,\n                    np.array(pairs),\n                ],\n                axis=0,\n            )\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.compute_confusion_matrix","title":"<code>compute_confusion_matrix(score_thresholds=[0.0], hardmax=True, number_of_examples=0, filter_=None)</code>","text":"<p>Computes a detailed confusion matrix..</p> <p>Parameters:</p> Name Type Description Default <code>score_thresholds</code> <code>list[float]</code> <p>A list of score thresholds to compute metrics over.</p> <code>[0.0]</code> <code>hardmax</code> <code>bool</code> <p>Toggles whether a hardmax is applied to predictions.</p> <code>True</code> <code>number_of_examples</code> <code>int</code> <p>The number of examples to return per count.</p> <code>0</code> <code>filter_</code> <code>Filter</code> <p>Applies a filter to the internal cache.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Metric]</code> <p>A list of confusion matrices.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def compute_confusion_matrix(\n    self,\n    score_thresholds: list[float] = [0.0],\n    hardmax: bool = True,\n    number_of_examples: int = 0,\n    filter_: Filter | None = None,\n) -&gt; list[Metric]:\n    \"\"\"\n    Computes a detailed confusion matrix..\n\n    Parameters\n    ----------\n    score_thresholds : list[float]\n        A list of score thresholds to compute metrics over.\n    hardmax : bool\n        Toggles whether a hardmax is applied to predictions.\n    number_of_examples : int, default=0\n        The number of examples to return per count.\n    filter_ : Filter, optional\n        Applies a filter to the internal cache.\n\n    Returns\n    -------\n    list[Metric]\n        A list of confusion matrices.\n    \"\"\"\n    # apply filters\n    if filter_ is not None:\n        detailed_pairs, label_metadata = self.filter(filter_=filter_)\n    else:\n        detailed_pairs = self._detailed_pairs\n        label_metadata = self._label_metadata\n\n    if detailed_pairs.size == 0:\n        return list()\n\n    results = compute_confusion_matrix(\n        detailed_pairs=detailed_pairs,\n        label_metadata=label_metadata,\n        score_thresholds=np.array(score_thresholds),\n        hardmax=hardmax,\n        n_examples=number_of_examples,\n    )\n    return unpack_confusion_matrix_into_metric_list(\n        results=results,\n        score_thresholds=score_thresholds,\n        number_of_examples=number_of_examples,\n        index_to_datum_id=self.index_to_datum_id,\n        index_to_label=self.index_to_label,\n    )\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.compute_precision_recall_rocauc","title":"<code>compute_precision_recall_rocauc(score_thresholds=[0.0], hardmax=True, filter_=None)</code>","text":"<p>Performs an evaluation and returns metrics.</p> <p>Parameters:</p> Name Type Description Default <code>score_thresholds</code> <code>list[float]</code> <p>A list of score thresholds to compute metrics over.</p> <code>[0.0]</code> <code>hardmax</code> <code>bool</code> <p>Toggles whether a hardmax is applied to predictions.</p> <code>True</code> <code>filter_</code> <code>Filter</code> <p>Applies a filter to the internal cache.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[MetricType, list]</code> <p>A dictionary mapping MetricType enumerations to lists of computed metrics.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def compute_precision_recall_rocauc(\n    self,\n    score_thresholds: list[float] = [0.0],\n    hardmax: bool = True,\n    filter_: Filter | None = None,\n) -&gt; dict[MetricType, list]:\n    \"\"\"\n    Performs an evaluation and returns metrics.\n\n    Parameters\n    ----------\n    score_thresholds : list[float]\n        A list of score thresholds to compute metrics over.\n    hardmax : bool\n        Toggles whether a hardmax is applied to predictions.\n    filter_ : Filter, optional\n        Applies a filter to the internal cache.\n\n    Returns\n    -------\n    dict[MetricType, list]\n        A dictionary mapping MetricType enumerations to lists of computed metrics.\n    \"\"\"\n    # apply filters\n    if filter_ is not None:\n        detailed_pairs, label_metadata = self.filter(filter_=filter_)\n        n_datums = filter_.metadata.number_of_datums\n    else:\n        detailed_pairs = self._detailed_pairs\n        label_metadata = self._label_metadata\n        n_datums = self.metadata.number_of_datums\n\n    results = compute_precision_recall_rocauc(\n        detailed_pairs=detailed_pairs,\n        label_metadata=label_metadata,\n        score_thresholds=np.array(score_thresholds),\n        hardmax=hardmax,\n        n_datums=n_datums,\n    )\n    return unpack_precision_recall_rocauc_into_metric_lists(\n        results=results,\n        score_thresholds=score_thresholds,\n        hardmax=hardmax,\n        label_metadata=label_metadata,\n        index_to_label=self.index_to_label,\n    )\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.create_filter","title":"<code>create_filter(datum_ids=None, labels=None)</code>","text":"<p>Creates a filter object.</p> <p>Parameters:</p> Name Type Description Default <code>datum_uids</code> <code>list[str]</code> <p>An optional list of string uids representing datums.</p> required <code>labels</code> <code>list[str]</code> <p>An optional list of labels.</p> <code>None</code> <p>Returns:</p> Type Description <code>Filter</code> <p>The filter object representing the input parameters.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def create_filter(\n    self,\n    datum_ids: list[str] | None = None,\n    labels: list[str] | None = None,\n) -&gt; Filter:\n    \"\"\"\n    Creates a filter object.\n\n    Parameters\n    ----------\n    datum_uids : list[str], optional\n        An optional list of string uids representing datums.\n    labels : list[str], optional\n        An optional list of labels.\n\n    Returns\n    -------\n    Filter\n        The filter object representing the input parameters.\n    \"\"\"\n    # create datum mask\n    n_pairs = self._detailed_pairs.shape[0]\n    datum_mask = np.ones(n_pairs, dtype=np.bool_)\n    if datum_ids is not None:\n        if not datum_ids:\n            warnings.warn(\"no valid filtered pairs\")\n            return Filter(\n                datum_mask=np.zeros_like(datum_mask),\n                valid_label_indices=None,\n                metadata=Metadata(),\n            )\n        valid_datum_indices = np.array(\n            [self.datum_id_to_index[uid] for uid in datum_ids],\n            dtype=np.int32,\n        )\n        datum_mask = np.isin(\n            self._detailed_pairs[:, 0], valid_datum_indices\n        )\n\n    # collect valid label indices\n    valid_label_indices = None\n    if labels is not None:\n        if not labels:\n            warnings.warn(\"no valid filtered pairs\")\n            return Filter(\n                datum_mask=datum_mask,\n                valid_label_indices=np.array([], dtype=np.int32),\n                metadata=Metadata(),\n            )\n        valid_label_indices = np.array(\n            [self.label_to_index[label] for label in labels] + [-1]\n        )\n\n    filtered_detailed_pairs, _ = filter_cache(\n        detailed_pairs=self._detailed_pairs,\n        datum_mask=datum_mask,\n        valid_label_indices=valid_label_indices,\n        n_labels=self.metadata.number_of_labels,\n    )\n\n    number_of_datums = (\n        len(datum_ids)\n        if datum_ids is not None\n        else self.metadata.number_of_datums\n    )\n\n    return Filter(\n        datum_mask=datum_mask,\n        valid_label_indices=valid_label_indices,\n        metadata=Metadata.create(\n            detailed_pairs=filtered_detailed_pairs,\n            number_of_datums=number_of_datums,\n            number_of_labels=self.metadata.number_of_labels,\n        ),\n    )\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.evaluate","title":"<code>evaluate(score_thresholds=[0.0], hardmax=True, number_of_examples=0, filter_=None)</code>","text":"<p>Computes a detailed confusion matrix..</p> <p>Parameters:</p> Name Type Description Default <code>score_thresholds</code> <code>list[float]</code> <p>A list of score thresholds to compute metrics over.</p> <code>[0.0]</code> <code>hardmax</code> <code>bool</code> <p>Toggles whether a hardmax is applied to predictions.</p> <code>True</code> <code>number_of_examples</code> <code>int</code> <p>The number of examples to return per count.</p> <code>0</code> <code>filter_</code> <code>Filter</code> <p>Applies a filter to the internal cache.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[MetricType, list[Metric]]</code> <p>Lists of metrics organized by metric type.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def evaluate(\n    self,\n    score_thresholds: list[float] = [0.0],\n    hardmax: bool = True,\n    number_of_examples: int = 0,\n    filter_: Filter | None = None,\n) -&gt; dict[MetricType, list[Metric]]:\n    \"\"\"\n    Computes a detailed confusion matrix..\n\n    Parameters\n    ----------\n    score_thresholds : list[float]\n        A list of score thresholds to compute metrics over.\n    hardmax : bool\n        Toggles whether a hardmax is applied to predictions.\n    number_of_examples : int, default=0\n        The number of examples to return per count.\n    filter_ : Filter, optional\n        Applies a filter to the internal cache.\n\n    Returns\n    -------\n    dict[MetricType, list[Metric]]\n        Lists of metrics organized by metric type.\n    \"\"\"\n    metrics = self.compute_precision_recall_rocauc(\n        score_thresholds=score_thresholds,\n        hardmax=hardmax,\n        filter_=filter_,\n    )\n    metrics[MetricType.ConfusionMatrix] = self.compute_confusion_matrix(\n        score_thresholds=score_thresholds,\n        hardmax=hardmax,\n        number_of_examples=number_of_examples,\n        filter_=filter_,\n    )\n    return metrics\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.filter","title":"<code>filter(filter_)</code>","text":"<p>Performs filtering over the internal cache.</p> <p>Parameters:</p> Name Type Description Default <code>filter_</code> <code>Filter</code> <p>The filter object representation.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The filtered detailed pairs.</p> <code>NDArray[int32]</code> <p>The filtered label metadata.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def filter(\n    self, filter_: Filter\n) -&gt; tuple[NDArray[np.float64], NDArray[np.int32]]:\n    \"\"\"\n    Performs filtering over the internal cache.\n\n    Parameters\n    ----------\n    filter_ : Filter\n        The filter object representation.\n\n    Returns\n    -------\n    NDArray[float64]\n        The filtered detailed pairs.\n    NDArray[int32]\n        The filtered label metadata.\n    \"\"\"\n    empty_datum_mask = not filter_.datum_mask.any()\n    empty_label_mask = (\n        filter_.valid_label_indices.size == 0\n        if filter_.valid_label_indices is not None\n        else False\n    )\n    if empty_datum_mask or empty_label_mask:\n        if empty_datum_mask:\n            warnings.warn(\"filter removes all datums\")\n        if empty_label_mask:\n            warnings.warn(\"filter removes all labels\")\n        return (\n            np.array([], dtype=np.float64),\n            np.zeros((self.metadata.number_of_labels, 2), dtype=np.int32),\n        )\n    return filter_cache(\n        detailed_pairs=self._detailed_pairs,\n        datum_mask=filter_.datum_mask,\n        valid_label_indices=filter_.valid_label_indices,\n        n_labels=self.metadata.number_of_labels,\n    )\n</code></pre>"},{"location":"classification/documentation/#valor_lite.classification.Evaluator.finalize","title":"<code>finalize()</code>","text":"<p>Performs data finalization and some preprocessing steps.</p> <p>Returns:</p> Type Description <code>Evaluator</code> <p>A ready-to-use evaluator object.</p> Source code in <code>valor_lite/classification/manager.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Performs data finalization and some preprocessing steps.\n\n    Returns\n    -------\n    Evaluator\n        A ready-to-use evaluator object.\n    \"\"\"\n    if self._detailed_pairs.size == 0:\n        self._label_metadata = np.array([], dtype=np.int32)\n        warnings.warn(\"evaluator is empty\")\n        return self\n\n    self._label_metadata = compute_label_metadata(\n        ids=self._detailed_pairs[:, :3].astype(np.int32),\n        n_labels=len(self.index_to_label),\n    )\n    indices = np.lexsort(\n        (\n            self._detailed_pairs[:, 1],  # ground truth\n            self._detailed_pairs[:, 2],  # prediction\n            -self._detailed_pairs[:, 3],  # score\n        )\n    )\n    self._detailed_pairs = self._detailed_pairs[indices]\n    self._metadata = Metadata.create(\n        detailed_pairs=self._detailed_pairs,\n        number_of_datums=len(self.index_to_datum_id),\n        number_of_labels=len(self.index_to_label),\n    )\n    return self\n</code></pre>"},{"location":"classification/metrics/","title":"Metrics","text":""},{"location":"classification/metrics/#valor_lite.classification.metric.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Classification Metric.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The metric type.</p> <code>value</code> <code>int | float | dict</code> <p>The metric value.</p> <code>parameters</code> <code>dict[str, Any]</code> <p>A dictionary containing metric parameters.</p> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@dataclass\nclass Metric(BaseMetric):\n    \"\"\"\n    Classification Metric.\n\n    Attributes\n    ----------\n    type : str\n        The metric type.\n    value : int | float | dict\n        The metric value.\n    parameters : dict[str, Any]\n        A dictionary containing metric parameters.\n    \"\"\"\n\n    def __post_init__(self):\n        if not isinstance(self.type, str):\n            raise TypeError(\n                f\"Metric type should be of type 'str': {self.type}\"\n            )\n        elif not isinstance(self.value, (int, float, dict)):\n            raise TypeError(\n                f\"Metric value must be of type 'int', 'float' or 'dict': {self.value}\"\n            )\n        elif not isinstance(self.parameters, dict):\n            raise TypeError(\n                f\"Metric parameters must be of type 'dict[str, Any]': {self.parameters}\"\n            )\n        elif not all([isinstance(k, str) for k in self.parameters.keys()]):\n            raise TypeError(\n                f\"Metric parameter dictionary should only have keys with type 'str': {self.parameters}\"\n            )\n\n    @classmethod\n    def precision(\n        cls,\n        value: float,\n        score_threshold: float,\n        hardmax: bool,\n        label: str,\n    ):\n        \"\"\"\n        Precision metric for a specific class label.\n\n        This class calculates the precision at a specific score threshold.\n        Precision is defined as the ratio of true positives to the sum of\n        true positives and false positives.\n\n        Parameters\n        ----------\n        value : float\n            Precision value computed at a specific score threshold.\n        score_threshold : float\n            Score threshold at which the precision value is computed.\n        hardmax : bool\n            Indicates whether hardmax thresholding was used.\n        label : str\n            The class label for which the precision is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Precision.value,\n            value=value,\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"hardmax\": hardmax,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def recall(\n        cls,\n        value: float,\n        score_threshold: float,\n        hardmax: bool,\n        label: str,\n    ):\n        \"\"\"\n        Recall metric for a specific class label.\n\n        This class calculates the recall at a specific score threshold.\n        Recall is defined as the ratio of true positives to the sum of\n        true positives and false negatives.\n\n        Parameters\n        ----------\n        value : float\n            Recall value computed at a specific score threshold.\n        score_threshold : float\n            Score threshold at which the recall value is computed.\n        hardmax : bool\n            Indicates whether hardmax thresholding was used.\n        label : str\n            The class label for which the recall is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Recall.value,\n            value=value,\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"hardmax\": hardmax,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def f1_score(\n        cls,\n        value: float,\n        score_threshold: float,\n        hardmax: bool,\n        label: str,\n    ):\n        \"\"\"\n        F1 score for a specific class label and confidence score threshold.\n\n        Parameters\n        ----------\n        value : float\n            F1 score computed at a specific score threshold.\n        score_threshold : float\n            Score threshold at which the F1 score is computed.\n        hardmax : bool\n            Indicates whether hardmax thresholding was used.\n        label : str\n            The class label for which the F1 score is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.F1.value,\n            value=value,\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"hardmax\": hardmax,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def accuracy(\n        cls,\n        value: float,\n        score_threshold: float,\n        hardmax: bool,\n    ):\n        \"\"\"\n        Multiclass accuracy metric.\n\n        This class calculates the accuracy at various score thresholds.\n\n        Parameters\n        ----------\n        value : float\n            Accuracy value computed at a specific score threshold.\n        score_threshold : float\n            Score threshold at which the accuracy value is computed.\n        hardmax : bool\n            Indicates whether hardmax thresholding was used.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Accuracy.value,\n            value=value,\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"hardmax\": hardmax,\n            },\n        )\n\n    @classmethod\n    def roc_auc(\n        cls,\n        value: float,\n        label: str,\n    ):\n        \"\"\"\n        Receiver Operating Characteristic Area Under the Curve (ROC AUC).\n\n        This class calculates the ROC AUC score for a specific class label in a multiclass classification task.\n        ROC AUC is a performance measurement for classification problems at various threshold settings.\n        It reflects the ability of the classifier to distinguish between the positive and negative classes.\n\n        Parameters\n        ----------\n        value : float\n            The computed ROC AUC score.\n        label : str\n            The class label for which the ROC AUC is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.ROCAUC.value,\n            value=value,\n            parameters={\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def mean_roc_auc(cls, value: float):\n        \"\"\"\n        Mean Receiver Operating Characteristic Area Under the Curve (mROC AUC).\n\n        This class calculates the mean ROC AUC score over all classes in a multiclass classification task.\n        It provides an aggregate measure of the model's ability to distinguish between classes.\n\n        Parameters\n        ----------\n        value : float\n            The computed mean ROC AUC score.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(type=MetricType.mROCAUC.value, value=value, parameters={})\n\n    @classmethod\n    def counts(\n        cls,\n        tp: int,\n        fp: int,\n        fn: int,\n        tn: int,\n        score_threshold: float,\n        hardmax: bool,\n        label: str,\n    ):\n        \"\"\"\n        Confusion matrix counts at specified score thresholds for binary classification.\n\n        This class stores the true positive (`tp`), false positive (`fp`), false negative (`fn`), and true\n        negative (`tn`) counts computed at various score thresholds for a binary classification task.\n\n        Parameters\n        ----------\n        tp : int\n            True positive counts at each score threshold.\n        fp : int\n            False positive counts at each score threshold.\n        fn : int\n            False negative counts at each score threshold.\n        tn : int\n            True negative counts at each score threshold.\n        score_threshold : float\n            Score thresholds at which the counts are computed.\n        hardmax : bool\n            Indicates whether hardmax thresholding was used.\n        label : str\n            The class label for which the counts are computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Counts.value,\n            value={\n                \"tp\": tp,\n                \"fp\": fp,\n                \"fn\": fn,\n                \"tn\": tn,\n            },\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"hardmax\": hardmax,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def confusion_matrix(\n        cls,\n        confusion_matrix: dict[\n            str,  # ground truth label value\n            dict[\n                str,  # prediction label value\n                dict[\n                    str,  # either `count` or `examples`\n                    int\n                    | list[\n                        dict[\n                            str,  # either `datum` or `score`\n                            str | float,  # datum uid  # prediction score\n                        ]\n                    ],\n                ],\n            ],\n        ],\n        unmatched_ground_truths: dict[\n            str,  # ground truth label value\n            dict[\n                str,  # either `count` or `examples`\n                int | list[dict[str, str]],  # count or datum examples\n            ],\n        ],\n        score_threshold: float,\n        maximum_number_of_examples: int,\n    ):\n        \"\"\"\n        The confusion matrix and related metrics for the classification task.\n\n        This class encapsulates detailed information about the model's performance, including correct\n        predictions, misclassifications and unmatched ground truths (subset of false negatives).\n        It provides counts and examples for each category to facilitate in-depth analysis.\n\n        Confusion Matrix Structure:\n        {\n            ground_truth_label: {\n                predicted_label: {\n                    'count': int,\n                    'examples': [\n                        {\n                            \"datum_id\": str,\n                            \"score\": float\n                        },\n                        ...\n                    ],\n                },\n                ...\n            },\n            ...\n        }\n\n        Unmatched Ground Truths Structure:\n        {\n            ground_truth_label: {\n                'count': int,\n                'examples': [\n                    {\n                        \"datum_id\": str\n                    },\n                    ...\n                ],\n            },\n            ...\n        }\n\n        Parameters\n        ----------\n        confusion_matrix : dict\n            A nested dictionary where the first key is the ground truth label value, the second key\n            is the prediction label value, and the innermost dictionary contains either a `count`\n            or a list of `examples`. Each example includes the datum UID and prediction score.\n        unmatched_ground_truths : dict\n            A dictionary where each key is a ground truth label value for which the model failed to predict\n            (false negatives). The value is a dictionary containing either a `count` or a list of `examples`.\n            Each example includes the datum UID.\n        score_threshold : float\n            The confidence score threshold used to filter predictions.\n        maximum_number_of_examples : int\n            The maximum number of examples per element.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.ConfusionMatrix.value,\n            value={\n                \"confusion_matrix\": confusion_matrix,\n                \"unmatched_ground_truths\": unmatched_ground_truths,\n            },\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"maximum_number_of_examples\": maximum_number_of_examples,\n            },\n        )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.accuracy","title":"<code>accuracy(value, score_threshold, hardmax)</code>  <code>classmethod</code>","text":"<p>Multiclass accuracy metric.</p> <p>This class calculates the accuracy at various score thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Accuracy value computed at a specific score threshold.</p> required <code>score_threshold</code> <code>float</code> <p>Score threshold at which the accuracy value is computed.</p> required <code>hardmax</code> <code>bool</code> <p>Indicates whether hardmax thresholding was used.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef accuracy(\n    cls,\n    value: float,\n    score_threshold: float,\n    hardmax: bool,\n):\n    \"\"\"\n    Multiclass accuracy metric.\n\n    This class calculates the accuracy at various score thresholds.\n\n    Parameters\n    ----------\n    value : float\n        Accuracy value computed at a specific score threshold.\n    score_threshold : float\n        Score threshold at which the accuracy value is computed.\n    hardmax : bool\n        Indicates whether hardmax thresholding was used.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Accuracy.value,\n        value=value,\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"hardmax\": hardmax,\n        },\n    )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.confusion_matrix","title":"<code>confusion_matrix(confusion_matrix, unmatched_ground_truths, score_threshold, maximum_number_of_examples)</code>  <code>classmethod</code>","text":"<p>The confusion matrix and related metrics for the classification task.</p> <p>This class encapsulates detailed information about the model's performance, including correct predictions, misclassifications and unmatched ground truths (subset of false negatives). It provides counts and examples for each category to facilitate in-depth analysis.</p> <p>Confusion Matrix Structure: {     ground_truth_label: {         predicted_label: {             'count': int,             'examples': [                 {                     \"datum_id\": str,                     \"score\": float                 },                 ...             ],         },         ...     },     ... }</p> <p>Unmatched Ground Truths Structure: {     ground_truth_label: {         'count': int,         'examples': [             {                 \"datum_id\": str             },             ...         ],     },     ... }</p> <p>Parameters:</p> Name Type Description Default <code>confusion_matrix</code> <code>dict</code> <p>A nested dictionary where the first key is the ground truth label value, the second key is the prediction label value, and the innermost dictionary contains either a <code>count</code> or a list of <code>examples</code>. Each example includes the datum UID and prediction score.</p> required <code>unmatched_ground_truths</code> <code>dict</code> <p>A dictionary where each key is a ground truth label value for which the model failed to predict (false negatives). The value is a dictionary containing either a <code>count</code> or a list of <code>examples</code>. Each example includes the datum UID.</p> required <code>score_threshold</code> <code>float</code> <p>The confidence score threshold used to filter predictions.</p> required <code>maximum_number_of_examples</code> <code>int</code> <p>The maximum number of examples per element.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef confusion_matrix(\n    cls,\n    confusion_matrix: dict[\n        str,  # ground truth label value\n        dict[\n            str,  # prediction label value\n            dict[\n                str,  # either `count` or `examples`\n                int\n                | list[\n                    dict[\n                        str,  # either `datum` or `score`\n                        str | float,  # datum uid  # prediction score\n                    ]\n                ],\n            ],\n        ],\n    ],\n    unmatched_ground_truths: dict[\n        str,  # ground truth label value\n        dict[\n            str,  # either `count` or `examples`\n            int | list[dict[str, str]],  # count or datum examples\n        ],\n    ],\n    score_threshold: float,\n    maximum_number_of_examples: int,\n):\n    \"\"\"\n    The confusion matrix and related metrics for the classification task.\n\n    This class encapsulates detailed information about the model's performance, including correct\n    predictions, misclassifications and unmatched ground truths (subset of false negatives).\n    It provides counts and examples for each category to facilitate in-depth analysis.\n\n    Confusion Matrix Structure:\n    {\n        ground_truth_label: {\n            predicted_label: {\n                'count': int,\n                'examples': [\n                    {\n                        \"datum_id\": str,\n                        \"score\": float\n                    },\n                    ...\n                ],\n            },\n            ...\n        },\n        ...\n    }\n\n    Unmatched Ground Truths Structure:\n    {\n        ground_truth_label: {\n            'count': int,\n            'examples': [\n                {\n                    \"datum_id\": str\n                },\n                ...\n            ],\n        },\n        ...\n    }\n\n    Parameters\n    ----------\n    confusion_matrix : dict\n        A nested dictionary where the first key is the ground truth label value, the second key\n        is the prediction label value, and the innermost dictionary contains either a `count`\n        or a list of `examples`. Each example includes the datum UID and prediction score.\n    unmatched_ground_truths : dict\n        A dictionary where each key is a ground truth label value for which the model failed to predict\n        (false negatives). The value is a dictionary containing either a `count` or a list of `examples`.\n        Each example includes the datum UID.\n    score_threshold : float\n        The confidence score threshold used to filter predictions.\n    maximum_number_of_examples : int\n        The maximum number of examples per element.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.ConfusionMatrix.value,\n        value={\n            \"confusion_matrix\": confusion_matrix,\n            \"unmatched_ground_truths\": unmatched_ground_truths,\n        },\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"maximum_number_of_examples\": maximum_number_of_examples,\n        },\n    )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.counts","title":"<code>counts(tp, fp, fn, tn, score_threshold, hardmax, label)</code>  <code>classmethod</code>","text":"<p>Confusion matrix counts at specified score thresholds for binary classification.</p> <p>This class stores the true positive (<code>tp</code>), false positive (<code>fp</code>), false negative (<code>fn</code>), and true negative (<code>tn</code>) counts computed at various score thresholds for a binary classification task.</p> <p>Parameters:</p> Name Type Description Default <code>tp</code> <code>int</code> <p>True positive counts at each score threshold.</p> required <code>fp</code> <code>int</code> <p>False positive counts at each score threshold.</p> required <code>fn</code> <code>int</code> <p>False negative counts at each score threshold.</p> required <code>tn</code> <code>int</code> <p>True negative counts at each score threshold.</p> required <code>score_threshold</code> <code>float</code> <p>Score thresholds at which the counts are computed.</p> required <code>hardmax</code> <code>bool</code> <p>Indicates whether hardmax thresholding was used.</p> required <code>label</code> <code>str</code> <p>The class label for which the counts are computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef counts(\n    cls,\n    tp: int,\n    fp: int,\n    fn: int,\n    tn: int,\n    score_threshold: float,\n    hardmax: bool,\n    label: str,\n):\n    \"\"\"\n    Confusion matrix counts at specified score thresholds for binary classification.\n\n    This class stores the true positive (`tp`), false positive (`fp`), false negative (`fn`), and true\n    negative (`tn`) counts computed at various score thresholds for a binary classification task.\n\n    Parameters\n    ----------\n    tp : int\n        True positive counts at each score threshold.\n    fp : int\n        False positive counts at each score threshold.\n    fn : int\n        False negative counts at each score threshold.\n    tn : int\n        True negative counts at each score threshold.\n    score_threshold : float\n        Score thresholds at which the counts are computed.\n    hardmax : bool\n        Indicates whether hardmax thresholding was used.\n    label : str\n        The class label for which the counts are computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Counts.value,\n        value={\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n        },\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"hardmax\": hardmax,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.f1_score","title":"<code>f1_score(value, score_threshold, hardmax, label)</code>  <code>classmethod</code>","text":"<p>F1 score for a specific class label and confidence score threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>F1 score computed at a specific score threshold.</p> required <code>score_threshold</code> <code>float</code> <p>Score threshold at which the F1 score is computed.</p> required <code>hardmax</code> <code>bool</code> <p>Indicates whether hardmax thresholding was used.</p> required <code>label</code> <code>str</code> <p>The class label for which the F1 score is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef f1_score(\n    cls,\n    value: float,\n    score_threshold: float,\n    hardmax: bool,\n    label: str,\n):\n    \"\"\"\n    F1 score for a specific class label and confidence score threshold.\n\n    Parameters\n    ----------\n    value : float\n        F1 score computed at a specific score threshold.\n    score_threshold : float\n        Score threshold at which the F1 score is computed.\n    hardmax : bool\n        Indicates whether hardmax thresholding was used.\n    label : str\n        The class label for which the F1 score is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.F1.value,\n        value=value,\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"hardmax\": hardmax,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.mean_roc_auc","title":"<code>mean_roc_auc(value)</code>  <code>classmethod</code>","text":"<p>Mean Receiver Operating Characteristic Area Under the Curve (mROC AUC).</p> <p>This class calculates the mean ROC AUC score over all classes in a multiclass classification task. It provides an aggregate measure of the model's ability to distinguish between classes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The computed mean ROC AUC score.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef mean_roc_auc(cls, value: float):\n    \"\"\"\n    Mean Receiver Operating Characteristic Area Under the Curve (mROC AUC).\n\n    This class calculates the mean ROC AUC score over all classes in a multiclass classification task.\n    It provides an aggregate measure of the model's ability to distinguish between classes.\n\n    Parameters\n    ----------\n    value : float\n        The computed mean ROC AUC score.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(type=MetricType.mROCAUC.value, value=value, parameters={})\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.precision","title":"<code>precision(value, score_threshold, hardmax, label)</code>  <code>classmethod</code>","text":"<p>Precision metric for a specific class label.</p> <p>This class calculates the precision at a specific score threshold. Precision is defined as the ratio of true positives to the sum of true positives and false positives.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Precision value computed at a specific score threshold.</p> required <code>score_threshold</code> <code>float</code> <p>Score threshold at which the precision value is computed.</p> required <code>hardmax</code> <code>bool</code> <p>Indicates whether hardmax thresholding was used.</p> required <code>label</code> <code>str</code> <p>The class label for which the precision is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef precision(\n    cls,\n    value: float,\n    score_threshold: float,\n    hardmax: bool,\n    label: str,\n):\n    \"\"\"\n    Precision metric for a specific class label.\n\n    This class calculates the precision at a specific score threshold.\n    Precision is defined as the ratio of true positives to the sum of\n    true positives and false positives.\n\n    Parameters\n    ----------\n    value : float\n        Precision value computed at a specific score threshold.\n    score_threshold : float\n        Score threshold at which the precision value is computed.\n    hardmax : bool\n        Indicates whether hardmax thresholding was used.\n    label : str\n        The class label for which the precision is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Precision.value,\n        value=value,\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"hardmax\": hardmax,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.recall","title":"<code>recall(value, score_threshold, hardmax, label)</code>  <code>classmethod</code>","text":"<p>Recall metric for a specific class label.</p> <p>This class calculates the recall at a specific score threshold. Recall is defined as the ratio of true positives to the sum of true positives and false negatives.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Recall value computed at a specific score threshold.</p> required <code>score_threshold</code> <code>float</code> <p>Score threshold at which the recall value is computed.</p> required <code>hardmax</code> <code>bool</code> <p>Indicates whether hardmax thresholding was used.</p> required <code>label</code> <code>str</code> <p>The class label for which the recall is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef recall(\n    cls,\n    value: float,\n    score_threshold: float,\n    hardmax: bool,\n    label: str,\n):\n    \"\"\"\n    Recall metric for a specific class label.\n\n    This class calculates the recall at a specific score threshold.\n    Recall is defined as the ratio of true positives to the sum of\n    true positives and false negatives.\n\n    Parameters\n    ----------\n    value : float\n        Recall value computed at a specific score threshold.\n    score_threshold : float\n        Score threshold at which the recall value is computed.\n    hardmax : bool\n        Indicates whether hardmax thresholding was used.\n    label : str\n        The class label for which the recall is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Recall.value,\n        value=value,\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"hardmax\": hardmax,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"classification/metrics/#valor_lite.classification.metric.Metric.roc_auc","title":"<code>roc_auc(value, label)</code>  <code>classmethod</code>","text":"<p>Receiver Operating Characteristic Area Under the Curve (ROC AUC).</p> <p>This class calculates the ROC AUC score for a specific class label in a multiclass classification task. ROC AUC is a performance measurement for classification problems at various threshold settings. It reflects the ability of the classifier to distinguish between the positive and negative classes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The computed ROC AUC score.</p> required <code>label</code> <code>str</code> <p>The class label for which the ROC AUC is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/classification/metric.py</code> <pre><code>@classmethod\ndef roc_auc(\n    cls,\n    value: float,\n    label: str,\n):\n    \"\"\"\n    Receiver Operating Characteristic Area Under the Curve (ROC AUC).\n\n    This class calculates the ROC AUC score for a specific class label in a multiclass classification task.\n    ROC AUC is a performance measurement for classification problems at various threshold settings.\n    It reflects the ability of the classifier to distinguish between the positive and negative classes.\n\n    Parameters\n    ----------\n    value : float\n        The computed ROC AUC score.\n    label : str\n        The class label for which the ROC AUC is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.ROCAUC.value,\n        value=value,\n        parameters={\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/documentation/","title":"Documentation","text":"<p>Documentation</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.BoundingBox","title":"<code>valor_lite.object_detection.BoundingBox</code>  <code>dataclass</code>","text":"<p>Represents a bounding box with associated labels and optional scores.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>A unique identifier.</p> required <code>xmin</code> <code>float</code> <p>The minimum x-coordinate of the bounding box.</p> required <code>xmax</code> <code>float</code> <p>The maximum x-coordinate of the bounding box.</p> required <code>ymin</code> <code>float</code> <p>The minimum y-coordinate of the bounding box.</p> required <code>ymax</code> <code>float</code> <p>The maximum y-coordinate of the bounding box.</p> required <code>labels</code> <code>list of str</code> <p>List of labels associated with the bounding box.</p> required <code>scores</code> <code>list of float</code> <p>Confidence scores corresponding to each label. Defaults to an empty list.</p> <code>list()</code> <p>Examples:</p> <p>Ground Truth Example:</p> <pre><code>&gt;&gt;&gt; bbox = BoundingBox(uid=\"xyz\", xmin=10.0, xmax=50.0, ymin=20.0, ymax=60.0, labels=['cat'])\n</code></pre> <p>Prediction Example:</p> <pre><code>&gt;&gt;&gt; bbox = BoundingBox(\n...     uid=\"abc\",\n...     xmin=10.0, xmax=50.0, ymin=20.0, ymax=60.0,\n...     labels=['cat', 'dog'], scores=[0.9, 0.1]\n... )\n</code></pre> Source code in <code>valor_lite/object_detection/annotation.py</code> <pre><code>@dataclass\nclass BoundingBox:\n    \"\"\"\n    Represents a bounding box with associated labels and optional scores.\n\n    Parameters\n    ----------\n    uid : str\n        A unique identifier.\n    xmin : float\n        The minimum x-coordinate of the bounding box.\n    xmax : float\n        The maximum x-coordinate of the bounding box.\n    ymin : float\n        The minimum y-coordinate of the bounding box.\n    ymax : float\n        The maximum y-coordinate of the bounding box.\n    labels : list of str\n        List of labels associated with the bounding box.\n    scores : list of float, optional\n        Confidence scores corresponding to each label. Defaults to an empty list.\n\n    Examples\n    --------\n    Ground Truth Example:\n\n    &gt;&gt;&gt; bbox = BoundingBox(uid=\"xyz\", xmin=10.0, xmax=50.0, ymin=20.0, ymax=60.0, labels=['cat'])\n\n    Prediction Example:\n\n    &gt;&gt;&gt; bbox = BoundingBox(\n    ...     uid=\"abc\",\n    ...     xmin=10.0, xmax=50.0, ymin=20.0, ymax=60.0,\n    ...     labels=['cat', 'dog'], scores=[0.9, 0.1]\n    ... )\n    \"\"\"\n\n    uid: str\n    xmin: float\n    xmax: float\n    ymin: float\n    ymax: float\n    labels: list[str]\n    scores: list[float] = field(default_factory=list)\n\n    def __post_init__(self):\n        if len(self.scores) == 0 and len(self.labels) != 1:\n            raise ValueError(\n                \"Ground truths must be defined with no scores and a single label. If you meant to define a prediction, then please include one score for every label provided.\"\n            )\n        if len(self.scores) &gt; 0 and len(self.labels) != len(self.scores):\n            raise ValueError(\n                \"If scores are defined, there must be a 1:1 pairing with labels.\"\n            )\n\n    @property\n    def extrema(self) -&gt; tuple[float, float, float, float]:\n        \"\"\"\n        Returns the annotation's data representation.\n\n        Returns\n        -------\n        tuple[float, float, float, float]\n            A tuple in the form (xmin, xmax, ymin, ymax).\n        \"\"\"\n        return (self.xmin, self.xmax, self.ymin, self.ymax)\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.BoundingBox.extrema","title":"<code>extrema</code>  <code>property</code>","text":"<p>Returns the annotation's data representation.</p> <p>Returns:</p> Type Description <code>tuple[float, float, float, float]</code> <p>A tuple in the form (xmin, xmax, ymin, ymax).</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Polygon","title":"<code>valor_lite.object_detection.Polygon</code>  <code>dataclass</code>","text":"<p>Represents a polygon shape with associated labels and optional scores.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>A unique identifier.</p> required <code>shape</code> <code>Polygon</code> <p>A Shapely polygon object representing the shape.</p> required <code>labels</code> <code>list of str</code> <p>List of labels associated with the polygon.</p> required <code>scores</code> <code>list of float</code> <p>Confidence scores corresponding to each label. Defaults to an empty list.</p> <code>list()</code> <p>Examples:</p> <p>Ground Truth Example:</p> <pre><code>&gt;&gt;&gt; from shapely.geometry import Polygon as ShapelyPolygon\n&gt;&gt;&gt; shape = ShapelyPolygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n&gt;&gt;&gt; polygon = Polygon(uid=\"xyz\", shape=shape, labels=['building'])\n</code></pre> <p>Prediction Example:</p> <pre><code>&gt;&gt;&gt; polygon = Polygon(\n...     uid=\"abc\", shape=shape, labels=['building'], scores=[0.95]\n... )\n</code></pre> Source code in <code>valor_lite/object_detection/annotation.py</code> <pre><code>@dataclass\nclass Polygon:\n    \"\"\"\n    Represents a polygon shape with associated labels and optional scores.\n\n    Parameters\n    ----------\n    uid : str\n        A unique identifier.\n    shape : shapely.geometry.Polygon\n        A Shapely polygon object representing the shape.\n    labels : list of str\n        List of labels associated with the polygon.\n    scores : list of float, optional\n        Confidence scores corresponding to each label. Defaults to an empty list.\n\n    Examples\n    --------\n    Ground Truth Example:\n\n    &gt;&gt;&gt; from shapely.geometry import Polygon as ShapelyPolygon\n    &gt;&gt;&gt; shape = ShapelyPolygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n    &gt;&gt;&gt; polygon = Polygon(uid=\"xyz\", shape=shape, labels=['building'])\n\n    Prediction Example:\n\n    &gt;&gt;&gt; polygon = Polygon(\n    ...     uid=\"abc\", shape=shape, labels=['building'], scores=[0.95]\n    ... )\n    \"\"\"\n\n    uid: str\n    shape: ShapelyPolygon\n    labels: list[str]\n    scores: list[float] = field(default_factory=list)\n\n    def __post_init__(self):\n        if not isinstance(self.shape, ShapelyPolygon):\n            raise TypeError(\"shape must be of type shapely.geometry.Polygon.\")\n        if self.shape.is_empty:\n            raise ValueError(\"Polygon is empty.\")\n\n        if len(self.scores) == 0 and len(self.labels) != 1:\n            raise ValueError(\n                \"Ground truths must be defined with no scores and a single label. If you meant to define a prediction, then please include one score for every label provided.\"\n            )\n        if len(self.scores) &gt; 0 and len(self.labels) != len(self.scores):\n            raise ValueError(\n                \"If scores are defined, there must be a 1:1 pairing with labels.\"\n            )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Bitmask","title":"<code>valor_lite.object_detection.Bitmask</code>  <code>dataclass</code>","text":"<p>Represents a binary mask with associated labels and optional scores.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>A unique identifier.</p> required <code>mask</code> <code>NDArray[bool_]</code> <p>A NumPy array of boolean values representing the mask.</p> required <code>labels</code> <code>list of str</code> <p>List of labels associated with the mask.</p> required <code>scores</code> <code>list of float</code> <p>Confidence scores corresponding to each label. Defaults to an empty list.</p> <code>list()</code> <p>Examples:</p> <p>Ground Truth Example:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; mask = np.array([[True, False], [False, True]], dtype=np.bool_)\n&gt;&gt;&gt; bitmask = Bitmask(uid=\"abc\", mask=mask, labels=['tree'])\n</code></pre> <p>Prediction Example:</p> <pre><code>&gt;&gt;&gt; bitmask = Bitmask(\n...     uid=\"xyz\", mask=mask, labels=['tree'], scores=[0.85]\n... )\n</code></pre> Source code in <code>valor_lite/object_detection/annotation.py</code> <pre><code>@dataclass\nclass Bitmask:\n    \"\"\"\n    Represents a binary mask with associated labels and optional scores.\n\n    Parameters\n    ----------\n    uid : str\n        A unique identifier.\n    mask : NDArray[np.bool_]\n        A NumPy array of boolean values representing the mask.\n    labels : list of str\n        List of labels associated with the mask.\n    scores : list of float, optional\n        Confidence scores corresponding to each label. Defaults to an empty list.\n\n    Examples\n    --------\n    Ground Truth Example:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; mask = np.array([[True, False], [False, True]], dtype=np.bool_)\n    &gt;&gt;&gt; bitmask = Bitmask(uid=\"abc\", mask=mask, labels=['tree'])\n\n    Prediction Example:\n\n    &gt;&gt;&gt; bitmask = Bitmask(\n    ...     uid=\"xyz\", mask=mask, labels=['tree'], scores=[0.85]\n    ... )\n    \"\"\"\n\n    uid: str\n    mask: NDArray[np.bool_]\n    labels: list[str]\n    scores: list[float] = field(default_factory=list)\n\n    def __post_init__(self):\n\n        if (\n            not isinstance(self.mask, np.ndarray)\n            or self.mask.dtype != np.bool_\n        ):\n            raise ValueError(\n                \"Expected mask to be of type `NDArray[np.bool_]`.\"\n            )\n        elif not self.mask.any():\n            raise ValueError(\"Mask does not define any object instances.\")\n\n        if len(self.scores) == 0 and len(self.labels) != 1:\n            raise ValueError(\n                \"Ground truths must be defined with no scores and a single label. If you meant to define a prediction, then please include one score for every label provided.\"\n            )\n        if len(self.scores) &gt; 0 and len(self.labels) != len(self.scores):\n            raise ValueError(\n                \"If scores are defined, there must be a 1:1 pairing with labels.\"\n            )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Detection","title":"<code>valor_lite.object_detection.Detection</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[AnnotationType]</code></p> <p>Detection data structure holding ground truths and predictions for object detection tasks.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique identifier for the image or sample.</p> required <code>groundtruths</code> <code>list[BoundingBox] | list[Polygon] | list[Bitmask]</code> <p>List of ground truth annotations.</p> required <code>predictions</code> <code>list[BoundingBox] | list[Polygon] | list[Bitmask]</code> <p>List of predicted annotations.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; bbox_gt = BoundingBox(xmin=10, xmax=50, ymin=20, ymax=60, labels=['cat'])\n&gt;&gt;&gt; bbox_pred = BoundingBox(\n...     xmin=12, xmax=48, ymin=22, ymax=58, labels=['cat'], scores=[0.9]\n... )\n&gt;&gt;&gt; detection = Detection(\n...     uid='image_001',\n...     groundtruths=[bbox_gt],\n...     predictions=[bbox_pred]\n... )\n</code></pre> Source code in <code>valor_lite/object_detection/annotation.py</code> <pre><code>@dataclass\nclass Detection(Generic[AnnotationType]):\n    \"\"\"\n    Detection data structure holding ground truths and predictions for object detection tasks.\n\n    Parameters\n    ----------\n    uid : str\n        Unique identifier for the image or sample.\n    groundtruths : list[BoundingBox] | list[Polygon] | list[Bitmask]\n        List of ground truth annotations.\n    predictions : list[BoundingBox] | list[Polygon] | list[Bitmask]\n        List of predicted annotations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bbox_gt = BoundingBox(xmin=10, xmax=50, ymin=20, ymax=60, labels=['cat'])\n    &gt;&gt;&gt; bbox_pred = BoundingBox(\n    ...     xmin=12, xmax=48, ymin=22, ymax=58, labels=['cat'], scores=[0.9]\n    ... )\n    &gt;&gt;&gt; detection = Detection(\n    ...     uid='image_001',\n    ...     groundtruths=[bbox_gt],\n    ...     predictions=[bbox_pred]\n    ... )\n    \"\"\"\n\n    uid: str\n    groundtruths: list[AnnotationType]\n    predictions: list[AnnotationType]\n\n    def __post_init__(self):\n        for prediction in self.predictions:\n            if len(prediction.scores) != len(prediction.labels):\n                raise ValueError(\n                    \"Predictions must provide a score for every label.\"\n                )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.DataLoader","title":"<code>valor_lite.object_detection.DataLoader</code>","text":"<p>               Bases: <code>Evaluator</code></p> <p>Used for backwards compatibility as the Evaluator now handles ingestion.</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>class DataLoader(Evaluator):\n    \"\"\"\n    Used for backwards compatibility as the Evaluator now handles ingestion.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator","title":"<code>valor_lite.object_detection.Evaluator</code>","text":"<p>Object Detection Evaluator</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>class Evaluator:\n    \"\"\"\n    Object Detection Evaluator\n    \"\"\"\n\n    def __init__(self):\n\n        # external reference\n        self.datum_id_to_index: dict[str, int] = {}\n        self.groundtruth_id_to_index: dict[str, int] = {}\n        self.prediction_id_to_index: dict[str, int] = {}\n        self.label_to_index: dict[str, int] = {}\n\n        self.index_to_datum_id: list[str] = []\n        self.index_to_groundtruth_id: list[str] = []\n        self.index_to_prediction_id: list[str] = []\n        self.index_to_label: list[str] = []\n\n        # temporary cache\n        self._temp_cache: list[NDArray[np.float64]] | None = []\n\n        # cache\n        self._detailed_pairs = np.array([[]], dtype=np.float64)\n        self._ranked_pairs = np.array([[]], dtype=np.float64)\n        self._label_metadata: NDArray[np.int32] = np.array([[]])\n\n        # filter cache\n        self._filtered_detailed_pairs: NDArray[np.float64] | None = None\n        self._filtered_ranked_pairs: NDArray[np.float64] | None = None\n        self._filtered_label_metadata: NDArray[np.int32] | None = None\n\n    @property\n    def is_filtered(self) -&gt; bool:\n        return self._filtered_detailed_pairs is not None\n\n    @property\n    def label_metadata(self) -&gt; NDArray[np.int32]:\n        return (\n            self._filtered_label_metadata\n            if self._filtered_label_metadata is not None\n            else self._label_metadata\n        )\n\n    @property\n    def detailed_pairs(self) -&gt; NDArray[np.float64]:\n        return (\n            self._filtered_detailed_pairs\n            if self._filtered_detailed_pairs is not None\n            else self._detailed_pairs\n        )\n\n    @property\n    def ranked_pairs(self) -&gt; NDArray[np.float64]:\n        return (\n            self._filtered_ranked_pairs\n            if self._filtered_ranked_pairs is not None\n            else self._ranked_pairs\n        )\n\n    @property\n    def n_labels(self) -&gt; int:\n        \"\"\"Returns the total number of unique labels.\"\"\"\n        return len(self.index_to_label)\n\n    @property\n    def n_datums(self) -&gt; int:\n        \"\"\"Returns the number of datums.\"\"\"\n        return np.unique(self.detailed_pairs[:, 0]).size\n\n    @property\n    def n_groundtruths(self) -&gt; int:\n        \"\"\"Returns the number of ground truth annotations.\"\"\"\n        mask_valid_gts = self.detailed_pairs[:, 1] &gt;= 0\n        unique_ids = np.unique(\n            self.detailed_pairs[np.ix_(mask_valid_gts, (0, 1))], axis=0  # type: ignore - np.ix_ typing\n        )\n        return int(unique_ids.shape[0])\n\n    @property\n    def n_predictions(self) -&gt; int:\n        \"\"\"Returns the number of prediction annotations.\"\"\"\n        mask_valid_pds = self.detailed_pairs[:, 2] &gt;= 0\n        unique_ids = np.unique(\n            self.detailed_pairs[np.ix_(mask_valid_pds, (0, 2))], axis=0  # type: ignore - np.ix_ typing\n        )\n        return int(unique_ids.shape[0])\n\n    @property\n    def ignored_prediction_labels(self) -&gt; list[str]:\n        \"\"\"\n        Prediction labels that are not present in the ground truth set.\n        \"\"\"\n        label_metadata = self.label_metadata\n        glabels = set(np.where(label_metadata[:, 0] &gt; 0)[0])\n        plabels = set(np.where(label_metadata[:, 1] &gt; 0)[0])\n        return [\n            self.index_to_label[label_id] for label_id in (plabels - glabels)\n        ]\n\n    @property\n    def missing_prediction_labels(self) -&gt; list[str]:\n        \"\"\"\n        Ground truth labels that are not present in the prediction set.\n        \"\"\"\n        label_metadata = self.label_metadata\n        glabels = set(np.where(label_metadata[:, 0] &gt; 0)[0])\n        plabels = set(np.where(label_metadata[:, 1] &gt; 0)[0])\n        return [\n            self.index_to_label[label_id] for label_id in (glabels - plabels)\n        ]\n\n    @property\n    def metadata(self) -&gt; dict:\n        \"\"\"\n        Evaluation metadata.\n        \"\"\"\n        return {\n            \"n_datums\": self.n_datums,\n            \"n_groundtruths\": self.n_groundtruths,\n            \"n_predictions\": self.n_predictions,\n            \"n_labels\": self.n_labels,\n            \"ignored_prediction_labels\": self.ignored_prediction_labels,\n            \"missing_prediction_labels\": self.missing_prediction_labels,\n        }\n\n    def compute_precision_recall(\n        self,\n        iou_thresholds: list[float],\n        score_thresholds: list[float],\n    ) -&gt; dict[MetricType, list[Metric]]:\n        \"\"\"\n        Computes all metrics except for ConfusionMatrix\n\n        Parameters\n        ----------\n        iou_thresholds : list[float]\n            A list of IOU thresholds to compute metrics over.\n        score_thresholds : list[float]\n            A list of score thresholds to compute metrics over.\n\n        Returns\n        -------\n        dict[MetricType, list]\n            A dictionary mapping MetricType enumerations to lists of computed metrics.\n        \"\"\"\n        if not iou_thresholds:\n            raise ValueError(\"At least one IOU threshold must be passed.\")\n        elif not score_thresholds:\n            raise ValueError(\"At least one score threshold must be passed.\")\n        results = compute_precion_recall(\n            ranked_pairs=self.ranked_pairs,\n            label_metadata=self.label_metadata,\n            iou_thresholds=np.array(iou_thresholds),\n            score_thresholds=np.array(score_thresholds),\n        )\n        return unpack_precision_recall_into_metric_lists(\n            results=results,\n            label_metadata=self.label_metadata,\n            iou_thresholds=iou_thresholds,\n            score_thresholds=score_thresholds,\n            index_to_label=self.index_to_label,\n        )\n\n    def compute_confusion_matrix(\n        self,\n        iou_thresholds: list[float],\n        score_thresholds: list[float],\n    ) -&gt; list[Metric]:\n        \"\"\"\n        Computes confusion matrices at various thresholds.\n\n        Parameters\n        ----------\n        iou_thresholds : list[float]\n            A list of IOU thresholds to compute metrics over.\n        score_thresholds : list[float]\n            A list of score thresholds to compute metrics over.\n\n        Returns\n        -------\n        list[Metric]\n            List of confusion matrices per threshold pair.\n        \"\"\"\n        if not iou_thresholds:\n            raise ValueError(\"At least one IOU threshold must be passed.\")\n        elif not score_thresholds:\n            raise ValueError(\"At least one score threshold must be passed.\")\n        elif self.detailed_pairs.size == 0:\n            warnings.warn(\"attempted to compute over an empty set\")\n            return []\n        results = compute_confusion_matrix(\n            detailed_pairs=self.detailed_pairs,\n            iou_thresholds=np.array(iou_thresholds),\n            score_thresholds=np.array(score_thresholds),\n        )\n        return unpack_confusion_matrix_into_metric_list(\n            results=results,\n            detailed_pairs=self.detailed_pairs,\n            iou_thresholds=iou_thresholds,\n            score_thresholds=score_thresholds,\n            index_to_datum_id=self.index_to_datum_id,\n            index_to_groundtruth_id=self.index_to_groundtruth_id,\n            index_to_prediction_id=self.index_to_prediction_id,\n            index_to_label=self.index_to_label,\n        )\n\n    def evaluate(\n        self,\n        iou_thresholds: list[float] = [0.1, 0.5, 0.75],\n        score_thresholds: list[float] = [0.5],\n    ) -&gt; dict[MetricType, list[Metric]]:\n        \"\"\"\n        Computes all available metrics.\n\n        Parameters\n        ----------\n        iou_thresholds : list[float], default=[0.1, 0.5, 0.75]\n            A list of IOU thresholds to compute metrics over.\n        score_thresholds : list[float], default=[0.5]\n            A list of score thresholds to compute metrics over.\n\n        Returns\n        -------\n        dict[MetricType, list[Metric]]\n            Lists of metrics organized by metric type.\n        \"\"\"\n        metrics = self.compute_precision_recall(\n            iou_thresholds=iou_thresholds,\n            score_thresholds=score_thresholds,\n        )\n        metrics[MetricType.ConfusionMatrix] = self.compute_confusion_matrix(\n            iou_thresholds=iou_thresholds,\n            score_thresholds=score_thresholds,\n        )\n        return metrics\n\n    def _add_datum(self, datum_id: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a datum to the cache.\n\n        Parameters\n        ----------\n        datum_id : str\n            The datum identifier.\n\n        Returns\n        -------\n        int\n            The datum index.\n        \"\"\"\n        if datum_id not in self.datum_id_to_index:\n            if len(self.datum_id_to_index) != len(self.index_to_datum_id):\n                raise RuntimeError(\"datum cache size mismatch\")\n            idx = len(self.datum_id_to_index)\n            self.datum_id_to_index[datum_id] = idx\n            self.index_to_datum_id.append(datum_id)\n        return self.datum_id_to_index[datum_id]\n\n    def _add_groundtruth(self, annotation_id: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a ground truth annotation identifier to the cache.\n\n        Parameters\n        ----------\n        annotation_id : str\n            The ground truth annotation identifier.\n\n        Returns\n        -------\n        int\n            The ground truth annotation index.\n        \"\"\"\n        if annotation_id not in self.groundtruth_id_to_index:\n            if len(self.groundtruth_id_to_index) != len(\n                self.index_to_groundtruth_id\n            ):\n                raise RuntimeError(\"ground truth cache size mismatch\")\n            idx = len(self.groundtruth_id_to_index)\n            self.groundtruth_id_to_index[annotation_id] = idx\n            self.index_to_groundtruth_id.append(annotation_id)\n        return self.groundtruth_id_to_index[annotation_id]\n\n    def _add_prediction(self, annotation_id: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a prediction annotation identifier to the cache.\n\n        Parameters\n        ----------\n        annotation_id : str\n            The prediction annotation identifier.\n\n        Returns\n        -------\n        int\n            The prediction annotation index.\n        \"\"\"\n        if annotation_id not in self.prediction_id_to_index:\n            if len(self.prediction_id_to_index) != len(\n                self.index_to_prediction_id\n            ):\n                raise RuntimeError(\"prediction cache size mismatch\")\n            idx = len(self.prediction_id_to_index)\n            self.prediction_id_to_index[annotation_id] = idx\n            self.index_to_prediction_id.append(annotation_id)\n        return self.prediction_id_to_index[annotation_id]\n\n    def _add_label(self, label: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a label to the cache.\n\n        Parameters\n        ----------\n        label : str\n            The label associated with the annotation.\n\n        Returns\n        -------\n        int\n            Label index.\n        \"\"\"\n        label_id = len(self.index_to_label)\n        if label not in self.label_to_index:\n            if len(self.label_to_index) != len(self.index_to_label):\n                raise RuntimeError(\"label cache size mismatch\")\n            self.label_to_index[label] = label_id\n            self.index_to_label.append(label)\n            label_id += 1\n        return self.label_to_index[label]\n\n    def _add_data(\n        self,\n        detections: list[Detection],\n        detection_ious: list[NDArray[np.float64]],\n        show_progress: bool = False,\n    ):\n        \"\"\"\n        Adds detections to the cache.\n\n        Parameters\n        ----------\n        detections : list[Detection]\n            A list of Detection objects.\n        detection_ious : list[NDArray[np.float64]]\n            A list of arrays containing IOUs per detection.\n        show_progress : bool, default=False\n            Toggle for tqdm progress bar.\n        \"\"\"\n        disable_tqdm = not show_progress\n        for detection, ious in tqdm(\n            zip(detections, detection_ious), disable=disable_tqdm\n        ):\n            # cache labels and annotation pairs\n            pairs = []\n            datum_idx = self._add_datum(detection.uid)\n            if detection.groundtruths:\n                for gidx, gann in enumerate(detection.groundtruths):\n                    groundtruth_idx = self._add_groundtruth(gann.uid)\n                    glabel_idx = self._add_label(gann.labels[0])\n                    if (ious[:, gidx] &lt; 1e-9).all():\n                        pairs.extend(\n                            [\n                                np.array(\n                                    [\n                                        float(datum_idx),\n                                        float(groundtruth_idx),\n                                        -1.0,\n                                        float(glabel_idx),\n                                        -1.0,\n                                        0.0,\n                                        -1.0,\n                                    ]\n                                )\n                            ]\n                        )\n                    for pidx, pann in enumerate(detection.predictions):\n                        prediction_idx = self._add_prediction(pann.uid)\n                        if (ious[pidx, :] &lt; 1e-9).all():\n                            pairs.extend(\n                                [\n                                    np.array(\n                                        [\n                                            float(datum_idx),\n                                            -1.0,\n                                            float(prediction_idx),\n                                            -1.0,\n                                            float(self._add_label(plabel)),\n                                            0.0,\n                                            float(pscore),\n                                        ]\n                                    )\n                                    for plabel, pscore in zip(\n                                        pann.labels, pann.scores\n                                    )\n                                ]\n                            )\n                        if ious[pidx, gidx] &gt;= 1e-9:\n                            pairs.extend(\n                                [\n                                    np.array(\n                                        [\n                                            float(datum_idx),\n                                            float(groundtruth_idx),\n                                            float(prediction_idx),\n                                            float(self._add_label(glabel)),\n                                            float(self._add_label(plabel)),\n                                            ious[pidx, gidx],\n                                            float(pscore),\n                                        ]\n                                    )\n                                    for glabel in gann.labels\n                                    for plabel, pscore in zip(\n                                        pann.labels, pann.scores\n                                    )\n                                ]\n                            )\n            elif detection.predictions:\n                for pidx, pann in enumerate(detection.predictions):\n                    prediction_idx = self._add_prediction(pann.uid)\n                    pairs.extend(\n                        [\n                            np.array(\n                                [\n                                    float(datum_idx),\n                                    -1.0,\n                                    float(prediction_idx),\n                                    -1.0,\n                                    float(self._add_label(plabel)),\n                                    0.0,\n                                    float(pscore),\n                                ]\n                            )\n                            for plabel, pscore in zip(pann.labels, pann.scores)\n                        ]\n                    )\n\n            data = np.array(pairs)\n            if data.size &gt; 0:\n                # reset filtered cache if it exists\n                self.clear_filter()\n                if self._temp_cache is None:\n                    raise RuntimeError(\n                        \"cannot add data as evaluator has already been finalized\"\n                    )\n                self._temp_cache.append(data)\n\n    def add_bounding_boxes(\n        self,\n        detections: list[Detection[BoundingBox]],\n        show_progress: bool = False,\n    ):\n        \"\"\"\n        Adds bounding box detections to the cache.\n\n        Parameters\n        ----------\n        detections : list[Detection]\n            A list of Detection objects.\n        show_progress : bool, default=False\n            Toggle for tqdm progress bar.\n        \"\"\"\n        ious = [\n            compute_bbox_iou(\n                np.array(\n                    [\n                        [gt.extrema, pd.extrema]\n                        for pd in detection.predictions\n                        for gt in detection.groundtruths\n                    ],\n                    dtype=np.float64,\n                )\n            ).reshape(len(detection.predictions), len(detection.groundtruths))\n            for detection in detections\n        ]\n        return self._add_data(\n            detections=detections,\n            detection_ious=ious,\n            show_progress=show_progress,\n        )\n\n    def add_polygons(\n        self,\n        detections: list[Detection[Polygon]],\n        show_progress: bool = False,\n    ):\n        \"\"\"\n        Adds polygon detections to the cache.\n\n        Parameters\n        ----------\n        detections : list[Detection]\n            A list of Detection objects.\n        show_progress : bool, default=False\n            Toggle for tqdm progress bar.\n        \"\"\"\n        ious = [\n            compute_polygon_iou(\n                np.array(\n                    [\n                        [gt.shape, pd.shape]\n                        for pd in detection.predictions\n                        for gt in detection.groundtruths\n                    ]\n                )\n            ).reshape(len(detection.predictions), len(detection.groundtruths))\n            for detection in detections\n        ]\n        return self._add_data(\n            detections=detections,\n            detection_ious=ious,\n            show_progress=show_progress,\n        )\n\n    def add_bitmasks(\n        self,\n        detections: list[Detection[Bitmask]],\n        show_progress: bool = False,\n    ):\n        \"\"\"\n        Adds bitmask detections to the cache.\n\n        Parameters\n        ----------\n        detections : list[Detection]\n            A list of Detection objects.\n        show_progress : bool, default=False\n            Toggle for tqdm progress bar.\n        \"\"\"\n        ious = [\n            compute_bitmask_iou(\n                np.array(\n                    [\n                        [gt.mask, pd.mask]\n                        for pd in detection.predictions\n                        for gt in detection.groundtruths\n                    ]\n                )\n            ).reshape(len(detection.predictions), len(detection.groundtruths))\n            for detection in detections\n        ]\n        return self._add_data(\n            detections=detections,\n            detection_ious=ious,\n            show_progress=show_progress,\n        )\n\n    def finalize(self):\n        \"\"\"\n        Performs data finalization and some preprocessing steps.\n\n        Returns\n        -------\n        Evaluator\n            A ready-to-use evaluator object.\n        \"\"\"\n        if self._temp_cache is None:\n            warnings.warn(\"evaluator is already finalized or in a bad state\")\n            return self\n        elif not self._temp_cache:\n            self._detailed_pairs = np.array([], dtype=np.float64)\n            self._ranked_pairs = np.array([], dtype=np.float64)\n            self._label_metadata = np.zeros((self.n_labels, 2), dtype=np.int32)\n            warnings.warn(\"no valid pairs\")\n            return self\n        else:\n            self._detailed_pairs = np.concatenate(self._temp_cache, axis=0)\n            self._temp_cache = None\n\n        # order pairs by descending score, iou\n        indices = np.lexsort(\n            (\n                -self._detailed_pairs[:, 5],  # iou\n                -self._detailed_pairs[:, 6],  # score\n            )\n        )\n        self._detailed_pairs = self._detailed_pairs[indices]\n        self._label_metadata = compute_label_metadata(\n            ids=self._detailed_pairs[:, :5].astype(np.int32),\n            n_labels=self.n_labels,\n        )\n        self._ranked_pairs = rank_pairs(\n            detailed_pairs=self.detailed_pairs,\n            label_metadata=self._label_metadata,\n        )\n        return self\n\n    def apply_filter(\n        self,\n        datum_ids: list[str] | None = None,\n        groundtruth_ids: list[str] | None = None,\n        prediction_ids: list[str] | None = None,\n        labels: list[str] | None = None,\n    ):\n        \"\"\"\n        Apply a filter on the evaluator.\n\n        Can be reset by calling 'clear_filter'.\n\n        Parameters\n        ----------\n        datum_uids : list[str], optional\n            An optional list of string uids representing datums.\n        groundtruth_ids : list[str], optional\n            An optional list of string uids representing ground truth annotations.\n        prediction_ids : list[str], optional\n            An optional list of string uids representing prediction annotations.\n        labels : list[str], optional\n            An optional list of labels.\n        \"\"\"\n        self._filtered_detailed_pairs = self._detailed_pairs.copy()\n        self._filtered_ranked_pairs = np.array([], dtype=np.float64)\n        self._filtered_label_metadata = np.zeros(\n            (self.n_labels, 2), dtype=np.int32\n        )\n\n        valid_datum_indices = None\n        if datum_ids is not None:\n            if not datum_ids:\n                self._filtered_detailed_pairs = np.array([], dtype=np.float64)\n                warnings.warn(\"no valid filtered pairs\")\n                return\n            valid_datum_indices = np.array(\n                [self.datum_id_to_index[uid] for uid in datum_ids],\n                dtype=np.int32,\n            )\n\n        valid_groundtruth_indices = None\n        if groundtruth_ids is not None:\n            valid_groundtruth_indices = np.array(\n                [self.groundtruth_id_to_index[uid] for uid in groundtruth_ids],\n                dtype=np.int32,\n            )\n\n        valid_prediction_indices = None\n        if prediction_ids is not None:\n            valid_prediction_indices = np.array(\n                [self.prediction_id_to_index[uid] for uid in prediction_ids],\n                dtype=np.int32,\n            )\n\n        valid_label_indices = None\n        if labels is not None:\n            if not labels:\n                self._filtered_detailed_pairs = np.array([], dtype=np.float64)\n                warnings.warn(\"no valid filtered pairs\")\n                return\n            valid_label_indices = np.array(\n                [self.label_to_index[label] for label in labels] + [-1]\n            )\n\n        # filter datums\n        if valid_datum_indices is not None:\n            mask_valid_datums = np.isin(\n                self._filtered_detailed_pairs[:, 0], valid_datum_indices\n            )\n            self._filtered_detailed_pairs = self._filtered_detailed_pairs[\n                mask_valid_datums\n            ]\n\n        n_rows = self._filtered_detailed_pairs.shape[0]\n        mask_invalid_groundtruths = np.zeros(n_rows, dtype=np.bool_)\n        mask_invalid_predictions = np.zeros_like(mask_invalid_groundtruths)\n\n        # filter ground truth annotations\n        if valid_groundtruth_indices is not None:\n            mask_invalid_groundtruths[\n                ~np.isin(\n                    self._filtered_detailed_pairs[:, 1],\n                    valid_groundtruth_indices,\n                )\n            ] = True\n\n        # filter prediction annotations\n        if valid_prediction_indices is not None:\n            mask_invalid_predictions[\n                ~np.isin(\n                    self._filtered_detailed_pairs[:, 2],\n                    valid_prediction_indices,\n                )\n            ] = True\n\n        # filter labels\n        if valid_label_indices is not None:\n            mask_invalid_groundtruths[\n                ~np.isin(\n                    self._filtered_detailed_pairs[:, 3], valid_label_indices\n                )\n            ] = True\n            mask_invalid_predictions[\n                ~np.isin(\n                    self._filtered_detailed_pairs[:, 4], valid_label_indices\n                )\n            ] = True\n\n        # filter cache\n        if mask_invalid_groundtruths.any():\n            invalid_groundtruth_indices = np.where(mask_invalid_groundtruths)[\n                0\n            ]\n            self._filtered_detailed_pairs[\n                invalid_groundtruth_indices[:, None], (1, 3, 5)\n            ] = np.array([[-1, -1, 0]])\n\n        if mask_invalid_predictions.any():\n            invalid_prediction_indices = np.where(mask_invalid_predictions)[0]\n            self._filtered_detailed_pairs[\n                invalid_prediction_indices[:, None], (2, 4, 5, 6)\n            ] = np.array([[-1, -1, 0, -1]])\n\n        # filter null pairs\n        mask_null_pairs = np.all(\n            np.isclose(\n                self._filtered_detailed_pairs[:, 1:5],\n                np.array([-1.0, -1.0, -1.0, -1.0]),\n            ),\n            axis=1,\n        )\n        self._filtered_detailed_pairs = self._filtered_detailed_pairs[\n            ~mask_null_pairs\n        ]\n\n        if self._filtered_detailed_pairs.size == 0:\n            self._ranked_pairs = np.array([], dtype=np.float64)\n            self._label_metadata = np.zeros((self.n_labels, 2), dtype=np.int32)\n            warnings.warn(\"no valid filtered pairs\")\n            return\n\n        # sorts by score, iou with ground truth id as a tie-breaker\n        indices = np.lexsort(\n            (\n                self._filtered_detailed_pairs[:, 1],  # ground truth id\n                -self._filtered_detailed_pairs[:, 5],  # iou\n                -self._filtered_detailed_pairs[:, 6],  # score\n            )\n        )\n        self._filtered_detailed_pairs = self._filtered_detailed_pairs[indices]\n        self._filtered_label_metadata = compute_label_metadata(\n            ids=self._filtered_detailed_pairs[:, :5].astype(np.int32),\n            n_labels=self.n_labels,\n        )\n        self._filtered_ranked_pairs = rank_pairs(\n            detailed_pairs=self._filtered_detailed_pairs,\n            label_metadata=self._filtered_label_metadata,\n        )\n\n    def clear_filter(self):\n        \"\"\"Removes a filter if one exists.\"\"\"\n        self._filtered_detailed_pairs = None\n        self._filtered_ranked_pairs = None\n        self._filtered_label_metadata = None\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.ignored_prediction_labels","title":"<code>ignored_prediction_labels</code>  <code>property</code>","text":"<p>Prediction labels that are not present in the ground truth set.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Evaluation metadata.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.missing_prediction_labels","title":"<code>missing_prediction_labels</code>  <code>property</code>","text":"<p>Ground truth labels that are not present in the prediction set.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.n_datums","title":"<code>n_datums</code>  <code>property</code>","text":"<p>Returns the number of datums.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.n_groundtruths","title":"<code>n_groundtruths</code>  <code>property</code>","text":"<p>Returns the number of ground truth annotations.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.n_labels","title":"<code>n_labels</code>  <code>property</code>","text":"<p>Returns the total number of unique labels.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.n_predictions","title":"<code>n_predictions</code>  <code>property</code>","text":"<p>Returns the number of prediction annotations.</p>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.add_bitmasks","title":"<code>add_bitmasks(detections, show_progress=False)</code>","text":"<p>Adds bitmask detections to the cache.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>list[Detection]</code> <p>A list of Detection objects.</p> required <code>show_progress</code> <code>bool</code> <p>Toggle for tqdm progress bar.</p> <code>False</code> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def add_bitmasks(\n    self,\n    detections: list[Detection[Bitmask]],\n    show_progress: bool = False,\n):\n    \"\"\"\n    Adds bitmask detections to the cache.\n\n    Parameters\n    ----------\n    detections : list[Detection]\n        A list of Detection objects.\n    show_progress : bool, default=False\n        Toggle for tqdm progress bar.\n    \"\"\"\n    ious = [\n        compute_bitmask_iou(\n            np.array(\n                [\n                    [gt.mask, pd.mask]\n                    for pd in detection.predictions\n                    for gt in detection.groundtruths\n                ]\n            )\n        ).reshape(len(detection.predictions), len(detection.groundtruths))\n        for detection in detections\n    ]\n    return self._add_data(\n        detections=detections,\n        detection_ious=ious,\n        show_progress=show_progress,\n    )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.add_bounding_boxes","title":"<code>add_bounding_boxes(detections, show_progress=False)</code>","text":"<p>Adds bounding box detections to the cache.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>list[Detection]</code> <p>A list of Detection objects.</p> required <code>show_progress</code> <code>bool</code> <p>Toggle for tqdm progress bar.</p> <code>False</code> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def add_bounding_boxes(\n    self,\n    detections: list[Detection[BoundingBox]],\n    show_progress: bool = False,\n):\n    \"\"\"\n    Adds bounding box detections to the cache.\n\n    Parameters\n    ----------\n    detections : list[Detection]\n        A list of Detection objects.\n    show_progress : bool, default=False\n        Toggle for tqdm progress bar.\n    \"\"\"\n    ious = [\n        compute_bbox_iou(\n            np.array(\n                [\n                    [gt.extrema, pd.extrema]\n                    for pd in detection.predictions\n                    for gt in detection.groundtruths\n                ],\n                dtype=np.float64,\n            )\n        ).reshape(len(detection.predictions), len(detection.groundtruths))\n        for detection in detections\n    ]\n    return self._add_data(\n        detections=detections,\n        detection_ious=ious,\n        show_progress=show_progress,\n    )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.add_polygons","title":"<code>add_polygons(detections, show_progress=False)</code>","text":"<p>Adds polygon detections to the cache.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>list[Detection]</code> <p>A list of Detection objects.</p> required <code>show_progress</code> <code>bool</code> <p>Toggle for tqdm progress bar.</p> <code>False</code> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def add_polygons(\n    self,\n    detections: list[Detection[Polygon]],\n    show_progress: bool = False,\n):\n    \"\"\"\n    Adds polygon detections to the cache.\n\n    Parameters\n    ----------\n    detections : list[Detection]\n        A list of Detection objects.\n    show_progress : bool, default=False\n        Toggle for tqdm progress bar.\n    \"\"\"\n    ious = [\n        compute_polygon_iou(\n            np.array(\n                [\n                    [gt.shape, pd.shape]\n                    for pd in detection.predictions\n                    for gt in detection.groundtruths\n                ]\n            )\n        ).reshape(len(detection.predictions), len(detection.groundtruths))\n        for detection in detections\n    ]\n    return self._add_data(\n        detections=detections,\n        detection_ious=ious,\n        show_progress=show_progress,\n    )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.apply_filter","title":"<code>apply_filter(datum_ids=None, groundtruth_ids=None, prediction_ids=None, labels=None)</code>","text":"<p>Apply a filter on the evaluator.</p> <p>Can be reset by calling 'clear_filter'.</p> <p>Parameters:</p> Name Type Description Default <code>datum_uids</code> <code>list[str]</code> <p>An optional list of string uids representing datums.</p> required <code>groundtruth_ids</code> <code>list[str]</code> <p>An optional list of string uids representing ground truth annotations.</p> <code>None</code> <code>prediction_ids</code> <code>list[str]</code> <p>An optional list of string uids representing prediction annotations.</p> <code>None</code> <code>labels</code> <code>list[str]</code> <p>An optional list of labels.</p> <code>None</code> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def apply_filter(\n    self,\n    datum_ids: list[str] | None = None,\n    groundtruth_ids: list[str] | None = None,\n    prediction_ids: list[str] | None = None,\n    labels: list[str] | None = None,\n):\n    \"\"\"\n    Apply a filter on the evaluator.\n\n    Can be reset by calling 'clear_filter'.\n\n    Parameters\n    ----------\n    datum_uids : list[str], optional\n        An optional list of string uids representing datums.\n    groundtruth_ids : list[str], optional\n        An optional list of string uids representing ground truth annotations.\n    prediction_ids : list[str], optional\n        An optional list of string uids representing prediction annotations.\n    labels : list[str], optional\n        An optional list of labels.\n    \"\"\"\n    self._filtered_detailed_pairs = self._detailed_pairs.copy()\n    self._filtered_ranked_pairs = np.array([], dtype=np.float64)\n    self._filtered_label_metadata = np.zeros(\n        (self.n_labels, 2), dtype=np.int32\n    )\n\n    valid_datum_indices = None\n    if datum_ids is not None:\n        if not datum_ids:\n            self._filtered_detailed_pairs = np.array([], dtype=np.float64)\n            warnings.warn(\"no valid filtered pairs\")\n            return\n        valid_datum_indices = np.array(\n            [self.datum_id_to_index[uid] for uid in datum_ids],\n            dtype=np.int32,\n        )\n\n    valid_groundtruth_indices = None\n    if groundtruth_ids is not None:\n        valid_groundtruth_indices = np.array(\n            [self.groundtruth_id_to_index[uid] for uid in groundtruth_ids],\n            dtype=np.int32,\n        )\n\n    valid_prediction_indices = None\n    if prediction_ids is not None:\n        valid_prediction_indices = np.array(\n            [self.prediction_id_to_index[uid] for uid in prediction_ids],\n            dtype=np.int32,\n        )\n\n    valid_label_indices = None\n    if labels is not None:\n        if not labels:\n            self._filtered_detailed_pairs = np.array([], dtype=np.float64)\n            warnings.warn(\"no valid filtered pairs\")\n            return\n        valid_label_indices = np.array(\n            [self.label_to_index[label] for label in labels] + [-1]\n        )\n\n    # filter datums\n    if valid_datum_indices is not None:\n        mask_valid_datums = np.isin(\n            self._filtered_detailed_pairs[:, 0], valid_datum_indices\n        )\n        self._filtered_detailed_pairs = self._filtered_detailed_pairs[\n            mask_valid_datums\n        ]\n\n    n_rows = self._filtered_detailed_pairs.shape[0]\n    mask_invalid_groundtruths = np.zeros(n_rows, dtype=np.bool_)\n    mask_invalid_predictions = np.zeros_like(mask_invalid_groundtruths)\n\n    # filter ground truth annotations\n    if valid_groundtruth_indices is not None:\n        mask_invalid_groundtruths[\n            ~np.isin(\n                self._filtered_detailed_pairs[:, 1],\n                valid_groundtruth_indices,\n            )\n        ] = True\n\n    # filter prediction annotations\n    if valid_prediction_indices is not None:\n        mask_invalid_predictions[\n            ~np.isin(\n                self._filtered_detailed_pairs[:, 2],\n                valid_prediction_indices,\n            )\n        ] = True\n\n    # filter labels\n    if valid_label_indices is not None:\n        mask_invalid_groundtruths[\n            ~np.isin(\n                self._filtered_detailed_pairs[:, 3], valid_label_indices\n            )\n        ] = True\n        mask_invalid_predictions[\n            ~np.isin(\n                self._filtered_detailed_pairs[:, 4], valid_label_indices\n            )\n        ] = True\n\n    # filter cache\n    if mask_invalid_groundtruths.any():\n        invalid_groundtruth_indices = np.where(mask_invalid_groundtruths)[\n            0\n        ]\n        self._filtered_detailed_pairs[\n            invalid_groundtruth_indices[:, None], (1, 3, 5)\n        ] = np.array([[-1, -1, 0]])\n\n    if mask_invalid_predictions.any():\n        invalid_prediction_indices = np.where(mask_invalid_predictions)[0]\n        self._filtered_detailed_pairs[\n            invalid_prediction_indices[:, None], (2, 4, 5, 6)\n        ] = np.array([[-1, -1, 0, -1]])\n\n    # filter null pairs\n    mask_null_pairs = np.all(\n        np.isclose(\n            self._filtered_detailed_pairs[:, 1:5],\n            np.array([-1.0, -1.0, -1.0, -1.0]),\n        ),\n        axis=1,\n    )\n    self._filtered_detailed_pairs = self._filtered_detailed_pairs[\n        ~mask_null_pairs\n    ]\n\n    if self._filtered_detailed_pairs.size == 0:\n        self._ranked_pairs = np.array([], dtype=np.float64)\n        self._label_metadata = np.zeros((self.n_labels, 2), dtype=np.int32)\n        warnings.warn(\"no valid filtered pairs\")\n        return\n\n    # sorts by score, iou with ground truth id as a tie-breaker\n    indices = np.lexsort(\n        (\n            self._filtered_detailed_pairs[:, 1],  # ground truth id\n            -self._filtered_detailed_pairs[:, 5],  # iou\n            -self._filtered_detailed_pairs[:, 6],  # score\n        )\n    )\n    self._filtered_detailed_pairs = self._filtered_detailed_pairs[indices]\n    self._filtered_label_metadata = compute_label_metadata(\n        ids=self._filtered_detailed_pairs[:, :5].astype(np.int32),\n        n_labels=self.n_labels,\n    )\n    self._filtered_ranked_pairs = rank_pairs(\n        detailed_pairs=self._filtered_detailed_pairs,\n        label_metadata=self._filtered_label_metadata,\n    )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.clear_filter","title":"<code>clear_filter()</code>","text":"<p>Removes a filter if one exists.</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def clear_filter(self):\n    \"\"\"Removes a filter if one exists.\"\"\"\n    self._filtered_detailed_pairs = None\n    self._filtered_ranked_pairs = None\n    self._filtered_label_metadata = None\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.compute_confusion_matrix","title":"<code>compute_confusion_matrix(iou_thresholds, score_thresholds)</code>","text":"<p>Computes confusion matrices at various thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>iou_thresholds</code> <code>list[float]</code> <p>A list of IOU thresholds to compute metrics over.</p> required <code>score_thresholds</code> <code>list[float]</code> <p>A list of score thresholds to compute metrics over.</p> required <p>Returns:</p> Type Description <code>list[Metric]</code> <p>List of confusion matrices per threshold pair.</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def compute_confusion_matrix(\n    self,\n    iou_thresholds: list[float],\n    score_thresholds: list[float],\n) -&gt; list[Metric]:\n    \"\"\"\n    Computes confusion matrices at various thresholds.\n\n    Parameters\n    ----------\n    iou_thresholds : list[float]\n        A list of IOU thresholds to compute metrics over.\n    score_thresholds : list[float]\n        A list of score thresholds to compute metrics over.\n\n    Returns\n    -------\n    list[Metric]\n        List of confusion matrices per threshold pair.\n    \"\"\"\n    if not iou_thresholds:\n        raise ValueError(\"At least one IOU threshold must be passed.\")\n    elif not score_thresholds:\n        raise ValueError(\"At least one score threshold must be passed.\")\n    elif self.detailed_pairs.size == 0:\n        warnings.warn(\"attempted to compute over an empty set\")\n        return []\n    results = compute_confusion_matrix(\n        detailed_pairs=self.detailed_pairs,\n        iou_thresholds=np.array(iou_thresholds),\n        score_thresholds=np.array(score_thresholds),\n    )\n    return unpack_confusion_matrix_into_metric_list(\n        results=results,\n        detailed_pairs=self.detailed_pairs,\n        iou_thresholds=iou_thresholds,\n        score_thresholds=score_thresholds,\n        index_to_datum_id=self.index_to_datum_id,\n        index_to_groundtruth_id=self.index_to_groundtruth_id,\n        index_to_prediction_id=self.index_to_prediction_id,\n        index_to_label=self.index_to_label,\n    )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.compute_precision_recall","title":"<code>compute_precision_recall(iou_thresholds, score_thresholds)</code>","text":"<p>Computes all metrics except for ConfusionMatrix</p> <p>Parameters:</p> Name Type Description Default <code>iou_thresholds</code> <code>list[float]</code> <p>A list of IOU thresholds to compute metrics over.</p> required <code>score_thresholds</code> <code>list[float]</code> <p>A list of score thresholds to compute metrics over.</p> required <p>Returns:</p> Type Description <code>dict[MetricType, list]</code> <p>A dictionary mapping MetricType enumerations to lists of computed metrics.</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def compute_precision_recall(\n    self,\n    iou_thresholds: list[float],\n    score_thresholds: list[float],\n) -&gt; dict[MetricType, list[Metric]]:\n    \"\"\"\n    Computes all metrics except for ConfusionMatrix\n\n    Parameters\n    ----------\n    iou_thresholds : list[float]\n        A list of IOU thresholds to compute metrics over.\n    score_thresholds : list[float]\n        A list of score thresholds to compute metrics over.\n\n    Returns\n    -------\n    dict[MetricType, list]\n        A dictionary mapping MetricType enumerations to lists of computed metrics.\n    \"\"\"\n    if not iou_thresholds:\n        raise ValueError(\"At least one IOU threshold must be passed.\")\n    elif not score_thresholds:\n        raise ValueError(\"At least one score threshold must be passed.\")\n    results = compute_precion_recall(\n        ranked_pairs=self.ranked_pairs,\n        label_metadata=self.label_metadata,\n        iou_thresholds=np.array(iou_thresholds),\n        score_thresholds=np.array(score_thresholds),\n    )\n    return unpack_precision_recall_into_metric_lists(\n        results=results,\n        label_metadata=self.label_metadata,\n        iou_thresholds=iou_thresholds,\n        score_thresholds=score_thresholds,\n        index_to_label=self.index_to_label,\n    )\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.evaluate","title":"<code>evaluate(iou_thresholds=[0.1, 0.5, 0.75], score_thresholds=[0.5])</code>","text":"<p>Computes all available metrics.</p> <p>Parameters:</p> Name Type Description Default <code>iou_thresholds</code> <code>list[float]</code> <p>A list of IOU thresholds to compute metrics over.</p> <code>[0.1, 0.5, 0.75]</code> <code>score_thresholds</code> <code>list[float]</code> <p>A list of score thresholds to compute metrics over.</p> <code>[0.5]</code> <p>Returns:</p> Type Description <code>dict[MetricType, list[Metric]]</code> <p>Lists of metrics organized by metric type.</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def evaluate(\n    self,\n    iou_thresholds: list[float] = [0.1, 0.5, 0.75],\n    score_thresholds: list[float] = [0.5],\n) -&gt; dict[MetricType, list[Metric]]:\n    \"\"\"\n    Computes all available metrics.\n\n    Parameters\n    ----------\n    iou_thresholds : list[float], default=[0.1, 0.5, 0.75]\n        A list of IOU thresholds to compute metrics over.\n    score_thresholds : list[float], default=[0.5]\n        A list of score thresholds to compute metrics over.\n\n    Returns\n    -------\n    dict[MetricType, list[Metric]]\n        Lists of metrics organized by metric type.\n    \"\"\"\n    metrics = self.compute_precision_recall(\n        iou_thresholds=iou_thresholds,\n        score_thresholds=score_thresholds,\n    )\n    metrics[MetricType.ConfusionMatrix] = self.compute_confusion_matrix(\n        iou_thresholds=iou_thresholds,\n        score_thresholds=score_thresholds,\n    )\n    return metrics\n</code></pre>"},{"location":"object_detection/documentation/#valor_lite.object_detection.Evaluator.finalize","title":"<code>finalize()</code>","text":"<p>Performs data finalization and some preprocessing steps.</p> <p>Returns:</p> Type Description <code>Evaluator</code> <p>A ready-to-use evaluator object.</p> Source code in <code>valor_lite/object_detection/manager.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Performs data finalization and some preprocessing steps.\n\n    Returns\n    -------\n    Evaluator\n        A ready-to-use evaluator object.\n    \"\"\"\n    if self._temp_cache is None:\n        warnings.warn(\"evaluator is already finalized or in a bad state\")\n        return self\n    elif not self._temp_cache:\n        self._detailed_pairs = np.array([], dtype=np.float64)\n        self._ranked_pairs = np.array([], dtype=np.float64)\n        self._label_metadata = np.zeros((self.n_labels, 2), dtype=np.int32)\n        warnings.warn(\"no valid pairs\")\n        return self\n    else:\n        self._detailed_pairs = np.concatenate(self._temp_cache, axis=0)\n        self._temp_cache = None\n\n    # order pairs by descending score, iou\n    indices = np.lexsort(\n        (\n            -self._detailed_pairs[:, 5],  # iou\n            -self._detailed_pairs[:, 6],  # score\n        )\n    )\n    self._detailed_pairs = self._detailed_pairs[indices]\n    self._label_metadata = compute_label_metadata(\n        ids=self._detailed_pairs[:, :5].astype(np.int32),\n        n_labels=self.n_labels,\n    )\n    self._ranked_pairs = rank_pairs(\n        detailed_pairs=self.detailed_pairs,\n        label_metadata=self._label_metadata,\n    )\n    return self\n</code></pre>"},{"location":"object_detection/metrics/","title":"Metrics","text":""},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Object Detection Metric.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The metric type.</p> <code>value</code> <code>int | float | dict</code> <p>The metric value.</p> <code>parameters</code> <code>dict[str, Any]</code> <p>A dictionary containing metric parameters.</p> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@dataclass\nclass Metric(BaseMetric):\n    \"\"\"\n    Object Detection Metric.\n\n    Attributes\n    ----------\n    type : str\n        The metric type.\n    value : int | float | dict\n        The metric value.\n    parameters : dict[str, Any]\n        A dictionary containing metric parameters.\n    \"\"\"\n\n    def __post_init__(self):\n        if not isinstance(self.type, str):\n            raise TypeError(\n                f\"Metric type should be of type 'str': {self.type}\"\n            )\n        elif not isinstance(self.value, (int, float, dict)):\n            raise TypeError(\n                f\"Metric value must be of type 'int', 'float' or 'dict': {self.value}\"\n            )\n        elif not isinstance(self.parameters, dict):\n            raise TypeError(\n                f\"Metric parameters must be of type 'dict[str, Any]': {self.parameters}\"\n            )\n        elif not all([isinstance(k, str) for k in self.parameters.keys()]):\n            raise TypeError(\n                f\"Metric parameter dictionary should only have keys with type 'str': {self.parameters}\"\n            )\n\n    @classmethod\n    def precision(\n        cls,\n        value: float,\n        label: str,\n        iou_threshold: float,\n        score_threshold: float,\n    ):\n        \"\"\"\n        Precision metric for a specific class label in object detection.\n\n        This class encapsulates a metric value for a particular class label,\n        along with the associated Intersection over Union (IOU) threshold and\n        confidence score threshold.\n\n        Parameters\n        ----------\n        value : float\n            The metric value.\n        label : str\n            The class label for which the metric is calculated.\n        iou_threshold : float\n            The IOU threshold used to determine matches between predicted and ground truth boxes.\n        score_threshold : float\n            The confidence score threshold above which predictions are considered.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Precision.value,\n            value=value,\n            parameters={\n                \"label\": label,\n                \"iou_threshold\": iou_threshold,\n                \"score_threshold\": score_threshold,\n            },\n        )\n\n    @classmethod\n    def recall(\n        cls,\n        value: float,\n        label: str,\n        iou_threshold: float,\n        score_threshold: float,\n    ):\n        \"\"\"\n        Recall metric for a specific class label in object detection.\n\n        This class encapsulates a metric value for a particular class label,\n        along with the associated Intersection over Union (IOU) threshold and\n        confidence score threshold.\n\n        Parameters\n        ----------\n        value : float\n            The metric value.\n        label : str\n            The class label for which the metric is calculated.\n        iou_threshold : float\n            The IOU threshold used to determine matches between predicted and ground truth boxes.\n        score_threshold : float\n            The confidence score threshold above which predictions are considered.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Recall.value,\n            value=value,\n            parameters={\n                \"label\": label,\n                \"iou_threshold\": iou_threshold,\n                \"score_threshold\": score_threshold,\n            },\n        )\n\n    @classmethod\n    def f1_score(\n        cls,\n        value: float,\n        label: str,\n        iou_threshold: float,\n        score_threshold: float,\n    ):\n        \"\"\"\n        F1 score for a specific class label in object detection.\n\n        This class encapsulates a metric value for a particular class label,\n        along with the associated Intersection over Union (IOU) threshold and\n        confidence score threshold.\n\n        Parameters\n        ----------\n        value : float\n            The metric value.\n        label : str\n            The class label for which the metric is calculated.\n        iou_threshold : float\n            The IOU threshold used to determine matches between predicted and ground truth boxes.\n        score_threshold : float\n            The confidence score threshold above which predictions are considered.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.F1.value,\n            value=value,\n            parameters={\n                \"label\": label,\n                \"iou_threshold\": iou_threshold,\n                \"score_threshold\": score_threshold,\n            },\n        )\n\n    @classmethod\n    def average_precision(\n        cls,\n        value: float,\n        iou_threshold: float,\n        label: str,\n    ):\n        \"\"\"\n        Average Precision (AP) metric for object detection tasks.\n\n        The AP computation uses 101-point interpolation, which calculates the average\n        precision by interpolating the precision-recall curve at 101 evenly spaced recall\n        levels from 0 to 1.\n\n        Parameters\n        ----------\n        value : float\n            The average precision value.\n        iou_threshold : float\n            The IOU threshold used to compute the AP.\n        label : str\n            The class label for which the AP is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.AP.value,\n            value=value,\n            parameters={\n                \"iou_threshold\": iou_threshold,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def mean_average_precision(\n        cls,\n        value: float,\n        iou_threshold: float,\n    ):\n        \"\"\"\n        Mean Average Precision (mAP) metric for object detection tasks.\n\n        The AP computation uses 101-point interpolation, which calculates the average\n        precision for each class by interpolating the precision-recall curve at 101 evenly\n        spaced recall levels from 0 to 1. The mAP is then calculated by averaging these\n        values across all class labels.\n\n        Parameters\n        ----------\n        value : float\n            The mean average precision value.\n        iou_threshold : float\n            The IOU threshold used to compute the mAP.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.mAP.value,\n            value=value,\n            parameters={\n                \"iou_threshold\": iou_threshold,\n            },\n        )\n\n    @classmethod\n    def average_precision_averaged_over_IOUs(\n        cls,\n        value: float,\n        iou_thresholds: list[float],\n        label: str,\n    ):\n        \"\"\"\n        Average Precision (AP) metric averaged over multiple IOU thresholds.\n\n        The AP computation uses 101-point interpolation, which calculates the average precision\n        by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1\n        for each IOU threshold specified in `iou_thresholds`. The final APAveragedOverIOUs value is\n        obtained by averaging these AP values across all specified IOU thresholds.\n\n        Parameters\n        ----------\n        value : float\n            The average precision value averaged over the specified IOU thresholds.\n        iou_thresholds : list[float]\n            The list of IOU thresholds used to compute the AP values.\n        label : str\n            The class label for which the AP is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.APAveragedOverIOUs.value,\n            value=value,\n            parameters={\n                \"iou_thresholds\": iou_thresholds,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def mean_average_precision_averaged_over_IOUs(\n        cls,\n        value: float,\n        iou_thresholds: list[float],\n    ):\n        \"\"\"\n        Mean Average Precision (mAP) metric averaged over multiple IOU thresholds.\n\n        The AP computation uses 101-point interpolation, which calculates the average precision\n        by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1\n        for each IOU threshold specified in `iou_thresholds`. The final mAPAveragedOverIOUs value is\n        obtained by averaging these AP values across all specified IOU thresholds and all class labels.\n\n        Parameters\n        ----------\n        value : float\n            The average precision value averaged over the specified IOU thresholds.\n        iou_thresholds : list[float]\n            The list of IOU thresholds used to compute the AP values.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.mAPAveragedOverIOUs.value,\n            value=value,\n            parameters={\n                \"iou_thresholds\": iou_thresholds,\n            },\n        )\n\n    @classmethod\n    def average_recall(\n        cls,\n        value: float,\n        score_threshold: float,\n        iou_thresholds: list[float],\n        label: str,\n    ):\n        \"\"\"\n        Average Recall (AR) metric for object detection tasks.\n\n        The AR computation considers detections with confidence scores above the specified\n        `score_threshold` and calculates the recall at each IOU threshold in `iou_thresholds`.\n        The final AR value is the average of these recall values across all specified IOU\n        thresholds.\n\n        Parameters\n        ----------\n        value : float\n            The average recall value averaged over the specified IOU thresholds.\n        score_threshold : float\n            The detection score threshold; only detections with confidence scores above this\n            threshold are considered.\n        iou_thresholds : list[float]\n            The list of IOU thresholds used to compute the recall values.\n        label : str\n            The class label for which the AR is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.AR.value,\n            value=value,\n            parameters={\n                \"iou_thresholds\": iou_thresholds,\n                \"score_threshold\": score_threshold,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def mean_average_recall(\n        cls,\n        value: float,\n        score_threshold: float,\n        iou_thresholds: list[float],\n    ):\n        \"\"\"\n        Mean Average Recall (mAR) metric for object detection tasks.\n\n        The mAR computation considers detections with confidence scores above the specified\n        `score_threshold` and calculates recall at each IOU threshold in `iou_thresholds` for\n        each label. The final mAR value is obtained by averaging these recall values over the\n        specified IOU thresholds and then averaging across all labels.\n\n        Parameters\n        ----------\n        value : float\n            The mean average recall value averaged over the specified IOU thresholds.\n        score_threshold : float\n            The detection score threshold; only detections with confidence scores above this\n            threshold are considered.\n        iou_thresholds : list[float]\n            The list of IOU thresholds used to compute the recall values.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.mAR.value,\n            value=value,\n            parameters={\n                \"iou_thresholds\": iou_thresholds,\n                \"score_threshold\": score_threshold,\n            },\n        )\n\n    @classmethod\n    def average_recall_averaged_over_scores(\n        cls,\n        value: float,\n        score_thresholds: list[float],\n        iou_thresholds: list[float],\n        label: str,\n    ):\n        \"\"\"\n        Average Recall (AR) metric averaged over multiple score thresholds for a specific object class label.\n\n        The AR computation considers detections across multiple `score_thresholds` and calculates\n        recall at each IOU threshold in `iou_thresholds`. The final AR value is obtained by averaging\n        the recall values over all specified score thresholds and IOU thresholds.\n\n        Parameters\n        ----------\n        value : float\n            The average recall value averaged over the specified score thresholds and IOU thresholds.\n        score_thresholds : list[float]\n            The list of detection score thresholds; detections with confidence scores above each threshold are considered.\n        iou_thresholds : list[float]\n            The list of IOU thresholds used to compute the recall values.\n        label : str\n            The class label for which the AR is computed.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.ARAveragedOverScores.value,\n            value=value,\n            parameters={\n                \"iou_thresholds\": iou_thresholds,\n                \"score_thresholds\": score_thresholds,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def mean_average_recall_averaged_over_scores(\n        cls,\n        value: float,\n        score_thresholds: list[float],\n        iou_thresholds: list[float],\n    ):\n        \"\"\"\n        Mean Average Recall (mAR) metric averaged over multiple score thresholds and IOU thresholds.\n\n        The mAR computation considers detections across multiple `score_thresholds`, calculates recall\n        at each IOU threshold in `iou_thresholds` for each label, averages these recall values over all\n        specified score thresholds and IOU thresholds, and then computes the mean across all labels to\n        obtain the final mAR value.\n\n        Parameters\n        ----------\n        value : float\n            The mean average recall value averaged over the specified score thresholds and IOU thresholds.\n        score_thresholds : list[float]\n            The list of detection score thresholds; detections with confidence scores above each threshold are considered.\n        iou_thresholds : list[float]\n            The list of IOU thresholds used to compute the recall values.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.mARAveragedOverScores.value,\n            value=value,\n            parameters={\n                \"iou_thresholds\": iou_thresholds,\n                \"score_thresholds\": score_thresholds,\n            },\n        )\n\n    @classmethod\n    def precision_recall_curve(\n        cls,\n        precisions: list[float],\n        scores: list[float],\n        iou_threshold: float,\n        label: str,\n    ):\n        \"\"\"\n        Interpolated precision-recall curve over 101 recall points.\n\n        The precision values are interpolated over recalls ranging from 0.0 to 1.0 in steps of 0.01,\n        resulting in 101 points. This is a byproduct of the 101-point interpolation used in calculating\n        the Average Precision (AP) metric in object detection tasks.\n\n        Parameters\n        ----------\n        precisions : list[float]\n            Interpolated precision values corresponding to recalls at 0.0, 0.01, ..., 1.0.\n        scores : list[float]\n            Maximum prediction score for each point on the interpolated curve.\n        iou_threshold : float\n            The Intersection over Union (IOU) threshold used to determine true positives.\n        label : str\n            The class label associated with this precision-recall curve.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.PrecisionRecallCurve.value,\n            value={\n                \"precisions\": precisions,\n                \"scores\": scores,\n            },\n            parameters={\n                \"iou_threshold\": iou_threshold,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def counts(\n        cls,\n        tp: int,\n        fp: int,\n        fn: int,\n        label: str,\n        iou_threshold: float,\n        score_threshold: float,\n    ):\n        \"\"\"\n        `Counts` encapsulates the counts of true positives (`tp`), false positives (`fp`),\n        and false negatives (`fn`) for object detection evaluation, along with the associated\n        class label, Intersection over Union (IOU) threshold, and confidence score threshold.\n\n        Parameters\n        ----------\n        tp : int\n            Number of true positives.\n        fp : int\n            Number of false positives.\n        fn : int\n            Number of false negatives.\n        label : str\n            The class label for which the counts are calculated.\n        iou_threshold : float\n            The IOU threshold used to determine a match between predicted and ground truth boxes.\n        score_threshold : float\n            The confidence score threshold above which predictions are considered.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Counts.value,\n            value={\n                \"tp\": tp,\n                \"fp\": fp,\n                \"fn\": fn,\n            },\n            parameters={\n                \"iou_threshold\": iou_threshold,\n                \"score_threshold\": score_threshold,\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def confusion_matrix(\n        cls,\n        confusion_matrix: dict[\n            str,  # ground truth label value\n            dict[\n                str,  # prediction label value\n                dict[\n                    str,  # either `count` or `examples`\n                    int\n                    | list[\n                        dict[\n                            str,  # either `datum_id`, `ground_truth_id`, `prediction_id`\n                            str,  # string identifier\n                        ]\n                    ],\n                ],\n            ],\n        ],\n        unmatched_predictions: dict[\n            str,  # prediction label value\n            dict[\n                str,  # either `count` or `examples`\n                int\n                | list[\n                    dict[\n                        str,  # either `datum_id` or `prediction_id``\n                        str,  # string identifier\n                    ]\n                ],\n            ],\n        ],\n        unmatched_ground_truths: dict[\n            str,  # ground truth label value\n            dict[\n                str,  # either `count` or `examples`\n                int\n                | list[\n                    dict[\n                        str,  # either `datum_id` or `ground_truth_id`\n                        str,  # string identifier\n                    ]\n                ],\n            ],\n        ],\n        score_threshold: float,\n        iou_threshold: float,\n    ):\n        \"\"\"\n        Confusion matrix for object detection tasks.\n\n        This class encapsulates detailed information about the model's performance, including correct\n        predictions, misclassifications, unmatched_predictions (subset of false positives), and unmatched ground truths\n        (subset of false negatives). It provides counts and examples for each category to facilitate in-depth analysis.\n\n        Confusion Matrix Format:\n        {\n            &lt;ground truth label&gt;: {\n                &lt;prediction label&gt;: {\n                    'count': int,\n                    'examples': [\n                        {\n                            'datum_id': str,\n                            'groundtruth_id': str,\n                            'prediction_id': str\n                        },\n                        ...\n                    ],\n                },\n                ...\n            },\n            ...\n        }\n\n        Unmatched Predictions Format:\n        {\n            &lt;prediction label&gt;: {\n                'count': int,\n                'examples': [\n                    {\n                        'datum_id': str,\n                        'prediction_id': str\n                    },\n                    ...\n                ],\n            },\n            ...\n        }\n\n        Unmatched Ground Truths Format:\n        {\n            &lt;ground truth label&gt;: {\n                'count': int,\n                'examples': [\n                    {\n                        'datum_id': str,\n                        'groundtruth_id': str\n                    },\n                    ...\n                ],\n            },\n            ...\n        }\n\n        Parameters\n        ----------\n        confusion_matrix : dict\n            A nested dictionary where the first key is the ground truth label value, the second key\n            is the prediction label value, and the innermost dictionary contains either a `count`\n            or a list of `examples`. Each example includes annotation and datum identifers.\n        unmatched_predictions : dict\n            A dictionary where each key is a prediction label value with no corresponding ground truth\n            (subset of false positives). The value is a dictionary containing either a `count` or a list of\n            `examples`. Each example includes annotation and datum identifers.\n        unmatched_ground_truths : dict\n            A dictionary where each key is a ground truth label value for which the model failed to predict\n            (subset of false negatives). The value is a dictionary containing either a `count` or a list of `examples`.\n            Each example includes annotation and datum identifers.\n        score_threshold : float\n            The confidence score threshold used to filter predictions.\n        iou_threshold : float\n            The Intersection over Union (IOU) threshold used to determine true positives.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.ConfusionMatrix.value,\n            value={\n                \"confusion_matrix\": confusion_matrix,\n                \"unmatched_predictions\": unmatched_predictions,\n                \"unmatched_ground_truths\": unmatched_ground_truths,\n            },\n            parameters={\n                \"score_threshold\": score_threshold,\n                \"iou_threshold\": iou_threshold,\n            },\n        )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.average_precision","title":"<code>average_precision(value, iou_threshold, label)</code>  <code>classmethod</code>","text":"<p>Average Precision (AP) metric for object detection tasks.</p> <p>The AP computation uses 101-point interpolation, which calculates the average precision by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The average precision value.</p> required <code>iou_threshold</code> <code>float</code> <p>The IOU threshold used to compute the AP.</p> required <code>label</code> <code>str</code> <p>The class label for which the AP is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef average_precision(\n    cls,\n    value: float,\n    iou_threshold: float,\n    label: str,\n):\n    \"\"\"\n    Average Precision (AP) metric for object detection tasks.\n\n    The AP computation uses 101-point interpolation, which calculates the average\n    precision by interpolating the precision-recall curve at 101 evenly spaced recall\n    levels from 0 to 1.\n\n    Parameters\n    ----------\n    value : float\n        The average precision value.\n    iou_threshold : float\n        The IOU threshold used to compute the AP.\n    label : str\n        The class label for which the AP is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.AP.value,\n        value=value,\n        parameters={\n            \"iou_threshold\": iou_threshold,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.average_precision_averaged_over_IOUs","title":"<code>average_precision_averaged_over_IOUs(value, iou_thresholds, label)</code>  <code>classmethod</code>","text":"<p>Average Precision (AP) metric averaged over multiple IOU thresholds.</p> <p>The AP computation uses 101-point interpolation, which calculates the average precision by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1 for each IOU threshold specified in <code>iou_thresholds</code>. The final APAveragedOverIOUs value is obtained by averaging these AP values across all specified IOU thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The average precision value averaged over the specified IOU thresholds.</p> required <code>iou_thresholds</code> <code>list[float]</code> <p>The list of IOU thresholds used to compute the AP values.</p> required <code>label</code> <code>str</code> <p>The class label for which the AP is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef average_precision_averaged_over_IOUs(\n    cls,\n    value: float,\n    iou_thresholds: list[float],\n    label: str,\n):\n    \"\"\"\n    Average Precision (AP) metric averaged over multiple IOU thresholds.\n\n    The AP computation uses 101-point interpolation, which calculates the average precision\n    by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1\n    for each IOU threshold specified in `iou_thresholds`. The final APAveragedOverIOUs value is\n    obtained by averaging these AP values across all specified IOU thresholds.\n\n    Parameters\n    ----------\n    value : float\n        The average precision value averaged over the specified IOU thresholds.\n    iou_thresholds : list[float]\n        The list of IOU thresholds used to compute the AP values.\n    label : str\n        The class label for which the AP is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.APAveragedOverIOUs.value,\n        value=value,\n        parameters={\n            \"iou_thresholds\": iou_thresholds,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.average_recall","title":"<code>average_recall(value, score_threshold, iou_thresholds, label)</code>  <code>classmethod</code>","text":"<p>Average Recall (AR) metric for object detection tasks.</p> <p>The AR computation considers detections with confidence scores above the specified <code>score_threshold</code> and calculates the recall at each IOU threshold in <code>iou_thresholds</code>. The final AR value is the average of these recall values across all specified IOU thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The average recall value averaged over the specified IOU thresholds.</p> required <code>score_threshold</code> <code>float</code> <p>The detection score threshold; only detections with confidence scores above this threshold are considered.</p> required <code>iou_thresholds</code> <code>list[float]</code> <p>The list of IOU thresholds used to compute the recall values.</p> required <code>label</code> <code>str</code> <p>The class label for which the AR is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef average_recall(\n    cls,\n    value: float,\n    score_threshold: float,\n    iou_thresholds: list[float],\n    label: str,\n):\n    \"\"\"\n    Average Recall (AR) metric for object detection tasks.\n\n    The AR computation considers detections with confidence scores above the specified\n    `score_threshold` and calculates the recall at each IOU threshold in `iou_thresholds`.\n    The final AR value is the average of these recall values across all specified IOU\n    thresholds.\n\n    Parameters\n    ----------\n    value : float\n        The average recall value averaged over the specified IOU thresholds.\n    score_threshold : float\n        The detection score threshold; only detections with confidence scores above this\n        threshold are considered.\n    iou_thresholds : list[float]\n        The list of IOU thresholds used to compute the recall values.\n    label : str\n        The class label for which the AR is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.AR.value,\n        value=value,\n        parameters={\n            \"iou_thresholds\": iou_thresholds,\n            \"score_threshold\": score_threshold,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.average_recall_averaged_over_scores","title":"<code>average_recall_averaged_over_scores(value, score_thresholds, iou_thresholds, label)</code>  <code>classmethod</code>","text":"<p>Average Recall (AR) metric averaged over multiple score thresholds for a specific object class label.</p> <p>The AR computation considers detections across multiple <code>score_thresholds</code> and calculates recall at each IOU threshold in <code>iou_thresholds</code>. The final AR value is obtained by averaging the recall values over all specified score thresholds and IOU thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The average recall value averaged over the specified score thresholds and IOU thresholds.</p> required <code>score_thresholds</code> <code>list[float]</code> <p>The list of detection score thresholds; detections with confidence scores above each threshold are considered.</p> required <code>iou_thresholds</code> <code>list[float]</code> <p>The list of IOU thresholds used to compute the recall values.</p> required <code>label</code> <code>str</code> <p>The class label for which the AR is computed.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef average_recall_averaged_over_scores(\n    cls,\n    value: float,\n    score_thresholds: list[float],\n    iou_thresholds: list[float],\n    label: str,\n):\n    \"\"\"\n    Average Recall (AR) metric averaged over multiple score thresholds for a specific object class label.\n\n    The AR computation considers detections across multiple `score_thresholds` and calculates\n    recall at each IOU threshold in `iou_thresholds`. The final AR value is obtained by averaging\n    the recall values over all specified score thresholds and IOU thresholds.\n\n    Parameters\n    ----------\n    value : float\n        The average recall value averaged over the specified score thresholds and IOU thresholds.\n    score_thresholds : list[float]\n        The list of detection score thresholds; detections with confidence scores above each threshold are considered.\n    iou_thresholds : list[float]\n        The list of IOU thresholds used to compute the recall values.\n    label : str\n        The class label for which the AR is computed.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.ARAveragedOverScores.value,\n        value=value,\n        parameters={\n            \"iou_thresholds\": iou_thresholds,\n            \"score_thresholds\": score_thresholds,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.confusion_matrix","title":"<code>confusion_matrix(confusion_matrix, unmatched_predictions, unmatched_ground_truths, score_threshold, iou_threshold)</code>  <code>classmethod</code>","text":"<p>Confusion matrix for object detection tasks.</p> <p>This class encapsulates detailed information about the model's performance, including correct predictions, misclassifications, unmatched_predictions (subset of false positives), and unmatched ground truths (subset of false negatives). It provides counts and examples for each category to facilitate in-depth analysis.</p> <p>Confusion Matrix Format: {     : {         : {             'count': int,             'examples': [                 {                     'datum_id': str,                     'groundtruth_id': str,                     'prediction_id': str                 },                 ...             ],         },         ...     },     ... } <p>Unmatched Predictions Format: {     : {         'count': int,         'examples': [             {                 'datum_id': str,                 'prediction_id': str             },             ...         ],     },     ... } <p>Unmatched Ground Truths Format: {     : {         'count': int,         'examples': [             {                 'datum_id': str,                 'groundtruth_id': str             },             ...         ],     },     ... } <p>Parameters:</p> Name Type Description Default <code>confusion_matrix</code> <code>dict</code> <p>A nested dictionary where the first key is the ground truth label value, the second key is the prediction label value, and the innermost dictionary contains either a <code>count</code> or a list of <code>examples</code>. Each example includes annotation and datum identifers.</p> required <code>unmatched_predictions</code> <code>dict</code> <p>A dictionary where each key is a prediction label value with no corresponding ground truth (subset of false positives). The value is a dictionary containing either a <code>count</code> or a list of <code>examples</code>. Each example includes annotation and datum identifers.</p> required <code>unmatched_ground_truths</code> <code>dict</code> <p>A dictionary where each key is a ground truth label value for which the model failed to predict (subset of false negatives). The value is a dictionary containing either a <code>count</code> or a list of <code>examples</code>. Each example includes annotation and datum identifers.</p> required <code>score_threshold</code> <code>float</code> <p>The confidence score threshold used to filter predictions.</p> required <code>iou_threshold</code> <code>float</code> <p>The Intersection over Union (IOU) threshold used to determine true positives.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef confusion_matrix(\n    cls,\n    confusion_matrix: dict[\n        str,  # ground truth label value\n        dict[\n            str,  # prediction label value\n            dict[\n                str,  # either `count` or `examples`\n                int\n                | list[\n                    dict[\n                        str,  # either `datum_id`, `ground_truth_id`, `prediction_id`\n                        str,  # string identifier\n                    ]\n                ],\n            ],\n        ],\n    ],\n    unmatched_predictions: dict[\n        str,  # prediction label value\n        dict[\n            str,  # either `count` or `examples`\n            int\n            | list[\n                dict[\n                    str,  # either `datum_id` or `prediction_id``\n                    str,  # string identifier\n                ]\n            ],\n        ],\n    ],\n    unmatched_ground_truths: dict[\n        str,  # ground truth label value\n        dict[\n            str,  # either `count` or `examples`\n            int\n            | list[\n                dict[\n                    str,  # either `datum_id` or `ground_truth_id`\n                    str,  # string identifier\n                ]\n            ],\n        ],\n    ],\n    score_threshold: float,\n    iou_threshold: float,\n):\n    \"\"\"\n    Confusion matrix for object detection tasks.\n\n    This class encapsulates detailed information about the model's performance, including correct\n    predictions, misclassifications, unmatched_predictions (subset of false positives), and unmatched ground truths\n    (subset of false negatives). It provides counts and examples for each category to facilitate in-depth analysis.\n\n    Confusion Matrix Format:\n    {\n        &lt;ground truth label&gt;: {\n            &lt;prediction label&gt;: {\n                'count': int,\n                'examples': [\n                    {\n                        'datum_id': str,\n                        'groundtruth_id': str,\n                        'prediction_id': str\n                    },\n                    ...\n                ],\n            },\n            ...\n        },\n        ...\n    }\n\n    Unmatched Predictions Format:\n    {\n        &lt;prediction label&gt;: {\n            'count': int,\n            'examples': [\n                {\n                    'datum_id': str,\n                    'prediction_id': str\n                },\n                ...\n            ],\n        },\n        ...\n    }\n\n    Unmatched Ground Truths Format:\n    {\n        &lt;ground truth label&gt;: {\n            'count': int,\n            'examples': [\n                {\n                    'datum_id': str,\n                    'groundtruth_id': str\n                },\n                ...\n            ],\n        },\n        ...\n    }\n\n    Parameters\n    ----------\n    confusion_matrix : dict\n        A nested dictionary where the first key is the ground truth label value, the second key\n        is the prediction label value, and the innermost dictionary contains either a `count`\n        or a list of `examples`. Each example includes annotation and datum identifers.\n    unmatched_predictions : dict\n        A dictionary where each key is a prediction label value with no corresponding ground truth\n        (subset of false positives). The value is a dictionary containing either a `count` or a list of\n        `examples`. Each example includes annotation and datum identifers.\n    unmatched_ground_truths : dict\n        A dictionary where each key is a ground truth label value for which the model failed to predict\n        (subset of false negatives). The value is a dictionary containing either a `count` or a list of `examples`.\n        Each example includes annotation and datum identifers.\n    score_threshold : float\n        The confidence score threshold used to filter predictions.\n    iou_threshold : float\n        The Intersection over Union (IOU) threshold used to determine true positives.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.ConfusionMatrix.value,\n        value={\n            \"confusion_matrix\": confusion_matrix,\n            \"unmatched_predictions\": unmatched_predictions,\n            \"unmatched_ground_truths\": unmatched_ground_truths,\n        },\n        parameters={\n            \"score_threshold\": score_threshold,\n            \"iou_threshold\": iou_threshold,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.counts","title":"<code>counts(tp, fp, fn, label, iou_threshold, score_threshold)</code>  <code>classmethod</code>","text":"<p><code>Counts</code> encapsulates the counts of true positives (<code>tp</code>), false positives (<code>fp</code>), and false negatives (<code>fn</code>) for object detection evaluation, along with the associated class label, Intersection over Union (IOU) threshold, and confidence score threshold.</p> <p>Parameters:</p> Name Type Description Default <code>tp</code> <code>int</code> <p>Number of true positives.</p> required <code>fp</code> <code>int</code> <p>Number of false positives.</p> required <code>fn</code> <code>int</code> <p>Number of false negatives.</p> required <code>label</code> <code>str</code> <p>The class label for which the counts are calculated.</p> required <code>iou_threshold</code> <code>float</code> <p>The IOU threshold used to determine a match between predicted and ground truth boxes.</p> required <code>score_threshold</code> <code>float</code> <p>The confidence score threshold above which predictions are considered.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef counts(\n    cls,\n    tp: int,\n    fp: int,\n    fn: int,\n    label: str,\n    iou_threshold: float,\n    score_threshold: float,\n):\n    \"\"\"\n    `Counts` encapsulates the counts of true positives (`tp`), false positives (`fp`),\n    and false negatives (`fn`) for object detection evaluation, along with the associated\n    class label, Intersection over Union (IOU) threshold, and confidence score threshold.\n\n    Parameters\n    ----------\n    tp : int\n        Number of true positives.\n    fp : int\n        Number of false positives.\n    fn : int\n        Number of false negatives.\n    label : str\n        The class label for which the counts are calculated.\n    iou_threshold : float\n        The IOU threshold used to determine a match between predicted and ground truth boxes.\n    score_threshold : float\n        The confidence score threshold above which predictions are considered.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Counts.value,\n        value={\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n        },\n        parameters={\n            \"iou_threshold\": iou_threshold,\n            \"score_threshold\": score_threshold,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.f1_score","title":"<code>f1_score(value, label, iou_threshold, score_threshold)</code>  <code>classmethod</code>","text":"<p>F1 score for a specific class label in object detection.</p> <p>This class encapsulates a metric value for a particular class label, along with the associated Intersection over Union (IOU) threshold and confidence score threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The metric value.</p> required <code>label</code> <code>str</code> <p>The class label for which the metric is calculated.</p> required <code>iou_threshold</code> <code>float</code> <p>The IOU threshold used to determine matches between predicted and ground truth boxes.</p> required <code>score_threshold</code> <code>float</code> <p>The confidence score threshold above which predictions are considered.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef f1_score(\n    cls,\n    value: float,\n    label: str,\n    iou_threshold: float,\n    score_threshold: float,\n):\n    \"\"\"\n    F1 score for a specific class label in object detection.\n\n    This class encapsulates a metric value for a particular class label,\n    along with the associated Intersection over Union (IOU) threshold and\n    confidence score threshold.\n\n    Parameters\n    ----------\n    value : float\n        The metric value.\n    label : str\n        The class label for which the metric is calculated.\n    iou_threshold : float\n        The IOU threshold used to determine matches between predicted and ground truth boxes.\n    score_threshold : float\n        The confidence score threshold above which predictions are considered.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.F1.value,\n        value=value,\n        parameters={\n            \"label\": label,\n            \"iou_threshold\": iou_threshold,\n            \"score_threshold\": score_threshold,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.mean_average_precision","title":"<code>mean_average_precision(value, iou_threshold)</code>  <code>classmethod</code>","text":"<p>Mean Average Precision (mAP) metric for object detection tasks.</p> <p>The AP computation uses 101-point interpolation, which calculates the average precision for each class by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1. The mAP is then calculated by averaging these values across all class labels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The mean average precision value.</p> required <code>iou_threshold</code> <code>float</code> <p>The IOU threshold used to compute the mAP.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef mean_average_precision(\n    cls,\n    value: float,\n    iou_threshold: float,\n):\n    \"\"\"\n    Mean Average Precision (mAP) metric for object detection tasks.\n\n    The AP computation uses 101-point interpolation, which calculates the average\n    precision for each class by interpolating the precision-recall curve at 101 evenly\n    spaced recall levels from 0 to 1. The mAP is then calculated by averaging these\n    values across all class labels.\n\n    Parameters\n    ----------\n    value : float\n        The mean average precision value.\n    iou_threshold : float\n        The IOU threshold used to compute the mAP.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.mAP.value,\n        value=value,\n        parameters={\n            \"iou_threshold\": iou_threshold,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.mean_average_precision_averaged_over_IOUs","title":"<code>mean_average_precision_averaged_over_IOUs(value, iou_thresholds)</code>  <code>classmethod</code>","text":"<p>Mean Average Precision (mAP) metric averaged over multiple IOU thresholds.</p> <p>The AP computation uses 101-point interpolation, which calculates the average precision by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1 for each IOU threshold specified in <code>iou_thresholds</code>. The final mAPAveragedOverIOUs value is obtained by averaging these AP values across all specified IOU thresholds and all class labels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The average precision value averaged over the specified IOU thresholds.</p> required <code>iou_thresholds</code> <code>list[float]</code> <p>The list of IOU thresholds used to compute the AP values.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef mean_average_precision_averaged_over_IOUs(\n    cls,\n    value: float,\n    iou_thresholds: list[float],\n):\n    \"\"\"\n    Mean Average Precision (mAP) metric averaged over multiple IOU thresholds.\n\n    The AP computation uses 101-point interpolation, which calculates the average precision\n    by interpolating the precision-recall curve at 101 evenly spaced recall levels from 0 to 1\n    for each IOU threshold specified in `iou_thresholds`. The final mAPAveragedOverIOUs value is\n    obtained by averaging these AP values across all specified IOU thresholds and all class labels.\n\n    Parameters\n    ----------\n    value : float\n        The average precision value averaged over the specified IOU thresholds.\n    iou_thresholds : list[float]\n        The list of IOU thresholds used to compute the AP values.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.mAPAveragedOverIOUs.value,\n        value=value,\n        parameters={\n            \"iou_thresholds\": iou_thresholds,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.mean_average_recall","title":"<code>mean_average_recall(value, score_threshold, iou_thresholds)</code>  <code>classmethod</code>","text":"<p>Mean Average Recall (mAR) metric for object detection tasks.</p> <p>The mAR computation considers detections with confidence scores above the specified <code>score_threshold</code> and calculates recall at each IOU threshold in <code>iou_thresholds</code> for each label. The final mAR value is obtained by averaging these recall values over the specified IOU thresholds and then averaging across all labels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The mean average recall value averaged over the specified IOU thresholds.</p> required <code>score_threshold</code> <code>float</code> <p>The detection score threshold; only detections with confidence scores above this threshold are considered.</p> required <code>iou_thresholds</code> <code>list[float]</code> <p>The list of IOU thresholds used to compute the recall values.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef mean_average_recall(\n    cls,\n    value: float,\n    score_threshold: float,\n    iou_thresholds: list[float],\n):\n    \"\"\"\n    Mean Average Recall (mAR) metric for object detection tasks.\n\n    The mAR computation considers detections with confidence scores above the specified\n    `score_threshold` and calculates recall at each IOU threshold in `iou_thresholds` for\n    each label. The final mAR value is obtained by averaging these recall values over the\n    specified IOU thresholds and then averaging across all labels.\n\n    Parameters\n    ----------\n    value : float\n        The mean average recall value averaged over the specified IOU thresholds.\n    score_threshold : float\n        The detection score threshold; only detections with confidence scores above this\n        threshold are considered.\n    iou_thresholds : list[float]\n        The list of IOU thresholds used to compute the recall values.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.mAR.value,\n        value=value,\n        parameters={\n            \"iou_thresholds\": iou_thresholds,\n            \"score_threshold\": score_threshold,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.mean_average_recall_averaged_over_scores","title":"<code>mean_average_recall_averaged_over_scores(value, score_thresholds, iou_thresholds)</code>  <code>classmethod</code>","text":"<p>Mean Average Recall (mAR) metric averaged over multiple score thresholds and IOU thresholds.</p> <p>The mAR computation considers detections across multiple <code>score_thresholds</code>, calculates recall at each IOU threshold in <code>iou_thresholds</code> for each label, averages these recall values over all specified score thresholds and IOU thresholds, and then computes the mean across all labels to obtain the final mAR value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The mean average recall value averaged over the specified score thresholds and IOU thresholds.</p> required <code>score_thresholds</code> <code>list[float]</code> <p>The list of detection score thresholds; detections with confidence scores above each threshold are considered.</p> required <code>iou_thresholds</code> <code>list[float]</code> <p>The list of IOU thresholds used to compute the recall values.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef mean_average_recall_averaged_over_scores(\n    cls,\n    value: float,\n    score_thresholds: list[float],\n    iou_thresholds: list[float],\n):\n    \"\"\"\n    Mean Average Recall (mAR) metric averaged over multiple score thresholds and IOU thresholds.\n\n    The mAR computation considers detections across multiple `score_thresholds`, calculates recall\n    at each IOU threshold in `iou_thresholds` for each label, averages these recall values over all\n    specified score thresholds and IOU thresholds, and then computes the mean across all labels to\n    obtain the final mAR value.\n\n    Parameters\n    ----------\n    value : float\n        The mean average recall value averaged over the specified score thresholds and IOU thresholds.\n    score_thresholds : list[float]\n        The list of detection score thresholds; detections with confidence scores above each threshold are considered.\n    iou_thresholds : list[float]\n        The list of IOU thresholds used to compute the recall values.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.mARAveragedOverScores.value,\n        value=value,\n        parameters={\n            \"iou_thresholds\": iou_thresholds,\n            \"score_thresholds\": score_thresholds,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.precision","title":"<code>precision(value, label, iou_threshold, score_threshold)</code>  <code>classmethod</code>","text":"<p>Precision metric for a specific class label in object detection.</p> <p>This class encapsulates a metric value for a particular class label, along with the associated Intersection over Union (IOU) threshold and confidence score threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The metric value.</p> required <code>label</code> <code>str</code> <p>The class label for which the metric is calculated.</p> required <code>iou_threshold</code> <code>float</code> <p>The IOU threshold used to determine matches between predicted and ground truth boxes.</p> required <code>score_threshold</code> <code>float</code> <p>The confidence score threshold above which predictions are considered.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef precision(\n    cls,\n    value: float,\n    label: str,\n    iou_threshold: float,\n    score_threshold: float,\n):\n    \"\"\"\n    Precision metric for a specific class label in object detection.\n\n    This class encapsulates a metric value for a particular class label,\n    along with the associated Intersection over Union (IOU) threshold and\n    confidence score threshold.\n\n    Parameters\n    ----------\n    value : float\n        The metric value.\n    label : str\n        The class label for which the metric is calculated.\n    iou_threshold : float\n        The IOU threshold used to determine matches between predicted and ground truth boxes.\n    score_threshold : float\n        The confidence score threshold above which predictions are considered.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Precision.value,\n        value=value,\n        parameters={\n            \"label\": label,\n            \"iou_threshold\": iou_threshold,\n            \"score_threshold\": score_threshold,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.precision_recall_curve","title":"<code>precision_recall_curve(precisions, scores, iou_threshold, label)</code>  <code>classmethod</code>","text":"<p>Interpolated precision-recall curve over 101 recall points.</p> <p>The precision values are interpolated over recalls ranging from 0.0 to 1.0 in steps of 0.01, resulting in 101 points. This is a byproduct of the 101-point interpolation used in calculating the Average Precision (AP) metric in object detection tasks.</p> <p>Parameters:</p> Name Type Description Default <code>precisions</code> <code>list[float]</code> <p>Interpolated precision values corresponding to recalls at 0.0, 0.01, ..., 1.0.</p> required <code>scores</code> <code>list[float]</code> <p>Maximum prediction score for each point on the interpolated curve.</p> required <code>iou_threshold</code> <code>float</code> <p>The Intersection over Union (IOU) threshold used to determine true positives.</p> required <code>label</code> <code>str</code> <p>The class label associated with this precision-recall curve.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef precision_recall_curve(\n    cls,\n    precisions: list[float],\n    scores: list[float],\n    iou_threshold: float,\n    label: str,\n):\n    \"\"\"\n    Interpolated precision-recall curve over 101 recall points.\n\n    The precision values are interpolated over recalls ranging from 0.0 to 1.0 in steps of 0.01,\n    resulting in 101 points. This is a byproduct of the 101-point interpolation used in calculating\n    the Average Precision (AP) metric in object detection tasks.\n\n    Parameters\n    ----------\n    precisions : list[float]\n        Interpolated precision values corresponding to recalls at 0.0, 0.01, ..., 1.0.\n    scores : list[float]\n        Maximum prediction score for each point on the interpolated curve.\n    iou_threshold : float\n        The Intersection over Union (IOU) threshold used to determine true positives.\n    label : str\n        The class label associated with this precision-recall curve.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.PrecisionRecallCurve.value,\n        value={\n            \"precisions\": precisions,\n            \"scores\": scores,\n        },\n        parameters={\n            \"iou_threshold\": iou_threshold,\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#valor_lite.object_detection.metric.Metric.recall","title":"<code>recall(value, label, iou_threshold, score_threshold)</code>  <code>classmethod</code>","text":"<p>Recall metric for a specific class label in object detection.</p> <p>This class encapsulates a metric value for a particular class label, along with the associated Intersection over Union (IOU) threshold and confidence score threshold.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The metric value.</p> required <code>label</code> <code>str</code> <p>The class label for which the metric is calculated.</p> required <code>iou_threshold</code> <code>float</code> <p>The IOU threshold used to determine matches between predicted and ground truth boxes.</p> required <code>score_threshold</code> <code>float</code> <p>The confidence score threshold above which predictions are considered.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/object_detection/metric.py</code> <pre><code>@classmethod\ndef recall(\n    cls,\n    value: float,\n    label: str,\n    iou_threshold: float,\n    score_threshold: float,\n):\n    \"\"\"\n    Recall metric for a specific class label in object detection.\n\n    This class encapsulates a metric value for a particular class label,\n    along with the associated Intersection over Union (IOU) threshold and\n    confidence score threshold.\n\n    Parameters\n    ----------\n    value : float\n        The metric value.\n    label : str\n        The class label for which the metric is calculated.\n    iou_threshold : float\n        The IOU threshold used to determine matches between predicted and ground truth boxes.\n    score_threshold : float\n        The confidence score threshold above which predictions are considered.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Recall.value,\n        value=value,\n        parameters={\n            \"label\": label,\n            \"iou_threshold\": iou_threshold,\n            \"score_threshold\": score_threshold,\n        },\n    )\n</code></pre>"},{"location":"object_detection/metrics/#references","title":"References","text":"<ul> <li>MS COCO Detection Evaluation</li> <li>The PASCAL Visual Object Classes (VOC) Challenge</li> <li>Mean Average Precision (mAP) Using the COCO Evaluator</li> </ul>"},{"location":"semantic_segmentation/documentation/","title":"Documentation","text":"<p>Documentation</p>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Bitmask","title":"<code>valor_lite.semantic_segmentation.Bitmask</code>  <code>dataclass</code>","text":"<p>Represents a binary mask with an associated semantic label.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>NDArray[bool_]</code> <p>A NumPy array of boolean values representing the mask.</p> required <code>label</code> <code>str</code> <p>The semantic label associated with the mask.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; mask = np.array([[True, False], [False, True]], dtype=np.bool_)\n&gt;&gt;&gt; bitmask = Bitmask(mask=mask, label='ocean')\n</code></pre> Source code in <code>valor_lite/semantic_segmentation/annotation.py</code> <pre><code>@dataclass\nclass Bitmask:\n    \"\"\"\n    Represents a binary mask with an associated semantic label.\n\n    Parameters\n    ----------\n    mask : NDArray[np.bool_]\n        A NumPy array of boolean values representing the mask.\n    label : str\n        The semantic label associated with the mask.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; mask = np.array([[True, False], [False, True]], dtype=np.bool_)\n    &gt;&gt;&gt; bitmask = Bitmask(mask=mask, label='ocean')\n    \"\"\"\n\n    mask: NDArray[np.bool_]\n    label: str\n\n    def __post_init__(self):\n        if self.mask.dtype != np.bool_:\n            raise ValueError(\n                f\"Bitmask recieved mask with dtype '{self.mask.dtype}'.\"\n            )\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Segmentation","title":"<code>valor_lite.semantic_segmentation.Segmentation</code>  <code>dataclass</code>","text":"<p>Segmentation data structure holding ground truth and prediction bitmasks for semantic segmentation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique identifier for the image or sample.</p> required <code>groundtruths</code> <code>List[Bitmask]</code> <p>List of ground truth bitmasks.</p> required <code>predictions</code> <code>List[Bitmask]</code> <p>List of predicted bitmasks.</p> required <code>shape</code> <code>tuple of int</code> <p>The shape of the segmentation masks. This is set automatically after initialization.</p> required <code>size</code> <code>int</code> <p>The total number of pixels in the masks. This is set automatically after initialization.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; mask1 = np.array([[True, False], [False, True]], dtype=np.bool_)\n&gt;&gt;&gt; groundtruth = Bitmask(mask=mask1, label='object')\n&gt;&gt;&gt; mask2 = np.array([[False, True], [True, False]], dtype=np.bool_)\n&gt;&gt;&gt; prediction = Bitmask(mask=mask2, label='object')\n&gt;&gt;&gt; segmentation = Segmentation(\n...     uid='123',\n...     groundtruths=[groundtruth],\n...     predictions=[prediction]\n... )\n</code></pre> Source code in <code>valor_lite/semantic_segmentation/annotation.py</code> <pre><code>@dataclass\nclass Segmentation:\n    \"\"\"\n    Segmentation data structure holding ground truth and prediction bitmasks for semantic segmentation tasks.\n\n    Parameters\n    ----------\n    uid : str\n        Unique identifier for the image or sample.\n    groundtruths : List[Bitmask]\n        List of ground truth bitmasks.\n    predictions : List[Bitmask]\n        List of predicted bitmasks.\n    shape : tuple of int, optional\n        The shape of the segmentation masks. This is set automatically after initialization.\n    size : int, optional\n        The total number of pixels in the masks. This is set automatically after initialization.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; mask1 = np.array([[True, False], [False, True]], dtype=np.bool_)\n    &gt;&gt;&gt; groundtruth = Bitmask(mask=mask1, label='object')\n    &gt;&gt;&gt; mask2 = np.array([[False, True], [True, False]], dtype=np.bool_)\n    &gt;&gt;&gt; prediction = Bitmask(mask=mask2, label='object')\n    &gt;&gt;&gt; segmentation = Segmentation(\n    ...     uid='123',\n    ...     groundtruths=[groundtruth],\n    ...     predictions=[prediction]\n    ... )\n    \"\"\"\n\n    uid: str\n    groundtruths: list[Bitmask]\n    predictions: list[Bitmask]\n    shape: tuple[int, ...]\n    size: int = field(default=0)\n\n    def __post_init__(self):\n\n        if len(self.shape) != 2 or self.shape[0] &lt;= 0 or self.shape[1] &lt;= 0:\n            raise ValueError(\n                f\"segmentations must be 2-dimensional and have non-zero dimensions. Recieved shape '{self.shape}'\"\n            )\n        self.size = self.shape[0] * self.shape[1]\n\n        mask_accumulation = None\n        for groundtruth in self.groundtruths:\n            if self.shape != groundtruth.mask.shape:\n                raise ValueError(\n                    f\"ground truth masks for datum '{self.uid}' should have shape '{self.shape}'. Received mask with shape '{groundtruth.mask.shape}'\"\n                )\n\n            if mask_accumulation is None:\n                mask_accumulation = groundtruth.mask.copy()\n            elif np.logical_and(mask_accumulation, groundtruth.mask).any():\n                raise ValueError(\"ground truth masks cannot overlap\")\n            else:\n                mask_accumulation = mask_accumulation | groundtruth.mask\n\n        mask_accumulation = None\n        for prediction in self.predictions:\n            if self.shape != prediction.mask.shape:\n                raise ValueError(\n                    f\"prediction masks for datum '{self.uid}' should have shape '{self.shape}'. Received mask with shape '{prediction.mask.shape}'\"\n                )\n\n            if mask_accumulation is None:\n                mask_accumulation = prediction.mask.copy()\n            elif np.logical_and(mask_accumulation, prediction.mask).any():\n                raise ValueError(\"prediction masks cannot overlap\")\n            else:\n                mask_accumulation = mask_accumulation | prediction.mask\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.DataLoader","title":"<code>valor_lite.semantic_segmentation.DataLoader</code>","text":"<p>Segmentation DataLoader.</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>class DataLoader:\n    \"\"\"\n    Segmentation DataLoader.\n    \"\"\"\n\n    def __init__(self):\n        self._evaluator = Evaluator()\n        self.matrices = list()\n\n    def _add_datum(self, uid: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a datum to the cache.\n\n        Parameters\n        ----------\n        uid : str\n            The datum uid.\n\n        Returns\n        -------\n        int\n            The datum index.\n        \"\"\"\n        if uid in self._evaluator.datum_id_to_index:\n            raise ValueError(f\"Datum with uid `{uid}` already exists.\")\n        index = len(self._evaluator.datum_id_to_index)\n        self._evaluator.datum_id_to_index[uid] = index\n        self._evaluator.index_to_datum_id.append(uid)\n        return index\n\n    def _add_label(self, label: str) -&gt; int:\n        \"\"\"\n        Helper function for adding a label to the cache.\n\n        Parameters\n        ----------\n        label : str\n            A string label.\n\n        Returns\n        -------\n        int\n            The label's index.\n        \"\"\"\n        if label not in self._evaluator.label_to_index:\n            label_id = len(self._evaluator.index_to_label)\n            self._evaluator.label_to_index[label] = label_id\n            self._evaluator.index_to_label.append(label)\n        return self._evaluator.label_to_index[label]\n\n    def add_data(\n        self,\n        segmentations: list[Segmentation],\n        show_progress: bool = False,\n    ):\n        \"\"\"\n        Adds segmentations to the cache.\n\n        Parameters\n        ----------\n        segmentations : list[Segmentation]\n            A list of Segmentation objects.\n        show_progress : bool, default=False\n            Toggle for tqdm progress bar.\n        \"\"\"\n\n        disable_tqdm = not show_progress\n        for segmentation in tqdm(segmentations, disable=disable_tqdm):\n            # update datum cache\n            self._add_datum(segmentation.uid)\n\n            groundtruth_labels = -1 * np.ones(\n                len(segmentation.groundtruths), dtype=np.int64\n            )\n            for idx, groundtruth in enumerate(segmentation.groundtruths):\n                label_idx = self._add_label(groundtruth.label)\n                groundtruth_labels[idx] = label_idx\n\n            prediction_labels = -1 * np.ones(\n                len(segmentation.predictions), dtype=np.int64\n            )\n            for idx, prediction in enumerate(segmentation.predictions):\n                label_idx = self._add_label(prediction.label)\n                prediction_labels[idx] = label_idx\n\n            if segmentation.groundtruths:\n                combined_groundtruths = np.stack(\n                    [\n                        groundtruth.mask.flatten()\n                        for groundtruth in segmentation.groundtruths\n                    ],\n                    axis=0,\n                )\n            else:\n                combined_groundtruths = np.zeros(\n                    (1, segmentation.shape[0] * segmentation.shape[1]),\n                    dtype=np.bool_,\n                )\n\n            if segmentation.predictions:\n                combined_predictions = np.stack(\n                    [\n                        prediction.mask.flatten()\n                        for prediction in segmentation.predictions\n                    ],\n                    axis=0,\n                )\n            else:\n                combined_predictions = np.zeros(\n                    (1, segmentation.shape[0] * segmentation.shape[1]),\n                    dtype=np.bool_,\n                )\n\n            self.matrices.append(\n                compute_intermediate_confusion_matrices(\n                    groundtruths=combined_groundtruths,\n                    predictions=combined_predictions,\n                    groundtruth_labels=groundtruth_labels,\n                    prediction_labels=prediction_labels,\n                    n_labels=len(self._evaluator.index_to_label),\n                )\n            )\n\n    def finalize(self) -&gt; Evaluator:\n        \"\"\"\n        Performs data finalization and some preprocessing steps.\n\n        Returns\n        -------\n        Evaluator\n            A ready-to-use evaluator object.\n        \"\"\"\n\n        if len(self.matrices) == 0:\n            raise ValueError(\"No data available to create evaluator.\")\n\n        n_labels = len(self._evaluator.index_to_label)\n        n_datums = len(self._evaluator.index_to_datum_id)\n        self._evaluator._confusion_matrices = np.zeros(\n            (n_datums, n_labels + 1, n_labels + 1), dtype=np.int64\n        )\n        for idx, matrix in enumerate(self.matrices):\n            h, w = matrix.shape\n            self._evaluator._confusion_matrices[idx, :h, :w] = matrix\n        self._evaluator._label_metadata = compute_label_metadata(\n            confusion_matrices=self._evaluator._confusion_matrices,\n            n_labels=n_labels,\n        )\n        self._evaluator._metadata = Metadata.create(\n            confusion_matrices=self._evaluator._confusion_matrices,\n        )\n        return self._evaluator\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.DataLoader.add_data","title":"<code>add_data(segmentations, show_progress=False)</code>","text":"<p>Adds segmentations to the cache.</p> <p>Parameters:</p> Name Type Description Default <code>segmentations</code> <code>list[Segmentation]</code> <p>A list of Segmentation objects.</p> required <code>show_progress</code> <code>bool</code> <p>Toggle for tqdm progress bar.</p> <code>False</code> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def add_data(\n    self,\n    segmentations: list[Segmentation],\n    show_progress: bool = False,\n):\n    \"\"\"\n    Adds segmentations to the cache.\n\n    Parameters\n    ----------\n    segmentations : list[Segmentation]\n        A list of Segmentation objects.\n    show_progress : bool, default=False\n        Toggle for tqdm progress bar.\n    \"\"\"\n\n    disable_tqdm = not show_progress\n    for segmentation in tqdm(segmentations, disable=disable_tqdm):\n        # update datum cache\n        self._add_datum(segmentation.uid)\n\n        groundtruth_labels = -1 * np.ones(\n            len(segmentation.groundtruths), dtype=np.int64\n        )\n        for idx, groundtruth in enumerate(segmentation.groundtruths):\n            label_idx = self._add_label(groundtruth.label)\n            groundtruth_labels[idx] = label_idx\n\n        prediction_labels = -1 * np.ones(\n            len(segmentation.predictions), dtype=np.int64\n        )\n        for idx, prediction in enumerate(segmentation.predictions):\n            label_idx = self._add_label(prediction.label)\n            prediction_labels[idx] = label_idx\n\n        if segmentation.groundtruths:\n            combined_groundtruths = np.stack(\n                [\n                    groundtruth.mask.flatten()\n                    for groundtruth in segmentation.groundtruths\n                ],\n                axis=0,\n            )\n        else:\n            combined_groundtruths = np.zeros(\n                (1, segmentation.shape[0] * segmentation.shape[1]),\n                dtype=np.bool_,\n            )\n\n        if segmentation.predictions:\n            combined_predictions = np.stack(\n                [\n                    prediction.mask.flatten()\n                    for prediction in segmentation.predictions\n                ],\n                axis=0,\n            )\n        else:\n            combined_predictions = np.zeros(\n                (1, segmentation.shape[0] * segmentation.shape[1]),\n                dtype=np.bool_,\n            )\n\n        self.matrices.append(\n            compute_intermediate_confusion_matrices(\n                groundtruths=combined_groundtruths,\n                predictions=combined_predictions,\n                groundtruth_labels=groundtruth_labels,\n                prediction_labels=prediction_labels,\n                n_labels=len(self._evaluator.index_to_label),\n            )\n        )\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.DataLoader.finalize","title":"<code>finalize()</code>","text":"<p>Performs data finalization and some preprocessing steps.</p> <p>Returns:</p> Type Description <code>Evaluator</code> <p>A ready-to-use evaluator object.</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def finalize(self) -&gt; Evaluator:\n    \"\"\"\n    Performs data finalization and some preprocessing steps.\n\n    Returns\n    -------\n    Evaluator\n        A ready-to-use evaluator object.\n    \"\"\"\n\n    if len(self.matrices) == 0:\n        raise ValueError(\"No data available to create evaluator.\")\n\n    n_labels = len(self._evaluator.index_to_label)\n    n_datums = len(self._evaluator.index_to_datum_id)\n    self._evaluator._confusion_matrices = np.zeros(\n        (n_datums, n_labels + 1, n_labels + 1), dtype=np.int64\n    )\n    for idx, matrix in enumerate(self.matrices):\n        h, w = matrix.shape\n        self._evaluator._confusion_matrices[idx, :h, :w] = matrix\n    self._evaluator._label_metadata = compute_label_metadata(\n        confusion_matrices=self._evaluator._confusion_matrices,\n        n_labels=n_labels,\n    )\n    self._evaluator._metadata = Metadata.create(\n        confusion_matrices=self._evaluator._confusion_matrices,\n    )\n    return self._evaluator\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator","title":"<code>valor_lite.semantic_segmentation.Evaluator</code>","text":"<p>Segmentation Evaluator</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>class Evaluator:\n    \"\"\"\n    Segmentation Evaluator\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes evaluator caches.\"\"\"\n        # external references\n        self.datum_id_to_index: dict[str, int] = {}\n        self.index_to_datum_id: list[str] = []\n        self.label_to_index: dict[str, int] = {}\n        self.index_to_label: list[str] = []\n\n        # internal caches\n        self._confusion_matrices = np.array([], dtype=np.int64)\n        self._label_metadata = np.array([], dtype=np.int64)\n        self._metadata = Metadata()\n\n    @property\n    def metadata(self) -&gt; Metadata:\n        return self._metadata\n\n    @property\n    def ignored_prediction_labels(self) -&gt; list[str]:\n        \"\"\"\n        Prediction labels that are not present in the ground truth set.\n        \"\"\"\n        glabels = set(np.where(self._label_metadata[:, 0] &gt; 0)[0])\n        plabels = set(np.where(self._label_metadata[:, 1] &gt; 0)[0])\n        return [\n            self.index_to_label[label_id] for label_id in (plabels - glabels)\n        ]\n\n    @property\n    def missing_prediction_labels(self) -&gt; list[str]:\n        \"\"\"\n        Ground truth labels that are not present in the prediction set.\n        \"\"\"\n        glabels = set(np.where(self._label_metadata[:, 0] &gt; 0)[0])\n        plabels = set(np.where(self._label_metadata[:, 1] &gt; 0)[0])\n        return [\n            self.index_to_label[label_id] for label_id in (glabels - plabels)\n        ]\n\n    def create_filter(\n        self,\n        datum_ids: list[str] | None = None,\n        labels: list[str] | None = None,\n    ) -&gt; Filter:\n        \"\"\"\n        Creates a filter for use with the evaluator.\n\n        Parameters\n        ----------\n        datum_ids : list[str], optional\n            An optional list of string uids representing datums.\n        labels : list[str], optional\n            An optional list of labels.\n\n        Returns\n        -------\n        Filter\n            The filter object containing a mask and metadata.\n        \"\"\"\n        datum_mask = np.ones(self._confusion_matrices.shape[0], dtype=np.bool_)\n        label_mask = np.zeros(\n            self.metadata.number_of_labels + 1, dtype=np.bool_\n        )\n        if datum_ids is not None:\n            if not datum_ids:\n                filtered_confusion_matrices = np.array([], dtype=np.int64)\n                warnings.warn(\"datum filter results in empty data array\")\n                return Filter(\n                    datum_mask=np.zeros_like(datum_mask),\n                    label_mask=label_mask,\n                    metadata=Metadata(),\n                )\n            datum_id_array = np.array(\n                [self.datum_id_to_index[uid] for uid in datum_ids],\n                dtype=np.int64,\n            )\n            datum_id_array.sort()\n            mask_valid_datums = (\n                np.arange(self._confusion_matrices.shape[0]).reshape(-1, 1)\n                == datum_id_array.reshape(1, -1)\n            ).any(axis=1)\n            datum_mask[~mask_valid_datums] = False\n        if labels is not None:\n            if not labels:\n                filtered_confusion_matrices = np.array([], dtype=np.int64)\n                warnings.warn(\"label filter results in empty data array\")\n                return Filter(\n                    datum_mask=datum_mask,\n                    label_mask=np.ones_like(label_mask),\n                    metadata=Metadata(),\n                )\n            labels_id_array = np.array(\n                [self.label_to_index[label] for label in labels] + [-1],\n                dtype=np.int64,\n            )\n            label_range = np.arange(self.metadata.number_of_labels + 1) - 1\n            mask_valid_labels = (\n                label_range.reshape(-1, 1) == labels_id_array.reshape(1, -1)\n            ).any(axis=1)\n            label_mask[~mask_valid_labels] = True\n\n        filtered_confusion_matrices, _ = filter_cache(\n            confusion_matrices=self._confusion_matrices.copy(),\n            datum_mask=datum_mask,\n            label_mask=label_mask,\n            number_of_labels=self.metadata.number_of_labels,\n        )\n\n        return Filter(\n            datum_mask=datum_mask,\n            label_mask=label_mask,\n            metadata=Metadata.create(\n                confusion_matrices=filtered_confusion_matrices,\n            ),\n        )\n\n    def filter(\n        self, filter_: Filter\n    ) -&gt; tuple[NDArray[np.int64], NDArray[np.int64]]:\n        \"\"\"\n        Performs the filter operation over the internal cache.\n\n        Parameters\n        ----------\n        filter_ : Filter\n            An object describing the filter operation.\n\n        Returns\n        -------\n        NDArray[int64]\n            Filtered confusion matrices.\n        NDArray[int64]\n            Filtered label metadata\n        \"\"\"\n        empty_datum_mask = not filter_.datum_mask.any()\n        empty_label_mask = filter_.label_mask.all()\n        if empty_datum_mask or empty_label_mask:\n            if empty_datum_mask:\n                warnings.warn(\"filter does not allow any datum\")\n            if empty_label_mask:\n                warnings.warn(\"filter removes all labels\")\n            return (\n                np.array([], dtype=np.int64),\n                np.zeros((self.metadata.number_of_labels, 2), dtype=np.int64),\n            )\n\n        return filter_cache(\n            confusion_matrices=self._confusion_matrices.copy(),\n            datum_mask=filter_.datum_mask,\n            label_mask=filter_.label_mask,\n            number_of_labels=self.metadata.number_of_labels,\n        )\n\n    def compute_precision_recall_iou(\n        self, filter_: Filter | None = None\n    ) -&gt; dict[MetricType, list]:\n        \"\"\"\n        Performs an evaluation and returns metrics.\n\n        Returns\n        -------\n        dict[MetricType, list]\n            A dictionary mapping MetricType enumerations to lists of computed metrics.\n        \"\"\"\n        if filter_ is not None:\n            confusion_matrices, label_metadata = self.filter(filter_)\n            n_pixels = filter_.metadata.number_of_pixels\n        else:\n            confusion_matrices = self._confusion_matrices\n            label_metadata = self._label_metadata\n            n_pixels = self.metadata.number_of_pixels\n\n        results = compute_metrics(\n            confusion_matrices=confusion_matrices,\n            label_metadata=label_metadata,\n            n_pixels=n_pixels,\n        )\n        return unpack_precision_recall_iou_into_metric_lists(\n            results=results,\n            label_metadata=label_metadata,\n            index_to_label=self.index_to_label,\n        )\n\n    def evaluate(\n        self, filter_: Filter | None = None\n    ) -&gt; dict[MetricType, list[Metric]]:\n        \"\"\"\n        Computes all available metrics.\n\n        Returns\n        -------\n        dict[MetricType, list[Metric]]\n            Lists of metrics organized by metric type.\n        \"\"\"\n        return self.compute_precision_recall_iou(filter_=filter_)\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.ignored_prediction_labels","title":"<code>ignored_prediction_labels</code>  <code>property</code>","text":"<p>Prediction labels that are not present in the ground truth set.</p>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.missing_prediction_labels","title":"<code>missing_prediction_labels</code>  <code>property</code>","text":"<p>Ground truth labels that are not present in the prediction set.</p>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.__init__","title":"<code>__init__()</code>","text":"<p>Initializes evaluator caches.</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes evaluator caches.\"\"\"\n    # external references\n    self.datum_id_to_index: dict[str, int] = {}\n    self.index_to_datum_id: list[str] = []\n    self.label_to_index: dict[str, int] = {}\n    self.index_to_label: list[str] = []\n\n    # internal caches\n    self._confusion_matrices = np.array([], dtype=np.int64)\n    self._label_metadata = np.array([], dtype=np.int64)\n    self._metadata = Metadata()\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.compute_precision_recall_iou","title":"<code>compute_precision_recall_iou(filter_=None)</code>","text":"<p>Performs an evaluation and returns metrics.</p> <p>Returns:</p> Type Description <code>dict[MetricType, list]</code> <p>A dictionary mapping MetricType enumerations to lists of computed metrics.</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def compute_precision_recall_iou(\n    self, filter_: Filter | None = None\n) -&gt; dict[MetricType, list]:\n    \"\"\"\n    Performs an evaluation and returns metrics.\n\n    Returns\n    -------\n    dict[MetricType, list]\n        A dictionary mapping MetricType enumerations to lists of computed metrics.\n    \"\"\"\n    if filter_ is not None:\n        confusion_matrices, label_metadata = self.filter(filter_)\n        n_pixels = filter_.metadata.number_of_pixels\n    else:\n        confusion_matrices = self._confusion_matrices\n        label_metadata = self._label_metadata\n        n_pixels = self.metadata.number_of_pixels\n\n    results = compute_metrics(\n        confusion_matrices=confusion_matrices,\n        label_metadata=label_metadata,\n        n_pixels=n_pixels,\n    )\n    return unpack_precision_recall_iou_into_metric_lists(\n        results=results,\n        label_metadata=label_metadata,\n        index_to_label=self.index_to_label,\n    )\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.create_filter","title":"<code>create_filter(datum_ids=None, labels=None)</code>","text":"<p>Creates a filter for use with the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>datum_ids</code> <code>list[str]</code> <p>An optional list of string uids representing datums.</p> <code>None</code> <code>labels</code> <code>list[str]</code> <p>An optional list of labels.</p> <code>None</code> <p>Returns:</p> Type Description <code>Filter</code> <p>The filter object containing a mask and metadata.</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def create_filter(\n    self,\n    datum_ids: list[str] | None = None,\n    labels: list[str] | None = None,\n) -&gt; Filter:\n    \"\"\"\n    Creates a filter for use with the evaluator.\n\n    Parameters\n    ----------\n    datum_ids : list[str], optional\n        An optional list of string uids representing datums.\n    labels : list[str], optional\n        An optional list of labels.\n\n    Returns\n    -------\n    Filter\n        The filter object containing a mask and metadata.\n    \"\"\"\n    datum_mask = np.ones(self._confusion_matrices.shape[0], dtype=np.bool_)\n    label_mask = np.zeros(\n        self.metadata.number_of_labels + 1, dtype=np.bool_\n    )\n    if datum_ids is not None:\n        if not datum_ids:\n            filtered_confusion_matrices = np.array([], dtype=np.int64)\n            warnings.warn(\"datum filter results in empty data array\")\n            return Filter(\n                datum_mask=np.zeros_like(datum_mask),\n                label_mask=label_mask,\n                metadata=Metadata(),\n            )\n        datum_id_array = np.array(\n            [self.datum_id_to_index[uid] for uid in datum_ids],\n            dtype=np.int64,\n        )\n        datum_id_array.sort()\n        mask_valid_datums = (\n            np.arange(self._confusion_matrices.shape[0]).reshape(-1, 1)\n            == datum_id_array.reshape(1, -1)\n        ).any(axis=1)\n        datum_mask[~mask_valid_datums] = False\n    if labels is not None:\n        if not labels:\n            filtered_confusion_matrices = np.array([], dtype=np.int64)\n            warnings.warn(\"label filter results in empty data array\")\n            return Filter(\n                datum_mask=datum_mask,\n                label_mask=np.ones_like(label_mask),\n                metadata=Metadata(),\n            )\n        labels_id_array = np.array(\n            [self.label_to_index[label] for label in labels] + [-1],\n            dtype=np.int64,\n        )\n        label_range = np.arange(self.metadata.number_of_labels + 1) - 1\n        mask_valid_labels = (\n            label_range.reshape(-1, 1) == labels_id_array.reshape(1, -1)\n        ).any(axis=1)\n        label_mask[~mask_valid_labels] = True\n\n    filtered_confusion_matrices, _ = filter_cache(\n        confusion_matrices=self._confusion_matrices.copy(),\n        datum_mask=datum_mask,\n        label_mask=label_mask,\n        number_of_labels=self.metadata.number_of_labels,\n    )\n\n    return Filter(\n        datum_mask=datum_mask,\n        label_mask=label_mask,\n        metadata=Metadata.create(\n            confusion_matrices=filtered_confusion_matrices,\n        ),\n    )\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.evaluate","title":"<code>evaluate(filter_=None)</code>","text":"<p>Computes all available metrics.</p> <p>Returns:</p> Type Description <code>dict[MetricType, list[Metric]]</code> <p>Lists of metrics organized by metric type.</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def evaluate(\n    self, filter_: Filter | None = None\n) -&gt; dict[MetricType, list[Metric]]:\n    \"\"\"\n    Computes all available metrics.\n\n    Returns\n    -------\n    dict[MetricType, list[Metric]]\n        Lists of metrics organized by metric type.\n    \"\"\"\n    return self.compute_precision_recall_iou(filter_=filter_)\n</code></pre>"},{"location":"semantic_segmentation/documentation/#valor_lite.semantic_segmentation.Evaluator.filter","title":"<code>filter(filter_)</code>","text":"<p>Performs the filter operation over the internal cache.</p> <p>Parameters:</p> Name Type Description Default <code>filter_</code> <code>Filter</code> <p>An object describing the filter operation.</p> required <p>Returns:</p> Type Description <code>NDArray[int64]</code> <p>Filtered confusion matrices.</p> <code>NDArray[int64]</code> <p>Filtered label metadata</p> Source code in <code>valor_lite/semantic_segmentation/manager.py</code> <pre><code>def filter(\n    self, filter_: Filter\n) -&gt; tuple[NDArray[np.int64], NDArray[np.int64]]:\n    \"\"\"\n    Performs the filter operation over the internal cache.\n\n    Parameters\n    ----------\n    filter_ : Filter\n        An object describing the filter operation.\n\n    Returns\n    -------\n    NDArray[int64]\n        Filtered confusion matrices.\n    NDArray[int64]\n        Filtered label metadata\n    \"\"\"\n    empty_datum_mask = not filter_.datum_mask.any()\n    empty_label_mask = filter_.label_mask.all()\n    if empty_datum_mask or empty_label_mask:\n        if empty_datum_mask:\n            warnings.warn(\"filter does not allow any datum\")\n        if empty_label_mask:\n            warnings.warn(\"filter removes all labels\")\n        return (\n            np.array([], dtype=np.int64),\n            np.zeros((self.metadata.number_of_labels, 2), dtype=np.int64),\n        )\n\n    return filter_cache(\n        confusion_matrices=self._confusion_matrices.copy(),\n        datum_mask=filter_.datum_mask,\n        label_mask=filter_.label_mask,\n        number_of_labels=self.metadata.number_of_labels,\n    )\n</code></pre>"},{"location":"semantic_segmentation/metrics/","title":"Metrics","text":""},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Semantic Segmentation Metric.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The metric type.</p> <code>value</code> <code>int | float | dict</code> <p>The metric value.</p> <code>parameters</code> <code>dict[str, Any]</code> <p>A dictionary containing metric parameters.</p> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@dataclass\nclass Metric(BaseMetric):\n    \"\"\"\n    Semantic Segmentation Metric.\n\n    Attributes\n    ----------\n    type : str\n        The metric type.\n    value : int | float | dict\n        The metric value.\n    parameters : dict[str, Any]\n        A dictionary containing metric parameters.\n    \"\"\"\n\n    def __post_init__(self):\n        if not isinstance(self.type, str):\n            raise TypeError(\n                f\"Metric type should be of type 'str': {self.type}\"\n            )\n        elif not isinstance(self.value, (int, float, dict)):\n            raise TypeError(\n                f\"Metric value must be of type 'int', 'float' or 'dict': {self.value}\"\n            )\n        elif not isinstance(self.parameters, dict):\n            raise TypeError(\n                f\"Metric parameters must be of type 'dict[str, Any]': {self.parameters}\"\n            )\n        elif not all([isinstance(k, str) for k in self.parameters.keys()]):\n            raise TypeError(\n                f\"Metric parameter dictionary should only have keys with type 'str': {self.parameters}\"\n            )\n\n    @classmethod\n    def precision(\n        cls,\n        value: float,\n        label: str,\n    ):\n        \"\"\"\n        Precision metric for a specific class label.\n\n        Precision is calulated using the number of true-positive pixels divided by\n        the sum of all true-positive and false-positive pixels.\n\n        Parameters\n        ----------\n        value : float\n            The computed precision value.\n        label : str\n            The label for which the precision is calculated.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Precision.value,\n            value=value,\n            parameters={\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def recall(\n        cls,\n        value: float,\n        label: str,\n    ):\n        \"\"\"\n        Recall metric for a specific class label.\n\n        Recall is calulated using the number of true-positive pixels divided by\n        the sum of all true-positive and false-negative pixels.\n\n        Parameters\n        ----------\n        value : float\n            The computed recall value.\n        label : str\n            The label for which the recall is calculated.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.Recall.value,\n            value=value,\n            parameters={\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def f1_score(\n        cls,\n        value: float,\n        label: str,\n    ):\n        \"\"\"\n        F1 score for a specific class label.\n\n        Parameters\n        ----------\n        value : float\n            The computed F1 score.\n        label : str\n            The label for which the F1 score is calculated.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.F1.value,\n            value=value,\n            parameters={\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def iou(\n        cls,\n        value: float,\n        label: str,\n    ):\n        \"\"\"\n        Intersection over Union (IOU) ratio for a specific class label.\n\n        Parameters\n        ----------\n        value : float\n            The computed IOU ratio.\n        label : str\n            The label for which the IOU is calculated.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.IOU.value,\n            value=value,\n            parameters={\n                \"label\": label,\n            },\n        )\n\n    @classmethod\n    def mean_iou(cls, value: float):\n        \"\"\"\n        Mean Intersection over Union (mIOU) ratio.\n\n        The mIOU value is computed by averaging IOU over all labels.\n\n        Parameters\n        ----------\n        value : float\n            The mIOU value.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(type=MetricType.mIOU.value, value=value, parameters={})\n\n    @classmethod\n    def accuracy(cls, value: float):\n        \"\"\"\n        Accuracy metric computed over all labels.\n\n        Parameters\n        ----------\n        value : float\n            The accuracy value.\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(type=MetricType.Accuracy.value, value=value, parameters={})\n\n    @classmethod\n    def confusion_matrix(\n        cls,\n        confusion_matrix: dict[\n            str,  # ground truth label value\n            dict[\n                str,  # prediction label value\n                dict[str, float],  # iou\n            ],\n        ],\n        unmatched_predictions: dict[\n            str,  # prediction label value\n            dict[str, float],  # pixel ratio\n        ],\n        unmatched_ground_truths: dict[\n            str,  # ground truth label value\n            dict[str, float],  # pixel ratio\n        ],\n    ):\n        \"\"\"\n        The confusion matrix and related metrics for semantic segmentation tasks.\n\n        This class encapsulates detailed information about the model's performance, including correct\n        predictions, misclassifications, unmatched_predictions (subset of false positives), and unmatched ground truths\n        (subset of false negatives). It provides counts for each category to facilitate in-depth analysis.\n\n        Confusion Matrix Format:\n        {\n            &lt;ground truth label&gt;: {\n                &lt;prediction label&gt;: {\n                    'iou': &lt;float&gt;,\n                },\n            },\n        }\n\n        Unmatched Predictions Format:\n        {\n            &lt;prediction label&gt;: {\n                'iou': &lt;float&gt;,\n            },\n        }\n\n        Unmatched Ground Truths Format:\n        {\n            &lt;ground truth label&gt;: {\n                'iou': &lt;float&gt;,\n            },\n        }\n\n        Parameters\n        ----------\n        confusion_matrix : dict\n            Nested dictionaries representing the Intersection over Union (IOU) scores for each\n            ground truth label and prediction label pair.\n        unmatched_predictions : dict\n            Dictionary representing the pixel ratios for predicted labels that do not correspond\n            to any ground truth labels (false positives).\n        unmatched_ground_truths : dict\n            Dictionary representing the pixel ratios for ground truth labels that were not predicted\n            (false negatives).\n\n        Returns\n        -------\n        Metric\n        \"\"\"\n        return cls(\n            type=MetricType.ConfusionMatrix.value,\n            value={\n                \"confusion_matrix\": confusion_matrix,\n                \"unmatched_predictions\": unmatched_predictions,\n                \"unmatched_ground_truths\": unmatched_ground_truths,\n            },\n            parameters={},\n        )\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.accuracy","title":"<code>accuracy(value)</code>  <code>classmethod</code>","text":"<p>Accuracy metric computed over all labels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The accuracy value.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef accuracy(cls, value: float):\n    \"\"\"\n    Accuracy metric computed over all labels.\n\n    Parameters\n    ----------\n    value : float\n        The accuracy value.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(type=MetricType.Accuracy.value, value=value, parameters={})\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.confusion_matrix","title":"<code>confusion_matrix(confusion_matrix, unmatched_predictions, unmatched_ground_truths)</code>  <code>classmethod</code>","text":"<p>The confusion matrix and related metrics for semantic segmentation tasks.</p> <p>This class encapsulates detailed information about the model's performance, including correct predictions, misclassifications, unmatched_predictions (subset of false positives), and unmatched ground truths (subset of false negatives). It provides counts for each category to facilitate in-depth analysis.</p> <p>Confusion Matrix Format: {     : {         : {             'iou': ,         },     }, } <p>Unmatched Predictions Format: {     : {         'iou': ,     }, } <p>Unmatched Ground Truths Format: {     : {         'iou': ,     }, } <p>Parameters:</p> Name Type Description Default <code>confusion_matrix</code> <code>dict</code> <p>Nested dictionaries representing the Intersection over Union (IOU) scores for each ground truth label and prediction label pair.</p> required <code>unmatched_predictions</code> <code>dict</code> <p>Dictionary representing the pixel ratios for predicted labels that do not correspond to any ground truth labels (false positives).</p> required <code>unmatched_ground_truths</code> <code>dict</code> <p>Dictionary representing the pixel ratios for ground truth labels that were not predicted (false negatives).</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef confusion_matrix(\n    cls,\n    confusion_matrix: dict[\n        str,  # ground truth label value\n        dict[\n            str,  # prediction label value\n            dict[str, float],  # iou\n        ],\n    ],\n    unmatched_predictions: dict[\n        str,  # prediction label value\n        dict[str, float],  # pixel ratio\n    ],\n    unmatched_ground_truths: dict[\n        str,  # ground truth label value\n        dict[str, float],  # pixel ratio\n    ],\n):\n    \"\"\"\n    The confusion matrix and related metrics for semantic segmentation tasks.\n\n    This class encapsulates detailed information about the model's performance, including correct\n    predictions, misclassifications, unmatched_predictions (subset of false positives), and unmatched ground truths\n    (subset of false negatives). It provides counts for each category to facilitate in-depth analysis.\n\n    Confusion Matrix Format:\n    {\n        &lt;ground truth label&gt;: {\n            &lt;prediction label&gt;: {\n                'iou': &lt;float&gt;,\n            },\n        },\n    }\n\n    Unmatched Predictions Format:\n    {\n        &lt;prediction label&gt;: {\n            'iou': &lt;float&gt;,\n        },\n    }\n\n    Unmatched Ground Truths Format:\n    {\n        &lt;ground truth label&gt;: {\n            'iou': &lt;float&gt;,\n        },\n    }\n\n    Parameters\n    ----------\n    confusion_matrix : dict\n        Nested dictionaries representing the Intersection over Union (IOU) scores for each\n        ground truth label and prediction label pair.\n    unmatched_predictions : dict\n        Dictionary representing the pixel ratios for predicted labels that do not correspond\n        to any ground truth labels (false positives).\n    unmatched_ground_truths : dict\n        Dictionary representing the pixel ratios for ground truth labels that were not predicted\n        (false negatives).\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.ConfusionMatrix.value,\n        value={\n            \"confusion_matrix\": confusion_matrix,\n            \"unmatched_predictions\": unmatched_predictions,\n            \"unmatched_ground_truths\": unmatched_ground_truths,\n        },\n        parameters={},\n    )\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.f1_score","title":"<code>f1_score(value, label)</code>  <code>classmethod</code>","text":"<p>F1 score for a specific class label.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The computed F1 score.</p> required <code>label</code> <code>str</code> <p>The label for which the F1 score is calculated.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef f1_score(\n    cls,\n    value: float,\n    label: str,\n):\n    \"\"\"\n    F1 score for a specific class label.\n\n    Parameters\n    ----------\n    value : float\n        The computed F1 score.\n    label : str\n        The label for which the F1 score is calculated.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.F1.value,\n        value=value,\n        parameters={\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.iou","title":"<code>iou(value, label)</code>  <code>classmethod</code>","text":"<p>Intersection over Union (IOU) ratio for a specific class label.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The computed IOU ratio.</p> required <code>label</code> <code>str</code> <p>The label for which the IOU is calculated.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef iou(\n    cls,\n    value: float,\n    label: str,\n):\n    \"\"\"\n    Intersection over Union (IOU) ratio for a specific class label.\n\n    Parameters\n    ----------\n    value : float\n        The computed IOU ratio.\n    label : str\n        The label for which the IOU is calculated.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.IOU.value,\n        value=value,\n        parameters={\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.mean_iou","title":"<code>mean_iou(value)</code>  <code>classmethod</code>","text":"<p>Mean Intersection over Union (mIOU) ratio.</p> <p>The mIOU value is computed by averaging IOU over all labels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The mIOU value.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef mean_iou(cls, value: float):\n    \"\"\"\n    Mean Intersection over Union (mIOU) ratio.\n\n    The mIOU value is computed by averaging IOU over all labels.\n\n    Parameters\n    ----------\n    value : float\n        The mIOU value.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(type=MetricType.mIOU.value, value=value, parameters={})\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.precision","title":"<code>precision(value, label)</code>  <code>classmethod</code>","text":"<p>Precision metric for a specific class label.</p> <p>Precision is calulated using the number of true-positive pixels divided by the sum of all true-positive and false-positive pixels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The computed precision value.</p> required <code>label</code> <code>str</code> <p>The label for which the precision is calculated.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef precision(\n    cls,\n    value: float,\n    label: str,\n):\n    \"\"\"\n    Precision metric for a specific class label.\n\n    Precision is calulated using the number of true-positive pixels divided by\n    the sum of all true-positive and false-positive pixels.\n\n    Parameters\n    ----------\n    value : float\n        The computed precision value.\n    label : str\n        The label for which the precision is calculated.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Precision.value,\n        value=value,\n        parameters={\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"semantic_segmentation/metrics/#valor_lite.semantic_segmentation.metric.Metric.recall","title":"<code>recall(value, label)</code>  <code>classmethod</code>","text":"<p>Recall metric for a specific class label.</p> <p>Recall is calulated using the number of true-positive pixels divided by the sum of all true-positive and false-negative pixels.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The computed recall value.</p> required <code>label</code> <code>str</code> <p>The label for which the recall is calculated.</p> required <p>Returns:</p> Type Description <code>Metric</code> Source code in <code>valor_lite/semantic_segmentation/metric.py</code> <pre><code>@classmethod\ndef recall(\n    cls,\n    value: float,\n    label: str,\n):\n    \"\"\"\n    Recall metric for a specific class label.\n\n    Recall is calulated using the number of true-positive pixels divided by\n    the sum of all true-positive and false-negative pixels.\n\n    Parameters\n    ----------\n    value : float\n        The computed recall value.\n    label : str\n        The label for which the recall is calculated.\n\n    Returns\n    -------\n    Metric\n    \"\"\"\n    return cls(\n        type=MetricType.Recall.value,\n        value=value,\n        parameters={\n            \"label\": label,\n        },\n    )\n</code></pre>"},{"location":"text_generation/documentation/","title":"Documentation","text":"<p>Documentation</p>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Context","title":"<code>valor_lite.text_generation.Context</code>  <code>dataclass</code>","text":"<p>Contextual ground truth and prediction.</p> <p>Attributes:</p> Name Type Description <code>groundtruth</code> <code>list[str]</code> <p>The definitive context.</p> <code>prediction</code> <code>list[str]</code> <p>Any retrieved context from a retrieval-augmented-generation (RAG) pipeline.</p> <p>Examples:</p> <p>... context = Context( ...     groundtruth=[...], ...     prediction=[...], ... )</p> Source code in <code>valor_lite/text_generation/annotation.py</code> <pre><code>@dataclass\nclass Context:\n    \"\"\"\n    Contextual ground truth and prediction.\n\n    Attributes\n    ----------\n    groundtruth : list[str]\n        The definitive context.\n    prediction : list[str]\n        Any retrieved context from a retrieval-augmented-generation (RAG) pipeline.\n\n    Examples\n    --------\n    ... context = Context(\n    ...     groundtruth=[...],\n    ...     prediction=[...],\n    ... )\n    \"\"\"\n\n    groundtruth: list[str] = field(default_factory=list)\n    prediction: list[str] = field(default_factory=list)\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.QueryResponse","title":"<code>valor_lite.text_generation.QueryResponse</code>  <code>dataclass</code>","text":"<p>Text generation data structure containing ground truths and predictions.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>str</code> <p>The user query.</p> <code>response</code> <code>str</code> <p>The language model's response.</p> <code>context</code> <code>Context</code> <p>Any context provided to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query = QueryResponse(\n...     query='When was George Washington born?',\n...     response=\"February 22, 1732\",\n...     context=Context(\n...         groundtruth=[\"02/22/1732\"],\n...         prediction=[\"02/22/1732\"],\n...     ),\n... )\n</code></pre> Source code in <code>valor_lite/text_generation/annotation.py</code> <pre><code>@dataclass\nclass QueryResponse:\n    \"\"\"\n    Text generation data structure containing ground truths and predictions.\n\n    Attributes\n    ----------\n    query : str\n        The user query.\n    response : str\n        The language model's response.\n    context : Context\n        Any context provided to the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; query = QueryResponse(\n    ...     query='When was George Washington born?',\n    ...     response=\"February 22, 1732\",\n    ...     context=Context(\n    ...         groundtruth=[\"02/22/1732\"],\n    ...         prediction=[\"02/22/1732\"],\n    ...     ),\n    ... )\n    \"\"\"\n\n    query: str\n    response: str\n    context: Context | None = field(default=None)\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator","title":"<code>valor_lite.text_generation.Evaluator</code>","text":"<p>Parent class for all LLM clients.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>(ClientWrapper, optional)</code> <p>An optional client to compute llm-guided metrics.</p> <code>retries</code> <code>int</code> <p>The number of times to retry the API call if it fails. Defaults to 0, indicating that the call will not be retried.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>class Evaluator:\n    \"\"\"\n    Parent class for all LLM clients.\n\n    Attributes\n    ----------\n    client : ClientWrapper, optional\n        An optional client to compute llm-guided metrics.\n    retries : int\n        The number of times to retry the API call if it fails. Defaults to 0, indicating\n        that the call will not be retried.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: ClientWrapper | None = None,\n        retries: int = 0,\n        default_system_prompt: str = \"You are a helpful assistant.\",\n    ):\n        \"\"\"\n        Creates an instance of a generic LLM client.\n\n        Parameters\n        ----------\n        client : ClientWrapper, optional\n            Any LLM client that conforms to _ClientWrapper. Required for LLM-guided metrics.\n        retries : int, default=0\n            The number of times to retry the API call if it fails. Defaults to 0, indicating\n            that the call will not be retried.\n        default_system_prompt : str, default=\"You are a helpful assistant.\"\n            The default system prompt that is given to the evaluating LLM.\n        \"\"\"\n\n        self.client = client\n        self.retries = retries\n        self.default_system_prompt = default_system_prompt\n\n    @classmethod\n    def openai(\n        cls,\n        model_name: str = \"gpt-3.5-turbo\",\n        api_key: str | None = None,\n        retries: int = 0,\n        seed: int | None = None,\n        default_system_prompt: str = \"You are a helpful assistant.\",\n    ):\n        \"\"\"\n        Create an evaluator using OpenAI's client.\n\n        Parameters\n        ----------\n        model_name : str, default=\"gpt-3.5-turbo\"\n            The model to use. Defaults to \"gpt-3.5-turbo\".\n        api_key : str, optional\n            The OpenAI API key to use. If not specified, then the OPENAI_API_KEY environment\n            variable will be used.\n        retries : int, default=0\n            The number of times to retry the API call if it fails. Defaults to 0, indicating\n            that the call will not be retried. For example, if self.retries is set to 3,\n            this means that the call will be retried up to 3 times, for a maximum of 4 calls.\n        seed : int, optional\n            An optional seed can be provided to GPT to get deterministic results.\n        default_system_prompt : str, default=\"You are a helpful assistant.\"\n            The default system prompt that is given to the evaluating LLM.\n        \"\"\"\n        if seed is not None:\n            if retries != 0:\n                raise ValueError(\n                    \"Seed is provided, but retries is not 0. Retries should be 0 when seed is provided.\"\n                )\n        client = OpenAIWrapper(\n            api_key=api_key,\n            model_name=model_name,\n            seed=seed,\n        )\n        return cls(\n            client=client,\n            retries=retries,\n            default_system_prompt=default_system_prompt,\n        )\n\n    @classmethod\n    def mistral(\n        cls,\n        model_name: str = \"mistral-small-latest\",\n        api_key: str | None = None,\n        retries: int = 0,\n        default_system_prompt: str = \"You are a helpful assistant.\",\n    ):\n        \"\"\"\n        Create an evaluator using the Mistral API.\n\n        Parameters\n        ----------\n        model_name : str, default=\"mistral-small-latest\"\n            The model to use. Defaults to \"mistral-small-latest\".\n        api_key : str, optional\n            The Mistral API key to use. If not specified, then the MISTRAL_API_KEY environment\n            variable will be used.\n        retries : int, default=0\n            The number of times to retry the API call if it fails. Defaults to 0, indicating\n            that the call will not be retried. For example, if self.retries is set to 3,\n            this means that the call will be retried up to 3 times, for a maximum of 4 calls.\n        default_system_prompt : str, default=\"You are a helpful assistant.\"\n            The default system prompt that is given to the evaluating LLM.\n        \"\"\"\n        client = MistralWrapper(\n            api_key=api_key,\n            model_name=model_name,\n        )\n        return cls(\n            client=client,\n            retries=retries,\n            default_system_prompt=default_system_prompt,\n        )\n\n    @llm_guided_metric\n    def compute_answer_correctness(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute answer correctness. Answer correctness is computed as an f1 score obtained\n        by comparing prediction statements to ground truth statements.\n\n        If there are multiple ground truths, then the f1 score is computed for each ground\n        truth and the maximum score is returned.\n\n        This metric was adapted from RAGAS. We follow a similar prompting strategy and\n        computation, however we do not do a weighted sum with an answer similarity score\n        using embeddings.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The answer correctness score between 0 and 1. Higher values indicate that the\n            answer is more correct. A score of 1 indicates that all statements in the\n            prediction are supported by the ground truth and all statements in the ground\n            truth are present in the prediction.\n        \"\"\"\n        if not response.context:\n            raise ValueError(\"The answer correctness metric requires context.\")\n\n        result = calculate_answer_correctness(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            query=response.query,\n            response=response.response,\n            groundtruths=response.context.groundtruth,\n        )\n        return Metric.answer_correctness(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_answer_relevance(self, response: QueryResponse) -&gt; Metric:\n        \"\"\"\n        Compute answer relevance, the proportion of the model response that is\n        relevant to the query, for a single piece of text.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The answer relevance score between 0 and 1. A score of 1 indicates that all\n            statements are relevant to the query.\n        \"\"\"\n        result = calculate_answer_relevance(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            query=response.query,\n            response=response.response,\n        )\n        return Metric.answer_relevance(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_bias(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute bias, the proportion of model opinions that are biased.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        float\n            The bias score between 0 and 1. A score of 1 indicates that all opinions in\n            the text are biased.\n        \"\"\"\n        result = calculate_bias(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            response=response.response,\n        )\n        return Metric.bias(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_context_precision(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute context precision, a score for evaluating the retrieval\n        mechanism of a RAG model.\n\n        First, an LLM is prompted to determine if each context in the context\n        list is useful for producing the ground truth answer to the query.\n\n        If there are multiple ground truths, then the verdict is \"yes\" for a\n        context if that context is useful for producing any of the ground truth\n        answers, and \"no\" otherwise.\n\n        Then, using these verdicts, the context precision score is computed as\n        a weighted sum of the precision at k for each k from 1 to the length\n        of the context list.\n\n        Note that the earlier a piece of context appears in the context list,\n        the more important it is in the computation of this score. For example,\n        the first context in the context list will be included in every precision\n        at k computation, so will have a large influence on the final score,\n        whereas the last context will only be used for the last precision at\n        k computation, so will have a small influence on the final score.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The context precision score between 0 and 1. A higher score indicates\n            better context precision.\n        \"\"\"\n        if not response.context:\n            raise ValueError(\"The context precision metric requires context.\")\n\n        result = calculate_context_precision(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            query=response.query,\n            predicted_context=response.context.prediction,\n            groundtruth_context=response.context.groundtruth,\n        )\n        return Metric.context_precision(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_context_recall(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute context recall, a score for evaluating the retrieval mechanism of a RAG model.\n\n        The context recall score is the proportion of statements in the ground truth\n        that are attributable to the context list.\n\n        If multiple ground truths are provided, then the context recall score is\n        computed for each ground truth and the maximum score is returned.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The context recall score between 0 and 1. A score of 1 indicates that\n            all ground truth statements are attributable to the contexts in the context list.\n        \"\"\"\n        if not response.context:\n            raise ValueError(\"The context recall metric requires context.\")\n\n        result = calculate_context_recall(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            predicted_context=response.context.prediction,\n            groundtruth_context=response.context.groundtruth,\n        )\n        return Metric.context_recall(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_context_relevance(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute context relevance, the proportion of contexts in the context list\n        that are relevant to the query.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The context relevance score between 0 and 1. A score of 0 indicates\n            that none of the contexts are relevant and a score of 1 indicates\n            that all of the contexts are relevant.\n        \"\"\"\n        if not response.context:\n            raise ValueError(\"The context relevance metric requires context.\")\n\n        result = calculate_context_relevance(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            query=response.query,\n            context=response.context.prediction,\n        )\n        return Metric.context_relevance(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_faithfulness(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute the faithfulness score. The faithfulness score is the proportion\n        of claims in the text that are implied by the list of contexts. Claims\n        that contradict the list of contexts and claims that are unrelated to\n        the list of contexts both count against the score.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The faithfulness score between 0 and 1. A score of 1 indicates that\n            all claims in the text are implied by the list of contexts.\n        \"\"\"\n\n        if not response.context:\n            raise ValueError(\"The faithfulness metric requires context.\")\n\n        result = calculate_faithfulness(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            response=response.response,\n            context=response.context.prediction,\n        )\n        return Metric.faithfulness(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_hallucination(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute the hallucination score, the proportion of contexts in the context\n        list that are contradicted by the text.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The hallucination score between 0 and 1. A score of 1 indicates that\n            all contexts are contradicted by the text.\n        \"\"\"\n\n        if not response.context:\n            raise ValueError(\"The hallucination metric requires context.\")\n\n        result = calculate_hallucination(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            response=response.response,\n            context=response.context.prediction,\n        )\n        return Metric.hallucination(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_summary_coherence(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute summary coherence, the collective quality of a summary.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The summary coherence score between 1 and 5. A score of 1 indicates\n            the lowest summary coherence and a score of 5 indicates the highest\n            summary coherence.\n        \"\"\"\n        result = calculate_summary_coherence(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            text=response.query,\n            summary=response.response,\n        )\n        return Metric.summary_coherence(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @llm_guided_metric\n    def compute_toxicity(\n        self,\n        response: QueryResponse,\n    ) -&gt; Metric:\n        \"\"\"\n        Compute toxicity, the portion of opinions that are toxic.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n\n        Returns\n        -------\n        Metric\n            The toxicity score will be evaluated as a float between 0 and 1, with\n            1 indicating that all opinions in the text are toxic.\n        \"\"\"\n        result = calculate_toxicity(\n            client=self.client,  # type: ignore - wrapper handles None case\n            system_prompt=self.default_system_prompt,\n            response=response.response,\n        )\n        return Metric.toxicity(\n            value=result,\n            model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n            retries=self.retries,\n        )\n\n    @staticmethod\n    def compute_rouge(\n        response: QueryResponse,\n        rouge_types: list[str] = [\n            \"rouge1\",\n            \"rouge2\",\n            \"rougeL\",\n            \"rougeLsum\",\n        ],\n        use_stemmer: bool = False,\n    ) -&gt; list[Metric]:\n        \"\"\"\n        Calculate ROUGE scores for a model response given some set of references.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n        rouge_types : list[str], optional\n            A list of rouge types to calculate.\n            Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'].\n        use_stemmer: bool, default=False\n            If True, uses Porter stemmer to strip word suffixes. Defaults to False.\n\n        Returns\n        -------\n        list[Metric]\n        \"\"\"\n\n        if not response.context:\n            raise ValueError(\"ROUGE metrics require context.\")\n\n        results = calculate_rouge_scores(\n            prediction=response.response,\n            references=response.context.groundtruth,\n            rouge_types=rouge_types,\n            use_stemmer=use_stemmer,\n        )\n        return [\n            Metric.rouge(\n                value=result,\n                rouge_type=rouge_type,\n                use_stemmer=use_stemmer,\n            )\n            for rouge_type, result in results.items()\n        ]\n\n    @staticmethod\n    def compute_sentence_bleu(\n        response: QueryResponse,\n        weights: list[float] = [0.25, 0.25, 0.25, 0.25],\n    ) -&gt; Metric:\n        \"\"\"\n        Calculate sentence BLEU scores for a set of model response - ground truth pairs.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n        weights: list[float], default=[0.25, 0.25, 0.25, 0.25]\n            The default BLEU calculates a score for up to 4-grams using uniform\n            weights (this is called BLEU-4). To evaluate your translations with\n            higher/lower order ngrams, use customized weights. Example: when accounting\n            for up to 5-grams with uniform weights (this is called BLEU-5) use [1/5]*5\n        \"\"\"\n\n        if not response.context:\n            raise ValueError(\"The sentence BLEU metric requires context.\")\n\n        result = calculate_sentence_bleu(\n            prediction=response.response,\n            references=response.context.groundtruth,\n            weights=weights,\n        )\n        return Metric.bleu(\n            value=result,\n            weights=weights,\n        )\n\n    def compute_all(\n        self,\n        response: QueryResponse,\n        bleu_weights: list[float] = [0.25, 0.25, 0.25, 0.25],\n        rouge_types: list[str] = [\n            \"rouge1\",\n            \"rouge2\",\n            \"rougeL\",\n            \"rougeLsum\",\n        ],\n        rouge_use_stemmer: bool = False,\n    ) -&gt; dict[MetricType, list[Metric]]:\n        \"\"\"\n        Computes all available metrics.\n\n        Parameters\n        ----------\n        response: QueryResponse\n            A user query with ground truth and generated response.\n        bleu_weights: list[float], default=[0.25, 0.25, 0.25, 0.25]\n            The default BLEU calculates a score for up to 4-grams using uniform\n            weights (this is called BLEU-4). To evaluate your translations with\n            higher/lower order ngrams, use customized weights. Example: when accounting\n            for up to 5-grams with uniform weights (this is called BLEU-5) use [1/5]*5\n        rouge_types : list[str], optional\n            A list of rouge types to calculate.\n            Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'].\n        rouge_use_stemmer: bool, default=False\n            If True, uses Porter stemmer to strip word suffixes. Defaults to False.\n        \"\"\"\n        results = dict()\n        results[MetricType.AnswerCorrectness] = [\n            self.compute_answer_correctness(response)\n        ]\n        results[MetricType.AnswerRelevance] = [\n            self.compute_answer_relevance(response)\n        ]\n        results[MetricType.Bias] = [self.compute_bias(response)]\n        results[MetricType.ContextPrecision] = [\n            self.compute_context_precision(response)\n        ]\n        results[MetricType.ContextRecall] = [\n            self.compute_context_recall(response)\n        ]\n        results[MetricType.ContextRelevance] = [\n            self.compute_context_relevance(response)\n        ]\n        results[MetricType.Faithfulness] = [\n            self.compute_faithfulness(response)\n        ]\n        results[MetricType.Hallucination] = [\n            self.compute_hallucination(response)\n        ]\n        results[MetricType.SummaryCoherence] = [\n            self.compute_summary_coherence(response)\n        ]\n        results[MetricType.Toxicity] = [self.compute_toxicity(response)]\n        results[MetricType.ROUGE] = self.compute_rouge(\n            response=response,\n            rouge_types=rouge_types,\n            use_stemmer=rouge_use_stemmer,\n        )\n        results[MetricType.BLEU] = [\n            self.compute_sentence_bleu(response=response, weights=bleu_weights)\n        ]\n        return results\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.__init__","title":"<code>__init__(client=None, retries=0, default_system_prompt='You are a helpful assistant.')</code>","text":"<p>Creates an instance of a generic LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientWrapper</code> <p>Any LLM client that conforms to _ClientWrapper. Required for LLM-guided metrics.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The number of times to retry the API call if it fails. Defaults to 0, indicating that the call will not be retried.</p> <code>0</code> <code>default_system_prompt</code> <code>str</code> <p>The default system prompt that is given to the evaluating LLM.</p> <code>\"You are a helpful assistant.\"</code> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>def __init__(\n    self,\n    client: ClientWrapper | None = None,\n    retries: int = 0,\n    default_system_prompt: str = \"You are a helpful assistant.\",\n):\n    \"\"\"\n    Creates an instance of a generic LLM client.\n\n    Parameters\n    ----------\n    client : ClientWrapper, optional\n        Any LLM client that conforms to _ClientWrapper. Required for LLM-guided metrics.\n    retries : int, default=0\n        The number of times to retry the API call if it fails. Defaults to 0, indicating\n        that the call will not be retried.\n    default_system_prompt : str, default=\"You are a helpful assistant.\"\n        The default system prompt that is given to the evaluating LLM.\n    \"\"\"\n\n    self.client = client\n    self.retries = retries\n    self.default_system_prompt = default_system_prompt\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_all","title":"<code>compute_all(response, bleu_weights=[0.25, 0.25, 0.25, 0.25], rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], rouge_use_stemmer=False)</code>","text":"<p>Computes all available metrics.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <code>bleu_weights</code> <code>list[float]</code> <p>The default BLEU calculates a score for up to 4-grams using uniform weights (this is called BLEU-4). To evaluate your translations with higher/lower order ngrams, use customized weights. Example: when accounting for up to 5-grams with uniform weights (this is called BLEU-5) use [1/5]*5</p> <code>[0.25, 0.25, 0.25, 0.25]</code> <code>rouge_types</code> <code>list[str]</code> <p>A list of rouge types to calculate. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'].</p> <code>['rouge1', 'rouge2', 'rougeL', 'rougeLsum']</code> <code>rouge_use_stemmer</code> <code>bool</code> <p>If True, uses Porter stemmer to strip word suffixes. Defaults to False.</p> <code>False</code> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>def compute_all(\n    self,\n    response: QueryResponse,\n    bleu_weights: list[float] = [0.25, 0.25, 0.25, 0.25],\n    rouge_types: list[str] = [\n        \"rouge1\",\n        \"rouge2\",\n        \"rougeL\",\n        \"rougeLsum\",\n    ],\n    rouge_use_stemmer: bool = False,\n) -&gt; dict[MetricType, list[Metric]]:\n    \"\"\"\n    Computes all available metrics.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n    bleu_weights: list[float], default=[0.25, 0.25, 0.25, 0.25]\n        The default BLEU calculates a score for up to 4-grams using uniform\n        weights (this is called BLEU-4). To evaluate your translations with\n        higher/lower order ngrams, use customized weights. Example: when accounting\n        for up to 5-grams with uniform weights (this is called BLEU-5) use [1/5]*5\n    rouge_types : list[str], optional\n        A list of rouge types to calculate.\n        Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'].\n    rouge_use_stemmer: bool, default=False\n        If True, uses Porter stemmer to strip word suffixes. Defaults to False.\n    \"\"\"\n    results = dict()\n    results[MetricType.AnswerCorrectness] = [\n        self.compute_answer_correctness(response)\n    ]\n    results[MetricType.AnswerRelevance] = [\n        self.compute_answer_relevance(response)\n    ]\n    results[MetricType.Bias] = [self.compute_bias(response)]\n    results[MetricType.ContextPrecision] = [\n        self.compute_context_precision(response)\n    ]\n    results[MetricType.ContextRecall] = [\n        self.compute_context_recall(response)\n    ]\n    results[MetricType.ContextRelevance] = [\n        self.compute_context_relevance(response)\n    ]\n    results[MetricType.Faithfulness] = [\n        self.compute_faithfulness(response)\n    ]\n    results[MetricType.Hallucination] = [\n        self.compute_hallucination(response)\n    ]\n    results[MetricType.SummaryCoherence] = [\n        self.compute_summary_coherence(response)\n    ]\n    results[MetricType.Toxicity] = [self.compute_toxicity(response)]\n    results[MetricType.ROUGE] = self.compute_rouge(\n        response=response,\n        rouge_types=rouge_types,\n        use_stemmer=rouge_use_stemmer,\n    )\n    results[MetricType.BLEU] = [\n        self.compute_sentence_bleu(response=response, weights=bleu_weights)\n    ]\n    return results\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_answer_correctness","title":"<code>compute_answer_correctness(response)</code>","text":"<p>Compute answer correctness. Answer correctness is computed as an f1 score obtained by comparing prediction statements to ground truth statements.</p> <p>If there are multiple ground truths, then the f1 score is computed for each ground truth and the maximum score is returned.</p> <p>This metric was adapted from RAGAS. We follow a similar prompting strategy and computation, however we do not do a weighted sum with an answer similarity score using embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The answer correctness score between 0 and 1. Higher values indicate that the answer is more correct. A score of 1 indicates that all statements in the prediction are supported by the ground truth and all statements in the ground truth are present in the prediction.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_answer_correctness(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute answer correctness. Answer correctness is computed as an f1 score obtained\n    by comparing prediction statements to ground truth statements.\n\n    If there are multiple ground truths, then the f1 score is computed for each ground\n    truth and the maximum score is returned.\n\n    This metric was adapted from RAGAS. We follow a similar prompting strategy and\n    computation, however we do not do a weighted sum with an answer similarity score\n    using embeddings.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The answer correctness score between 0 and 1. Higher values indicate that the\n        answer is more correct. A score of 1 indicates that all statements in the\n        prediction are supported by the ground truth and all statements in the ground\n        truth are present in the prediction.\n    \"\"\"\n    if not response.context:\n        raise ValueError(\"The answer correctness metric requires context.\")\n\n    result = calculate_answer_correctness(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        query=response.query,\n        response=response.response,\n        groundtruths=response.context.groundtruth,\n    )\n    return Metric.answer_correctness(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_answer_relevance","title":"<code>compute_answer_relevance(response)</code>","text":"<p>Compute answer relevance, the proportion of the model response that is relevant to the query, for a single piece of text.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The answer relevance score between 0 and 1. A score of 1 indicates that all statements are relevant to the query.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_answer_relevance(self, response: QueryResponse) -&gt; Metric:\n    \"\"\"\n    Compute answer relevance, the proportion of the model response that is\n    relevant to the query, for a single piece of text.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The answer relevance score between 0 and 1. A score of 1 indicates that all\n        statements are relevant to the query.\n    \"\"\"\n    result = calculate_answer_relevance(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        query=response.query,\n        response=response.response,\n    )\n    return Metric.answer_relevance(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_bias","title":"<code>compute_bias(response)</code>","text":"<p>Compute bias, the proportion of model opinions that are biased.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The bias score between 0 and 1. A score of 1 indicates that all opinions in the text are biased.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_bias(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute bias, the proportion of model opinions that are biased.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    float\n        The bias score between 0 and 1. A score of 1 indicates that all opinions in\n        the text are biased.\n    \"\"\"\n    result = calculate_bias(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        response=response.response,\n    )\n    return Metric.bias(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_context_precision","title":"<code>compute_context_precision(response)</code>","text":"<p>Compute context precision, a score for evaluating the retrieval mechanism of a RAG model.</p> <p>First, an LLM is prompted to determine if each context in the context list is useful for producing the ground truth answer to the query.</p> <p>If there are multiple ground truths, then the verdict is \"yes\" for a context if that context is useful for producing any of the ground truth answers, and \"no\" otherwise.</p> <p>Then, using these verdicts, the context precision score is computed as a weighted sum of the precision at k for each k from 1 to the length of the context list.</p> <p>Note that the earlier a piece of context appears in the context list, the more important it is in the computation of this score. For example, the first context in the context list will be included in every precision at k computation, so will have a large influence on the final score, whereas the last context will only be used for the last precision at k computation, so will have a small influence on the final score.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The context precision score between 0 and 1. A higher score indicates better context precision.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_context_precision(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute context precision, a score for evaluating the retrieval\n    mechanism of a RAG model.\n\n    First, an LLM is prompted to determine if each context in the context\n    list is useful for producing the ground truth answer to the query.\n\n    If there are multiple ground truths, then the verdict is \"yes\" for a\n    context if that context is useful for producing any of the ground truth\n    answers, and \"no\" otherwise.\n\n    Then, using these verdicts, the context precision score is computed as\n    a weighted sum of the precision at k for each k from 1 to the length\n    of the context list.\n\n    Note that the earlier a piece of context appears in the context list,\n    the more important it is in the computation of this score. For example,\n    the first context in the context list will be included in every precision\n    at k computation, so will have a large influence on the final score,\n    whereas the last context will only be used for the last precision at\n    k computation, so will have a small influence on the final score.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The context precision score between 0 and 1. A higher score indicates\n        better context precision.\n    \"\"\"\n    if not response.context:\n        raise ValueError(\"The context precision metric requires context.\")\n\n    result = calculate_context_precision(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        query=response.query,\n        predicted_context=response.context.prediction,\n        groundtruth_context=response.context.groundtruth,\n    )\n    return Metric.context_precision(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_context_recall","title":"<code>compute_context_recall(response)</code>","text":"<p>Compute context recall, a score for evaluating the retrieval mechanism of a RAG model.</p> <p>The context recall score is the proportion of statements in the ground truth that are attributable to the context list.</p> <p>If multiple ground truths are provided, then the context recall score is computed for each ground truth and the maximum score is returned.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The context recall score between 0 and 1. A score of 1 indicates that all ground truth statements are attributable to the contexts in the context list.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_context_recall(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute context recall, a score for evaluating the retrieval mechanism of a RAG model.\n\n    The context recall score is the proportion of statements in the ground truth\n    that are attributable to the context list.\n\n    If multiple ground truths are provided, then the context recall score is\n    computed for each ground truth and the maximum score is returned.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The context recall score between 0 and 1. A score of 1 indicates that\n        all ground truth statements are attributable to the contexts in the context list.\n    \"\"\"\n    if not response.context:\n        raise ValueError(\"The context recall metric requires context.\")\n\n    result = calculate_context_recall(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        predicted_context=response.context.prediction,\n        groundtruth_context=response.context.groundtruth,\n    )\n    return Metric.context_recall(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_context_relevance","title":"<code>compute_context_relevance(response)</code>","text":"<p>Compute context relevance, the proportion of contexts in the context list that are relevant to the query.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The context relevance score between 0 and 1. A score of 0 indicates that none of the contexts are relevant and a score of 1 indicates that all of the contexts are relevant.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_context_relevance(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute context relevance, the proportion of contexts in the context list\n    that are relevant to the query.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The context relevance score between 0 and 1. A score of 0 indicates\n        that none of the contexts are relevant and a score of 1 indicates\n        that all of the contexts are relevant.\n    \"\"\"\n    if not response.context:\n        raise ValueError(\"The context relevance metric requires context.\")\n\n    result = calculate_context_relevance(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        query=response.query,\n        context=response.context.prediction,\n    )\n    return Metric.context_relevance(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_faithfulness","title":"<code>compute_faithfulness(response)</code>","text":"<p>Compute the faithfulness score. The faithfulness score is the proportion of claims in the text that are implied by the list of contexts. Claims that contradict the list of contexts and claims that are unrelated to the list of contexts both count against the score.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The faithfulness score between 0 and 1. A score of 1 indicates that all claims in the text are implied by the list of contexts.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_faithfulness(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute the faithfulness score. The faithfulness score is the proportion\n    of claims in the text that are implied by the list of contexts. Claims\n    that contradict the list of contexts and claims that are unrelated to\n    the list of contexts both count against the score.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The faithfulness score between 0 and 1. A score of 1 indicates that\n        all claims in the text are implied by the list of contexts.\n    \"\"\"\n\n    if not response.context:\n        raise ValueError(\"The faithfulness metric requires context.\")\n\n    result = calculate_faithfulness(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        response=response.response,\n        context=response.context.prediction,\n    )\n    return Metric.faithfulness(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_hallucination","title":"<code>compute_hallucination(response)</code>","text":"<p>Compute the hallucination score, the proportion of contexts in the context list that are contradicted by the text.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The hallucination score between 0 and 1. A score of 1 indicates that all contexts are contradicted by the text.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_hallucination(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute the hallucination score, the proportion of contexts in the context\n    list that are contradicted by the text.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The hallucination score between 0 and 1. A score of 1 indicates that\n        all contexts are contradicted by the text.\n    \"\"\"\n\n    if not response.context:\n        raise ValueError(\"The hallucination metric requires context.\")\n\n    result = calculate_hallucination(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        response=response.response,\n        context=response.context.prediction,\n    )\n    return Metric.hallucination(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_rouge","title":"<code>compute_rouge(response, rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=False)</code>  <code>staticmethod</code>","text":"<p>Calculate ROUGE scores for a model response given some set of references.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <code>rouge_types</code> <code>list[str]</code> <p>A list of rouge types to calculate. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'].</p> <code>['rouge1', 'rouge2', 'rougeL', 'rougeLsum']</code> <code>use_stemmer</code> <code>bool</code> <p>If True, uses Porter stemmer to strip word suffixes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Metric]</code> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@staticmethod\ndef compute_rouge(\n    response: QueryResponse,\n    rouge_types: list[str] = [\n        \"rouge1\",\n        \"rouge2\",\n        \"rougeL\",\n        \"rougeLsum\",\n    ],\n    use_stemmer: bool = False,\n) -&gt; list[Metric]:\n    \"\"\"\n    Calculate ROUGE scores for a model response given some set of references.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n    rouge_types : list[str], optional\n        A list of rouge types to calculate.\n        Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'].\n    use_stemmer: bool, default=False\n        If True, uses Porter stemmer to strip word suffixes. Defaults to False.\n\n    Returns\n    -------\n    list[Metric]\n    \"\"\"\n\n    if not response.context:\n        raise ValueError(\"ROUGE metrics require context.\")\n\n    results = calculate_rouge_scores(\n        prediction=response.response,\n        references=response.context.groundtruth,\n        rouge_types=rouge_types,\n        use_stemmer=use_stemmer,\n    )\n    return [\n        Metric.rouge(\n            value=result,\n            rouge_type=rouge_type,\n            use_stemmer=use_stemmer,\n        )\n        for rouge_type, result in results.items()\n    ]\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_sentence_bleu","title":"<code>compute_sentence_bleu(response, weights=[0.25, 0.25, 0.25, 0.25])</code>  <code>staticmethod</code>","text":"<p>Calculate sentence BLEU scores for a set of model response - ground truth pairs.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <code>weights</code> <code>list[float]</code> <p>The default BLEU calculates a score for up to 4-grams using uniform weights (this is called BLEU-4). To evaluate your translations with higher/lower order ngrams, use customized weights. Example: when accounting for up to 5-grams with uniform weights (this is called BLEU-5) use [1/5]*5</p> <code>[0.25, 0.25, 0.25, 0.25]</code> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@staticmethod\ndef compute_sentence_bleu(\n    response: QueryResponse,\n    weights: list[float] = [0.25, 0.25, 0.25, 0.25],\n) -&gt; Metric:\n    \"\"\"\n    Calculate sentence BLEU scores for a set of model response - ground truth pairs.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n    weights: list[float], default=[0.25, 0.25, 0.25, 0.25]\n        The default BLEU calculates a score for up to 4-grams using uniform\n        weights (this is called BLEU-4). To evaluate your translations with\n        higher/lower order ngrams, use customized weights. Example: when accounting\n        for up to 5-grams with uniform weights (this is called BLEU-5) use [1/5]*5\n    \"\"\"\n\n    if not response.context:\n        raise ValueError(\"The sentence BLEU metric requires context.\")\n\n    result = calculate_sentence_bleu(\n        prediction=response.response,\n        references=response.context.groundtruth,\n        weights=weights,\n    )\n    return Metric.bleu(\n        value=result,\n        weights=weights,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_summary_coherence","title":"<code>compute_summary_coherence(response)</code>","text":"<p>Compute summary coherence, the collective quality of a summary.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The summary coherence score between 1 and 5. A score of 1 indicates the lowest summary coherence and a score of 5 indicates the highest summary coherence.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_summary_coherence(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute summary coherence, the collective quality of a summary.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The summary coherence score between 1 and 5. A score of 1 indicates\n        the lowest summary coherence and a score of 5 indicates the highest\n        summary coherence.\n    \"\"\"\n    result = calculate_summary_coherence(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        text=response.query,\n        summary=response.response,\n    )\n    return Metric.summary_coherence(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.compute_toxicity","title":"<code>compute_toxicity(response)</code>","text":"<p>Compute toxicity, the portion of opinions that are toxic.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>QueryResponse</code> <p>A user query with ground truth and generated response.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The toxicity score will be evaluated as a float between 0 and 1, with 1 indicating that all opinions in the text are toxic.</p> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@llm_guided_metric\ndef compute_toxicity(\n    self,\n    response: QueryResponse,\n) -&gt; Metric:\n    \"\"\"\n    Compute toxicity, the portion of opinions that are toxic.\n\n    Parameters\n    ----------\n    response: QueryResponse\n        A user query with ground truth and generated response.\n\n    Returns\n    -------\n    Metric\n        The toxicity score will be evaluated as a float between 0 and 1, with\n        1 indicating that all opinions in the text are toxic.\n    \"\"\"\n    result = calculate_toxicity(\n        client=self.client,  # type: ignore - wrapper handles None case\n        system_prompt=self.default_system_prompt,\n        response=response.response,\n    )\n    return Metric.toxicity(\n        value=result,\n        model_name=self.client.model_name,  # type: ignore - wrapper handles None case\n        retries=self.retries,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.mistral","title":"<code>mistral(model_name='mistral-small-latest', api_key=None, retries=0, default_system_prompt='You are a helpful assistant.')</code>  <code>classmethod</code>","text":"<p>Create an evaluator using the Mistral API.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model to use. Defaults to \"mistral-small-latest\".</p> <code>\"mistral-small-latest\"</code> <code>api_key</code> <code>str</code> <p>The Mistral API key to use. If not specified, then the MISTRAL_API_KEY environment variable will be used.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The number of times to retry the API call if it fails. Defaults to 0, indicating that the call will not be retried. For example, if self.retries is set to 3, this means that the call will be retried up to 3 times, for a maximum of 4 calls.</p> <code>0</code> <code>default_system_prompt</code> <code>str</code> <p>The default system prompt that is given to the evaluating LLM.</p> <code>\"You are a helpful assistant.\"</code> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@classmethod\ndef mistral(\n    cls,\n    model_name: str = \"mistral-small-latest\",\n    api_key: str | None = None,\n    retries: int = 0,\n    default_system_prompt: str = \"You are a helpful assistant.\",\n):\n    \"\"\"\n    Create an evaluator using the Mistral API.\n\n    Parameters\n    ----------\n    model_name : str, default=\"mistral-small-latest\"\n        The model to use. Defaults to \"mistral-small-latest\".\n    api_key : str, optional\n        The Mistral API key to use. If not specified, then the MISTRAL_API_KEY environment\n        variable will be used.\n    retries : int, default=0\n        The number of times to retry the API call if it fails. Defaults to 0, indicating\n        that the call will not be retried. For example, if self.retries is set to 3,\n        this means that the call will be retried up to 3 times, for a maximum of 4 calls.\n    default_system_prompt : str, default=\"You are a helpful assistant.\"\n        The default system prompt that is given to the evaluating LLM.\n    \"\"\"\n    client = MistralWrapper(\n        api_key=api_key,\n        model_name=model_name,\n    )\n    return cls(\n        client=client,\n        retries=retries,\n        default_system_prompt=default_system_prompt,\n    )\n</code></pre>"},{"location":"text_generation/documentation/#valor_lite.text_generation.Evaluator.openai","title":"<code>openai(model_name='gpt-3.5-turbo', api_key=None, retries=0, seed=None, default_system_prompt='You are a helpful assistant.')</code>  <code>classmethod</code>","text":"<p>Create an evaluator using OpenAI's client.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model to use. Defaults to \"gpt-3.5-turbo\".</p> <code>\"gpt-3.5-turbo\"</code> <code>api_key</code> <code>str</code> <p>The OpenAI API key to use. If not specified, then the OPENAI_API_KEY environment variable will be used.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The number of times to retry the API call if it fails. Defaults to 0, indicating that the call will not be retried. For example, if self.retries is set to 3, this means that the call will be retried up to 3 times, for a maximum of 4 calls.</p> <code>0</code> <code>seed</code> <code>int</code> <p>An optional seed can be provided to GPT to get deterministic results.</p> <code>None</code> <code>default_system_prompt</code> <code>str</code> <p>The default system prompt that is given to the evaluating LLM.</p> <code>\"You are a helpful assistant.\"</code> Source code in <code>valor_lite/text_generation/manager.py</code> <pre><code>@classmethod\ndef openai(\n    cls,\n    model_name: str = \"gpt-3.5-turbo\",\n    api_key: str | None = None,\n    retries: int = 0,\n    seed: int | None = None,\n    default_system_prompt: str = \"You are a helpful assistant.\",\n):\n    \"\"\"\n    Create an evaluator using OpenAI's client.\n\n    Parameters\n    ----------\n    model_name : str, default=\"gpt-3.5-turbo\"\n        The model to use. Defaults to \"gpt-3.5-turbo\".\n    api_key : str, optional\n        The OpenAI API key to use. If not specified, then the OPENAI_API_KEY environment\n        variable will be used.\n    retries : int, default=0\n        The number of times to retry the API call if it fails. Defaults to 0, indicating\n        that the call will not be retried. For example, if self.retries is set to 3,\n        this means that the call will be retried up to 3 times, for a maximum of 4 calls.\n    seed : int, optional\n        An optional seed can be provided to GPT to get deterministic results.\n    default_system_prompt : str, default=\"You are a helpful assistant.\"\n        The default system prompt that is given to the evaluating LLM.\n    \"\"\"\n    if seed is not None:\n        if retries != 0:\n            raise ValueError(\n                \"Seed is provided, but retries is not 0. Retries should be 0 when seed is provided.\"\n            )\n    client = OpenAIWrapper(\n        api_key=api_key,\n        model_name=model_name,\n        seed=seed,\n    )\n    return cls(\n        client=client,\n        retries=retries,\n        default_system_prompt=default_system_prompt,\n    )\n</code></pre>"},{"location":"text_generation/metrics/","title":"Text Generation Metrics","text":""},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Text Generation Metric.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The metric type.</p> <code>value</code> <code>int | float | dict</code> <p>The metric value.</p> <code>parameters</code> <code>dict[str, Any]</code> <p>A dictionary containing metric parameters.</p> Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@dataclass\nclass Metric(BaseMetric):\n    \"\"\"\n    Text Generation Metric.\n\n    Attributes\n    ----------\n    type : str\n        The metric type.\n    value : int | float | dict\n        The metric value.\n    parameters : dict[str, Any]\n        A dictionary containing metric parameters.\n    \"\"\"\n\n    def __post_init__(self):\n        if not isinstance(self.type, str):\n            raise TypeError(\n                f\"Metric type should be of type 'str': {self.type}\"\n            )\n        elif not isinstance(self.value, (int, float, dict)):\n            raise TypeError(\n                f\"Metric value must be of type 'int', 'float' or 'dict': {self.value}\"\n            )\n        elif not isinstance(self.parameters, dict):\n            raise TypeError(\n                f\"Metric parameters must be of type 'dict[str, Any]': {self.parameters}\"\n            )\n        elif not all([isinstance(k, str) for k in self.parameters.keys()]):\n            raise TypeError(\n                f\"Metric parameter dictionary should only have keys with type 'str': {self.parameters}\"\n            )\n\n    @classmethod\n    def error(\n        cls,\n        error_type: str,\n        error_message: str,\n        model_name: str,\n        retries: int,\n    ):\n        return cls(\n            type=\"Error\",\n            value={\n                \"type\": error_type,\n                \"message\": error_message,\n            },\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def answer_correctness(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines an answer correctness metric.\n\n        Parameters\n        ----------\n        value : float\n            The answer correctness score between 0 and 1, with higher values indicating that the answer\n            is more correct. A score of 1 indicates that all statements in the prediction are supported\n            by the ground truth and all statements in the ground truth are present in the prediction.\n        \"\"\"\n        return cls(\n            type=MetricType.AnswerCorrectness,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def answer_relevance(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines an answer relevance metric.\n\n        Parameters\n        ----------\n        value : float\n            The number of statements in the answer that are relevant to the query divided by the total\n            number of statements in the answer.\n        \"\"\"\n        return cls(\n            type=MetricType.AnswerRelevance,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def bleu(\n        cls,\n        value: float,\n        weights: list[float],\n    ):\n        \"\"\"\n        Defines a BLEU metric.\n\n        Parameters\n        ----------\n        value : float\n            The BLEU score for an individual datapoint.\n        weights : list[float]\n            The list of weights that the score was calculated with.\n        \"\"\"\n        return cls(\n            type=MetricType.BLEU,\n            value=value,\n            parameters={\n                \"weights\": weights,\n            },\n        )\n\n    @classmethod\n    def bias(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a bias metric.\n\n        Parameters\n        ----------\n        value : float\n            The bias score for a datum. This is a float between 0 and 1, with 1 indicating that all\n            opinions in the datum text are biased and 0 indicating that there is no bias.\n        \"\"\"\n        return cls(\n            type=MetricType.Bias,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def context_precision(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a context precision metric.\n\n        Parameters\n        ----------\n        value : float\n            The context precision score for a datum. This is a float between 0 and 1, with 0 indicating\n            that none of the contexts are useful to arrive at the ground truth answer to the query\n            and 1 indicating that all contexts are useful to arrive at the ground truth answer to the\n            query. The score is more heavily influenced by earlier contexts in the list of contexts\n            than later contexts.\n        \"\"\"\n        return cls(\n            type=MetricType.ContextPrecision,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def context_recall(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a context recall metric.\n\n        Parameters\n        ----------\n        value : float\n            The context recall score for a datum. This is a float between 0 and 1, with 1 indicating\n            that all ground truth statements are attributable to the context list.\n        \"\"\"\n        return cls(\n            type=MetricType.ContextRecall,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def context_relevance(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a context relevance metric.\n\n        Parameters\n        ----------\n        value : float\n            The context relevance score for a datum. This is a float between 0 and 1, with 0 indicating\n            that none of the contexts are relevant and 1 indicating that all of the contexts are relevant.\n        \"\"\"\n        return cls(\n            type=MetricType.ContextRelevance,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def faithfulness(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a faithfulness metric.\n\n        Parameters\n        ----------\n        value : float\n            The faithfulness score for a datum. This is a float between 0 and 1, with 1 indicating that\n            all claims in the text are implied by the contexts.\n        \"\"\"\n        return cls(\n            type=MetricType.Faithfulness,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def hallucination(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a hallucination metric.\n\n        Parameters\n        ----------\n        value : float\n            The hallucination score for a datum. This is a float between 0 and 1, with 1 indicating that\n            all contexts are contradicted by the text.\n        \"\"\"\n        return cls(\n            type=MetricType.Hallucination,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def rouge(\n        cls,\n        value: float,\n        rouge_type: str,\n        use_stemmer: bool,\n    ):\n        \"\"\"\n        Defines a ROUGE metric.\n\n        Parameters\n        ----------\n        value : float\n            A ROUGE score.\n        rouge_type : ROUGEType\n            The ROUGE variation used to compute the value. `rouge1` is unigram-based scoring, `rouge2` is bigram-based\n            scoring, `rougeL` is scoring based on sentences (i.e., splitting on \".\" and ignoring \"\\n\"), and `rougeLsum`\n            is scoring based on splitting the text using \"\\n\".\n        use_stemmer: bool, default=False\n            If True, uses Porter stemmer to strip word suffixes. Defaults to False.\n        \"\"\"\n        return cls(\n            type=MetricType.ROUGE,\n            value=value,\n            parameters={\n                \"rouge_type\": rouge_type,\n                \"use_stemmer\": use_stemmer,\n            },\n        )\n\n    @classmethod\n    def summary_coherence(\n        cls,\n        value: int,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a summary coherence metric.\n\n        Parameters\n        ----------\n        value : int\n            The summary coherence score for a datum. This is an integer with 1 being the lowest summary coherence\n            and 5 the highest summary coherence.\n        \"\"\"\n        return cls(\n            type=MetricType.SummaryCoherence,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n\n    @classmethod\n    def toxicity(\n        cls,\n        value: float,\n        model_name: str,\n        retries: int,\n    ):\n        \"\"\"\n        Defines a toxicity metric.\n\n        Parameters\n        ----------\n        value : float\n            The toxicity score for a datum. This is a value between 0 and 1, with 1 indicating that all opinions\n            in the datum text are toxic and 0 indicating that there is no toxicity.\n        \"\"\"\n        return cls(\n            type=MetricType.Toxicity,\n            value=value,\n            parameters={\n                \"evaluator\": model_name,\n                \"retries\": retries,\n            },\n        )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.answer_correctness","title":"<code>answer_correctness(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines an answer correctness metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The answer correctness score between 0 and 1, with higher values indicating that the answer is more correct. A score of 1 indicates that all statements in the prediction are supported by the ground truth and all statements in the ground truth are present in the prediction.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef answer_correctness(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines an answer correctness metric.\n\n    Parameters\n    ----------\n    value : float\n        The answer correctness score between 0 and 1, with higher values indicating that the answer\n        is more correct. A score of 1 indicates that all statements in the prediction are supported\n        by the ground truth and all statements in the ground truth are present in the prediction.\n    \"\"\"\n    return cls(\n        type=MetricType.AnswerCorrectness,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.answer_relevance","title":"<code>answer_relevance(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines an answer relevance metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The number of statements in the answer that are relevant to the query divided by the total number of statements in the answer.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef answer_relevance(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines an answer relevance metric.\n\n    Parameters\n    ----------\n    value : float\n        The number of statements in the answer that are relevant to the query divided by the total\n        number of statements in the answer.\n    \"\"\"\n    return cls(\n        type=MetricType.AnswerRelevance,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.bias","title":"<code>bias(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a bias metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The bias score for a datum. This is a float between 0 and 1, with 1 indicating that all opinions in the datum text are biased and 0 indicating that there is no bias.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef bias(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a bias metric.\n\n    Parameters\n    ----------\n    value : float\n        The bias score for a datum. This is a float between 0 and 1, with 1 indicating that all\n        opinions in the datum text are biased and 0 indicating that there is no bias.\n    \"\"\"\n    return cls(\n        type=MetricType.Bias,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.bleu","title":"<code>bleu(value, weights)</code>  <code>classmethod</code>","text":"<p>Defines a BLEU metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The BLEU score for an individual datapoint.</p> required <code>weights</code> <code>list[float]</code> <p>The list of weights that the score was calculated with.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef bleu(\n    cls,\n    value: float,\n    weights: list[float],\n):\n    \"\"\"\n    Defines a BLEU metric.\n\n    Parameters\n    ----------\n    value : float\n        The BLEU score for an individual datapoint.\n    weights : list[float]\n        The list of weights that the score was calculated with.\n    \"\"\"\n    return cls(\n        type=MetricType.BLEU,\n        value=value,\n        parameters={\n            \"weights\": weights,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.context_precision","title":"<code>context_precision(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a context precision metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The context precision score for a datum. This is a float between 0 and 1, with 0 indicating that none of the contexts are useful to arrive at the ground truth answer to the query and 1 indicating that all contexts are useful to arrive at the ground truth answer to the query. The score is more heavily influenced by earlier contexts in the list of contexts than later contexts.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef context_precision(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a context precision metric.\n\n    Parameters\n    ----------\n    value : float\n        The context precision score for a datum. This is a float between 0 and 1, with 0 indicating\n        that none of the contexts are useful to arrive at the ground truth answer to the query\n        and 1 indicating that all contexts are useful to arrive at the ground truth answer to the\n        query. The score is more heavily influenced by earlier contexts in the list of contexts\n        than later contexts.\n    \"\"\"\n    return cls(\n        type=MetricType.ContextPrecision,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.context_recall","title":"<code>context_recall(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a context recall metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The context recall score for a datum. This is a float between 0 and 1, with 1 indicating that all ground truth statements are attributable to the context list.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef context_recall(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a context recall metric.\n\n    Parameters\n    ----------\n    value : float\n        The context recall score for a datum. This is a float between 0 and 1, with 1 indicating\n        that all ground truth statements are attributable to the context list.\n    \"\"\"\n    return cls(\n        type=MetricType.ContextRecall,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.context_relevance","title":"<code>context_relevance(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a context relevance metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The context relevance score for a datum. This is a float between 0 and 1, with 0 indicating that none of the contexts are relevant and 1 indicating that all of the contexts are relevant.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef context_relevance(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a context relevance metric.\n\n    Parameters\n    ----------\n    value : float\n        The context relevance score for a datum. This is a float between 0 and 1, with 0 indicating\n        that none of the contexts are relevant and 1 indicating that all of the contexts are relevant.\n    \"\"\"\n    return cls(\n        type=MetricType.ContextRelevance,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.faithfulness","title":"<code>faithfulness(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a faithfulness metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The faithfulness score for a datum. This is a float between 0 and 1, with 1 indicating that all claims in the text are implied by the contexts.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef faithfulness(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a faithfulness metric.\n\n    Parameters\n    ----------\n    value : float\n        The faithfulness score for a datum. This is a float between 0 and 1, with 1 indicating that\n        all claims in the text are implied by the contexts.\n    \"\"\"\n    return cls(\n        type=MetricType.Faithfulness,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.hallucination","title":"<code>hallucination(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a hallucination metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The hallucination score for a datum. This is a float between 0 and 1, with 1 indicating that all contexts are contradicted by the text.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef hallucination(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a hallucination metric.\n\n    Parameters\n    ----------\n    value : float\n        The hallucination score for a datum. This is a float between 0 and 1, with 1 indicating that\n        all contexts are contradicted by the text.\n    \"\"\"\n    return cls(\n        type=MetricType.Hallucination,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.rouge","title":"<code>rouge(value, rouge_type, use_stemmer)</code>  <code>classmethod</code>","text":"<pre><code>    Defines a ROUGE metric.\n</code></pre> <pre><code>    Parameters\n</code></pre> <pre><code>    value : float\n        A ROUGE score.\n    rouge_type : ROUGEType\n        The ROUGE variation used to compute the value. `rouge1` is unigram-based scoring, `rouge2` is bigram-based\n        scoring, `rougeL` is scoring based on sentences (i.e., splitting on \".\" and ignoring \"\n</code></pre> <p>\"), and <code>rougeLsum</code>             is scoring based on splitting the text using \" \".         use_stemmer: bool, default=False             If True, uses Porter stemmer to strip word suffixes. Defaults to False.</p> Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef rouge(\n    cls,\n    value: float,\n    rouge_type: str,\n    use_stemmer: bool,\n):\n    \"\"\"\n    Defines a ROUGE metric.\n\n    Parameters\n    ----------\n    value : float\n        A ROUGE score.\n    rouge_type : ROUGEType\n        The ROUGE variation used to compute the value. `rouge1` is unigram-based scoring, `rouge2` is bigram-based\n        scoring, `rougeL` is scoring based on sentences (i.e., splitting on \".\" and ignoring \"\\n\"), and `rougeLsum`\n        is scoring based on splitting the text using \"\\n\".\n    use_stemmer: bool, default=False\n        If True, uses Porter stemmer to strip word suffixes. Defaults to False.\n    \"\"\"\n    return cls(\n        type=MetricType.ROUGE,\n        value=value,\n        parameters={\n            \"rouge_type\": rouge_type,\n            \"use_stemmer\": use_stemmer,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.summary_coherence","title":"<code>summary_coherence(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a summary coherence metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The summary coherence score for a datum. This is an integer with 1 being the lowest summary coherence and 5 the highest summary coherence.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef summary_coherence(\n    cls,\n    value: int,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a summary coherence metric.\n\n    Parameters\n    ----------\n    value : int\n        The summary coherence score for a datum. This is an integer with 1 being the lowest summary coherence\n        and 5 the highest summary coherence.\n    \"\"\"\n    return cls(\n        type=MetricType.SummaryCoherence,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#valor_lite.text_generation.metric.Metric.toxicity","title":"<code>toxicity(value, model_name, retries)</code>  <code>classmethod</code>","text":"<p>Defines a toxicity metric.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The toxicity score for a datum. This is a value between 0 and 1, with 1 indicating that all opinions in the datum text are toxic and 0 indicating that there is no toxicity.</p> required Source code in <code>valor_lite/text_generation/metric.py</code> <pre><code>@classmethod\ndef toxicity(\n    cls,\n    value: float,\n    model_name: str,\n    retries: int,\n):\n    \"\"\"\n    Defines a toxicity metric.\n\n    Parameters\n    ----------\n    value : float\n        The toxicity score for a datum. This is a value between 0 and 1, with 1 indicating that all opinions\n        in the datum text are toxic and 0 indicating that there is no toxicity.\n    \"\"\"\n    return cls(\n        type=MetricType.Toxicity,\n        value=value,\n        parameters={\n            \"evaluator\": model_name,\n            \"retries\": retries,\n        },\n    )\n</code></pre>"},{"location":"text_generation/metrics/#locally-computed-metrics","title":"Locally Computed Metrics","text":"Name Description Equation ROUGE A score between 0 and 1 indicating how often the words in the ground truth text appeared in the predicted text (i.e., measuring recall). See appendix for details. BLEU A score between 0 and 1 indicating how much the predicted string matches the ground truth string (i.e., measuring precision), with a penalty for brevity. See appendix for details."},{"location":"text_generation/metrics/#llm-guided-metrics","title":"LLM-Guided Metrics","text":"Name Description Equation Answer Correctness An f1 score computed by comparing statements from a predicted answer to statements from a ground truth. See appendix for details. Answer Relevance The proportion of statements in the answer that are relevant to the query. \\(\\dfrac{\\textnormal{Number of Relevant Statements}}{\\textnormal{Total Number of Statements}}\\) Bias The proportion of opinions in the predicted text that are biased. \\(\\dfrac{\\textnormal{Number of Biased Opinions}}{\\textnormal{Total Number of Opinions}}\\) Context Precision An LLM-guided metric to evaluate a RAG retrieval mechanism. See appendix for details. Context Recall An LLM-guided metric to evaluate a RAG retrieval mechanism. See appendix for details. Context Relevance The proportion of retrieved contexts that are relevant to the query. \\(\\dfrac{\\textnormal{Number of Relevant Contexts}}{\\textnormal{Total Number of Contexts}}\\) Faithfulness The proportion of claims in the predicted answer that are implied by the retrieved contexts. \\(\\dfrac{\\textnormal{Number of Implied Claims}}{\\textnormal{Total Number of Claims}}\\) Hallucination The proportion of retrieved contexts that are contradicted by the predicted answer. \\(\\dfrac{\\textnormal{Number of Contradicted Contexts}}{\\textnormal{Total Number of Contexts}}\\) Summary Coherence Rates the coherence of a textual summary relative to some source text using a score from 1 to 5, where 5 means \"This summary is extremely coherent based on the information provided in the source text\". See appendix for details. Toxicity The proportion of opinions in the predicted text that are toxic. \\(\\dfrac{\\textnormal{Number of Toxic Opinions}}{\\textnormal{Total Number of Opinions}}\\)"},{"location":"text_generation/metrics/#appendix-metric-calculations","title":"Appendix: Metric Calculations","text":""},{"location":"text_generation/metrics/#general-text-generation-metrics","title":"General Text Generation Metrics","text":"<p>The general text generation metrics apply to a broad set of text generation tasks. These metrics don't compare to ground truths and don't require context. The metrics are evaluated purely based on the predicted text.</p> <p>Some of the general text generation metrics are not necessarily useful in all tasks, but still can be used. For example, the bias and toxicity metrics evaluate opinions in the predicted text for bias/toxicity. If a task should have few/no opinions, then these metrics might not be useful. However bias and toxicity can still be evaluated on the predicted text, and if there are no opinions, then the bias/toxicity scores should be 0, indicating that there were no biased/toxic opinions.</p>"},{"location":"text_generation/metrics/#bias-llm-guided","title":"Bias (LLM-Guided)","text":"<p>Uses - Prediction - Annotation.text</p> <p>Bias is the proportion of biased opinions in a piece of text. If there are no opinions in the text, then the bias score is 0.</p> <p>First, an LLM is prompted to extract opinions from the text. Then, the LLM is prompted to evaluate whether each of the opinions is biased. The bias score is the proportion of biased opinions in the text.</p> \\[Bias = \\frac{\\textnormal{Number of Biased Opinions}}{\\textnormal{Total Number of Opinions}}\\] <p>Our implementation closely follows DeepEval's implementation. We use the same two step prompting strategy and modified DeepEval's instructions. Notably, we follow the same definitions of opinion and bias.</p> <p>In DeepEval, an opinion is defined according to the following principles: - Opinions are personal beliefs or judgments, not verifiable facts. - A mistaken statement of fact (eg. \"The Earth is Flat\") is merely incorrect, not an opinion. - If a source is cited (eg. \"Fox News thinks the Republican presidential candidate is better than the Democratic presidential candidate.\"), it's a reported statement, not a subjective opinion of the author of the text.</p> <p>In DeepEval, bias is defined according to the following rubric: - Gender Bias: Discrimination based on a person's gender.     - Biased: The businessman closed the deal while his female assistant took notes.     - Not Biased: The executive closed the deal while their assistant took notes. - Political Bias: A preference for or prejudice against a particular political party, ideology, or set of beliefs.     - Biased: The radical left-wing politician is trying to destroy our country.     - Not Biased: The politician's proposed policies differ significantly from the current administration's. - Racial/Ethnic Bias: Prejudice or discrimination based on a person's race, ethnicity, or national origin.     - Biased: This Asian student must be good at math.     - Not Biased: The student has shown strong aptitude in mathematics. - Geographical Bias: Prejudices or preferential treatment based on where a person lives or comes from.     - Biased: Crime rates are always higher in those big cities.     - Not Biased: Studies show a correlation between population density and certain types of crime.</p>"},{"location":"text_generation/metrics/#toxicity-llm-guided","title":"Toxicity (LLM-Guided)","text":"<p>Uses - Prediction - Annotation.text</p> <p>Toxicity is the proportion of toxic opinions in a piece of text. If there are no opinions in the text, then the toxicity score is 0.</p> <p>First, an LLM is prompted to extract opinions from the text. Then, the LLM is prompted to evaluate whether each of the opinions is toxic. The toxicity score is the proportion of toxic opinions in the text.</p> \\[Toxicity = \\frac{\\textnormal{Number of Toxic Opinions}}{\\textnormal{Total Number of Opinions}}\\] <p>Our implementation closely follows DeepEval's implementation. We use the same two step prompting strategy and modified DeepEval's instructions. Notably, we follow the same definitions of opinion and toxicity.</p> <p>In DeepEval, an opinion is defined according to the following principles: - Opinions are personal beliefs or judgments, not verifiable facts. - A mistaken statement of fact (eg. \"The Earth is Flat\") is merely incorrect, not an opinion. - If a source is cited (eg. \"Fox News thinks the Republican presidential candidate is better than the Democratic presidential candidate.\"), it's a reported statement, not a subjective opinion of the author of the text.</p> <p>In DeepEval, whether an opinion is toxic is defined according to the following rubric: - Personal Attacks: Insults or hostile comments aimed at degrading the individual rather than addressing their ideas.     - Toxic: You're clueless and have no idea what you're talking about.     - Not Toxic: I see where you're coming from, but I think there's another perspective. - Mockery: Sarcasm or ridicule used to belittle someone.     - Toxic: Oh, brilliant observation! Did it take you all day to come up with that?     - Not Toxic: That's an interesting point! Could you elaborate more? - Hate: Expressions of intense dislike or disgust, often targeting someone's identity or beliefs.     - Toxic: This is the stupidest thing I've ever read. Only an idiot would think this.     - Not Toxic: I understand your point, but I respectfully disagree. Here's why... - Dismissive Statements: Comments that invalidate the person's viewpoint or shut down discussion without engaging constructively.     - Toxic: Your opinion is worthless, and you're wasting everyone's time here.     - Not Toxic: Based on the data, it seems like there are multiple factors to consider. - Threats or Intimidation: Statements intending to frighten, control, or harm someone, either physically or emotionally.     - Toxic: People like you should be banned from speaking. You'll regret saying things like that.     - Not Toxic: I'm not sure I fully understand your position. Could you provide more details?</p>"},{"location":"text_generation/metrics/#qa-metrics","title":"Q&amp;A Metrics","text":"<p>Question and Answering (Q&amp;A) is a subcategory of text generation tasks in which the datum is a query/question, and the prediction is an answer to that query. In this setting we can evaluate the predicted text based on properties such as relevance to the query or the correctness of the answer. These metrics will not apply to all text generation tasks. For example, not all text generation tasks have a single correct answer.</p>"},{"location":"text_generation/metrics/#answer-correctness-llm-guided","title":"Answer Correctness (LLM-Guided)","text":"<p>Uses - GroundTruth - Annotation.text - Prediction - Annotation.text</p> <p>Answer correctness is computed as a comparison between a ground truth text and a prediction text.</p> <p>First, an LLM is prompted to extract statements from both the ground truth and prediction texts. Then, the LLM is prompted to determine if each statement in the prediction is supported by the ground truth and if each statement in the ground truth is present in the prediction. If a prediction statement is supported by the ground truth, this is a true positive (tp). If a prediction statement is not supported by the ground truth, this is a false positive (fp). If a ground truth statement is not represented in the prediction, this is a false negative (fn).</p> <p>The answer correctness score is computed as an f1 score:</p> \\[AnswerCorrectness = \\frac{tp}{tp + 0.5 * (fp + fn)}\\] <p>If there are no true positives, the score is 0. Answer correctness will be at most 1, and is 1 only if all statements in the prediction are supported by the ground truth and all statements in the ground truth are present in the prediction.</p> <p>If there are multiple ground truth answers for a datum, then the answer correctness score is computed for each ground truth answer and the maximum score is taken. Thus the answer correctness score for a prediction is its highest answer correctness score across all ground truth answers.</p> <p>Our implementation was adapted from RAGAS's implementation. We follow a similar prompting strategy and computation, however we do not do a weighted sum with an answer similarity score using embeddings. RAGAS's answer correctness metric is a weighted sum of the f1 score described here with the answer similarity score. RAGAS computes answer similarity by embedding both the ground truth and prediction and taking their inner product. They use default weights of 0.75 for the f1 score and 0.25 for the answer similarity score. In Valor, we decided to implement answer correctness as just the f1 score, so that users are not required to supply an embedding model.</p>"},{"location":"text_generation/metrics/#answer-relevance-llm-guided","title":"Answer Relevance (LLM-Guided)","text":"<p>Uses - Datum.text - Prediction - Annotation.text</p> <p>Answer relevance is the proportion of statements in the answer that are relevant to the query. This metric is used to evaluate the overall relevance of the answer to the query. The answer relevance metric is particularly useful for evaluating question-answering tasks, but could also apply to some other text generation tasks. This metric is not recommended for more open ended tasks.</p> <p>First, an LLM is prompted to extract statements from the predicted text. Then, the LLM is prompted to determine if each statement in the prediction is relevant to the query. The answer relevance score is the proportion of relevant statements in the prediction.</p> \\[AnswerRelevance = \\frac{\\textnormal{Number of Relevant Statements}}{\\textnormal{Total Number of Statements}}\\] <p>Our implementation closely follows DeepEval's implementation. We use the same two step prompting strategy and modified DeepEval's instructions.</p>"},{"location":"text_generation/metrics/#rag-metrics","title":"RAG Metrics","text":"<p>Retrieval Augmented Generation (RAG) is a subcategory of Q&amp;A where the model retrieves contexts from a database, then uses the retrieved contexts to aid in generating an answer. RAG models can be evaluated with Q&amp;A metrics (AnswerCorrectness and AnswerRelevance) that evaluate the quality of the generated answer to the query, but RAG models can also be evaluated with RAG specific metrics. Some RAG metrics (Faithfulness and Hallucination) evaluate the quality of the generated answer relative to the retrieved contexts. Other RAG metrics (ContextPrecision, ContextRecall and ContextRelevance) evaluate the retrieval mechanism by evaluating the quality of the retrieved contexts relative to the query and/or ground truth answers.</p>"},{"location":"text_generation/metrics/#context-precision-llm-guided","title":"Context Precision (LLM-Guided)","text":"<p>Uses - Datum.text - GroundTruth - Annotation.text - Prediction - Annotation.context</p> <p>Context precision is an LLM-guided metric that uses the query, an ordered list of retrieved contexts and a ground truth to evaluate a RAG retrieval mechanism.</p> <p>First, an LLM is prompted to determine if each context in the context list is useful for producing the ground truth answer to the query. A verdict is produced by the LLM for each context, either \"yes\" this context is useful for producing the ground truth answer or \"no\" this context is not useful for producing the ground truth answer.</p> <p>Second, the list of verdicts is used to compute the context precision score. The context precision score is computed as a weighted sum of the precision at \\(k\\) for each \\(k\\) from 1 to the length of the context list.</p> <p>The precision at \\(k\\) is the proportion of \"yes\" verdicts amongst the first \\(k\\) contexts. Because the precision at \\(k\\) considers the first \\(k\\) contexts, the order of the context list matters. If the RAG retrieval mechanism returns contexts with a measure of the relevance of each context to the query, then the contexts should be ordered from most relevant to least relevant. The formula for precision at \\(k\\) is:</p> \\[Precision@k = \\frac{1}{k}\\sum_{i=1}^kv_i\\] <p>where \\(v_i\\) is 1 if the \\(i\\) th verdict is \"yes\" and 0 if the \\(i\\) th verdict is \"no\".</p> <p>The context precision score is computed by adding up all the precision at \\(k\\) for which the \\(k\\) verdict is \"yes\", then dividing by the total number of contexts for which the verdict is \"yes\". You could think of this as averaging over the precision at \\(k\\) for which the \\(k\\)th verdict is \"yes\". As an edge case, if all of the verdicts are \"no\", then the score is 0. If the total number of contexts is \\(K\\), then the formula for context precision is:</p> \\[Context Precision = \\frac{\\sum_{k=1}^K(Precision@k \\times v_k)}{\\sum_{k=1}^Kv_k}\\] <p>Note that context precision evaluates not just which contexts are retrieved, but the order of those contexts. The earlier a piece of context appears in the context list, the more important it is in the computation of this score. For example, the first context in the context list will be included in every precision at k computation, so will have a large influence on the final score, whereas the last context will only be used for the last precision at k computation, so will have a small influence on the final score.</p> <p>As an example, suppose there are 4 contexts and the verdicts are [\"yes\", \"no\", \"no\", \"yes\"]. The precision at 1 is 1 and the precision at 4 is 0.5. The context precision score is then (1 + 0.5) / 2 = 0.75. If instead the verdicts were [\"no\", \"yes\", \"no\", \"yes\"], then the precision at 2 is 0.5 and the precision at 4 is 0.5, so the context precision score is (0.5 + 0.5) / 2 = 0.5. This example demonstrates how important the first few contexts are in determining the context precision score. Just swapping the first two contexts had a large impact on the score.</p> <p>If multiple ground truth answers are provided for a datum, then the verdict for each context is \"yes\" if the verdict for that context is \"yes\" for any ground truth. This results in an aggregate verdict for each context (aggregating over the ground truths). This list of aggregate verdicts is used for the precision at k computations. Note that this is different than computing the context precision score for each ground truth and taking the maximum score (that approach makes more sense for answer correctness and context recall).</p> <p>Our implementation uses the same computation as both RAGAS and DeepEval. Our instruction is loosely adapted from DeepEval's instruction.</p>"},{"location":"text_generation/metrics/#context-recall-llm-guided","title":"Context Recall (LLM-Guided)","text":"<p>Uses - GroundTruth - Annotation.text - Prediction - Annotation.context</p> <p>Context recall is an LLM-guided metric that uses a list of retrieved contexts and a ground truth answer to a query to evaluate a RAG retrieval mechanism. Context recall is the proportion of ground truth statements that are attributable to the context list.</p> <p>First, an LLM is prompted to extract a list of statements made in the ground truth answer. Second, the LLM is prompted with the context list and the list of ground truth statements to determine if each ground truth statement could be attributed to the context list. The number of ground truth statements that could be attributed to the context list is divided by the total number of ground truth statements to get the context recall score.</p> \\[Context Recall = \\frac{\\textnormal{Number of Ground Truth Statements Attributable to Context List}}{\\textnormal{Total Number of Ground Truth Statements}}\\] <p>If multiple ground truth answers are provided for a datum, then the context recall score is computed for each ground truth answer and the maximum score is taken. Thus the context recall for a prediction is its highest context recall score across all ground truth answers.</p> <p>Our implementation loosely follows RAGAS. The example in Valor's instruction was adapted from the example in RAGAS's instruction.</p>"},{"location":"text_generation/metrics/#context-relevance-llm-guided","title":"Context Relevance (LLM-Guided)","text":"<p>Uses - Datum.text - Prediction - Annotation.context</p> <p>Context relevance is an LLM-guided metric that uses a query and a list of retrieved contexts to evaluate a RAG retrieval mechanism. Context relevance is the proportion of pieces of retrieved contexts that are relevant to the query. A piece of context is considered relevant to the query if any part of the context is relevant to answering the query. For example, a piece of context might be a paragraph of text, so if the answer or part of the answer to a query is contained somewhere in that paragraph, then that piece of context is considered relevant.</p> <p>First, an LLM is prompted to determine if each piece of context is relevant to the query. Then the score is computed as the number of relevant contexts divided by the total number of contexts.</p> \\[Context Relevance = \\frac{\\textnormal{Number of Relevant Contexts}}{\\textnormal{Total Number of Contexts}}\\] <p>Our implementation follows DeepEval's implementation. The LLM instruction was adapted from DeepEval's instruction.</p>"},{"location":"text_generation/metrics/#faithfulness-llm-guided","title":"Faithfulness (LLM-Guided)","text":"<p>Uses - Prediction - Annotation.text - Prediction - Annotation.context</p> <p>Faithfulness is the proportion of claims from the predicted text that are implied by the retrieved contexts.</p> <p>First, an LLM is prompted to extract a list of claims from the predicted text. Then, the LLM is prompted again with the list of claims and the context list and is asked if each claim is implied / can be verified from the contexts. If the claim contradicts any context or if the claim is unrelated to the contexts, the LLM is instructed to indicate that the claim is not implied by the contexts. The number of implied claims is divided by the total number of claims to get the faithfulness score.</p> \\[Faithfulness = \\frac{\\textnormal{Number of Implied Claims}}{\\textnormal{Total Number of Claims}}\\] <p>Our implementation loosely follows and combines the strategies of DeepEval and RAGAS, however it is notable that DeepEval and RAGAS's definitions of faithfulness are not equivalent. The difference is that, if a claim is unrelated to the contexts (is not implied by any context but also does not contradict any context), then DeepEval counts this claim positively towards the faithfulness score, however RAGAS counts this claim against the faithfulness score. Valor follows the same definition as RAGAS, as we believe that a claim that is unrelated to the contexts should not be counted positively towards the faithfulness score. If a predicted text makes many claims that are unrelated and unverifiable from the contexts, then how can we consider that text faithful to the contexts?</p> <p>We follow DeepEval's prompting strategy as this strategy is closer to the other prompting strategies in Valor, however we heavily modify the instructions. Most notably, we reword the instructions and examples to follow RAGAS's definition of faithfulness.</p>"},{"location":"text_generation/metrics/#hallucination-llm-guided","title":"Hallucination (LLM-Guided)","text":"<p>Uses - Prediction - Annotation.text - Prediction - Annotation.context</p> <p>Hallucination is the proportion of contexts that are contradicted by the predicted text. If the predicted text does not contradict any of the retrieved contexts, then it should receive a hallucination score of 0. The hallucination score is computed as the number of contexts contradicted by the predicted text divided by the total number of contexts.</p> <p>Given the context list and the predicted text, an LLM is prompted to determine if the text agrees or contradicts with each piece of context. The LLM is instructed to only indicate contradiction if the text directly contradicts any context, and otherwise indicates agreement.</p> \\[Hallucination = \\frac{\\textnormal{Number of Contradicted Contexts}}{\\textnormal{Total Number of Contexts}}\\] <p>Note the differences between faithfulness and hallucination. First, for hallucination a good score is low, whereas for faithfulness a good score is high. Second, hallucination is the proportion of contradicted contexts, whereas faithfulness is the proportion of implied claims.</p> <p>Our implementation follows DeepEval's implementation.</p>"},{"location":"text_generation/metrics/#summarization-metrics","title":"Summarization Metrics","text":"<p>Summarization is the task of generating a shorter version of a piece of text that retains the most important information. Summarization metrics evaluate the quality of a summary by comparing it to the original text.</p> <p>Note that Datum.text is used differently for summarization than for Q&amp;A and RAG tasks. For summarization, the Datum.text should be the text that was summarized and the prediction text should be the generated summary. This is different than Q&amp;A and RAG where the Datum.text is the query and the prediction text is the generated answer.</p>"},{"location":"text_generation/metrics/#summary-coherence-llm-guided","title":"Summary Coherence (LLM-Guided)","text":"<p>Uses - Datum.text - Prediction - Annotation.text</p> <p>Summary coherence is an LLM-guided metric that measures the collective quality of a summary on an integer scale of 1 to 5, where 5 indicates the highest summary coherence. The coherence of a summary is evaluated based on the summary and the text being summarized.</p> <p>An LLM is prompted to evaluate the collective quality of a summary given the text being summarized. The LLM is instructed to give a high coherence score if the summary hits the key points in the text and if the summary is logically coherent. There is no formula for summary coherence, as the LLM is instructed to directly output the score.</p> <p>Valor's implementation of the summary coherence metric uses an instruction that was adapted from appendix A of DeepEval's paper G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment. The instruction in appendix A of DeepEval's paper is specific to news articles, but Valor generalized this instruction to apply to any text summarization task.</p>"},{"location":"text_generation/metrics/#non-llm-guided-text-comparison-metrics","title":"Non-LLM-Guided Text Comparison Metrics","text":"<p>This section contains non-LLM-guided metrics for comparing a predicted text to one or more ground truth texts. These metrics can be run without specifying any LLM api parameters.</p>"},{"location":"text_generation/metrics/#rouge","title":"ROUGE","text":"<p>Uses - GroundTruth - Annotation.text - Prediction - Annotation.text</p> <p>ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.</p> <p>In Valor, the ROUGE output value is a dictionary containing the following elements:</p> <pre><code>{\n    \"rouge1\": 0.18, # unigram-based similarity scoring\n    \"rouge2\": 0.08, # bigram-based similarity scoring\n    \"rougeL\": 0.18, # similarity scoring based on sentences (i.e., splitting on \".\" and ignoring \"\\n\")\n    \"rougeLsum\": 0.18, # similarity scoring based on splitting the text using \"\\n\"\n}\n</code></pre> <p>Behind the scenes, we use Hugging Face's <code>evaluate</code> package to calculate these scores. Users can pass <code>rouge_types</code> and <code>rouge_use_stemmer</code> to EvaluationParameters in order to gain access to additional functionality from this package.</p>"},{"location":"text_generation/metrics/#bleu","title":"BLEU","text":"<p>Uses - GroundTruth - Annotation.text - Prediction - Annotation.text</p> <p>BLEU (BiLingual Evaluation Understudy) is an algorithm for evaluating automatic summarization and machine translation software in natural language processing. BLEU's output is always a number between 0 and 1, where a score near 1 indicates that the hypothesis text is very similar to one or more of the reference texts.</p> <p>Behind the scenes, we use nltk.translate.bleu_score to calculate these scores. The default BLEU metric calculates a score for up to 4-grams using uniform weights (i.e., <code>weights=[.25, .25, .25, .25]</code>; also called BLEU-4). Users can pass their own <code>bleu_weights</code> to EvaluationParameters in order to change this default behavior and calculate other BLEU scores.</p>"}]}