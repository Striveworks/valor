{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Velour documentation","text":"<p>velour is an evaluation store that facilitates the computation, discoverability, and shareability of metrics for meachine learning models.</p> <p>The core of velour is a backend REST API service. user's will typically interact with this service via a python client. there is also a lightweight web interface. At a high-level, the typical workflow involves posting groundtruth annotations (class labels, bounding boxes, segmentation masks, etc.) and model predictions to the service. Velour, on the backend, then handles the computation of metrics, stores them centrally, and allows them to be queried. Velour does not store raw data (such as underlying images) or facilitate model inference. It only stores groundtruth annotations and the predictions outputted from a model.</p> <p>Some highlights:</p> <ul> <li>The service handles the computation of metrics. This help makes them trustworthy and auditable, and is also useful when metric computations can be computationally expensive (e.g. for object detection).</li> <li>Metrics are centralized and queryable. In particular, the service facilicates comparing performance of multiple models against multiple datasets.</li> <li>Since inferences and groundtruths are stored, additional metrics can be computed without having to redo model inferences. For example, maybe you run default AP metric settings for object detection but later decide you want to know AP at lower IOU thresholds.</li> </ul>"},{"location":"client/","title":"Working with the Python client","text":"<p>Here we cover the basic concepts involved in interacting with the <code>velour</code> service via the Python client.</p> <p>Additionally, sample Jupyter notebooks are available here.</p>"},{"location":"client/#installation","title":"Installation","text":"<p>The client is hosted on PyPI and can be installed via</p> <pre><code>pip install velour-client\n</code></pre>"},{"location":"client/#supported-tasks","title":"Supported tasks","text":"<p>Currently <code>velour</code> supports the following groundtruth label types:</p> <ul> <li>image classifications</li> <li>object detections</li> <li>instance segmentations</li> <li>semantic segmentations</li> <li>tabular data classifications</li> </ul> <p>Each case has the notion of a label, which is a key/value pair. This is used (instead of forcing labels to be strings) to support things such as</p> <ul> <li>multi-label classification. e.g. a dataset of cropped vehicles that have make, model, and year labels</li> <li>additional attributes, such as COCO's <code>isCrowd</code> attribute.</li> </ul>"},{"location":"client/#client","title":"Client","text":"<p>The <code>velour.Client</code> class gives an object that is used to communicate with the <code>velour</code> backend. It can be instantiated via</p> <pre><code>from velour.client import Client\n\nclient = Client(HOST_URL)\n</code></pre> <p>In the case that the host uses authentication, then the argument <code>access_token</code> should also be passed to <code>Client</code>.</p>"},{"location":"client/#dataset","title":"Dataset","text":"<p><code>velour</code> stores metadata and annotations associated to a machine learning dataset. For example, in the case of a computer vision dataset, <code>velour</code> needs unique identifiers for images, height and width of images, and annotations (such as image classifications, bounding boxes, segmentation masks, etc.) but the underlying images themselves are not stored or needed by velour.</p> <p>The process of creating a new dataset to be used in velour is to first create an empty dataset via</p> <pre><code>dataset = client.create_dataset(DATASET_NAME) # DATASET_NAME a string.\n</code></pre> <p><code>dataset</code> is then a <code>velour.Dataset</code> object and can be used to add groundtruth labels.</p>"},{"location":"client/#image","title":"Image","text":"<p>An image consists of specifying the following information:</p> name type description uid string A unique identifier for the image. This is up to the enduser but some typical options are a filename/path in object store or a dataset specific image id (such as the image id in the COCO dataset) height integer The height of the image. This is necessary for certain operations in the backend, such as converting a polygon contour to a mask width integer The width of the image. This is necessary for certain operations in the backend, such as converting a polygon contour to a mask frame (optional) integer The frame number in the case that the image is a frame of a video <p>For example:</p> <pre><code>from velour.data_types import Image\n\nimg = Image(uid=\"abc123\", height=128, width=256)\n</code></pre> <p>Note: this creates an image object but is not yet uploaded to the backend service.</p>"},{"location":"client/#adding-annotations","title":"Adding annotations","text":"<p>Data structures and methods are provided for adding annotations to an image and then adding the image metadata and annotations to the backend service. We now go over the different types of supported annotations</p>"},{"location":"client/#image-classification","title":"Image classification","text":"<p>An image classification is specified by a <code>velour.data_types.Image</code> object and a list of labels. Each label is of type <code>velour.data_types.Label</code> which consists of a key/value pairs of strings. For example:</p> <pre><code>from velour.data_types import Label, GroundTruthImageClassification\n\nlabel1 = Label(key=\"occluded\", value=\"yes\")\nlabel2 = Label(key=\"class_name\", value=\"truck\")\n\ngt_cls = GroundTruthImageClassification(image=img, labels=[label1, label2])\n</code></pre> <p>To associate ground truth image classifications (and in particular also the underlying <code>Image</code> objects) to a dataset, we use the <code>velour.Dataset.add_groundtruth_classifications</code> method which takes a list of image classifications</p> <pre><code>dataset.add_groundtruth_classifications([gt_cls])\n</code></pre> <p>This will post the annotations to the backend velour service.</p>"},{"location":"client/#object-detection","title":"Object Detection","text":""},{"location":"client/#semantic-segmentation","title":"Semantic Segmentation","text":""},{"location":"client/#model","title":"Model","text":"<p><code>velour</code> has the notion of a model that stores inferences of a machine learning model; <code>velour</code> does not need access to the model itself to evaluate, it just needs the predictions to be sent to it.</p>"},{"location":"client/#evaluation-job","title":"Evaluation job","text":"<p>An evaluation job sends a request to the backend to evaluate a model against a dataset. This will result in the computation of a host of metrics.</p>"},{"location":"client/#metric","title":"Metric","text":""},{"location":"deployment/","title":"Deploying the backend","text":""},{"location":"deployment/#components","title":"Components","text":"<p>The backend consists of three components</p> <ol> <li>A <code>velour</code> REST API service.</li> <li>A PostgreSQL instance with the PostGIS extention.</li> <li>A redis instance</li> </ol>"},{"location":"deployment/#docker","title":"Docker","text":""},{"location":"deployment/#velour-rest-api-docker-image","title":"velour REST API Docker Image","text":"<p>An image for the backend REST API service is hosted on GitHub's Container registry at <code>ghcr.io/striveworks/velour/velour-service</code>. Until the velour repo becomes public, you will need to authenticate to pull the image. To do this, you need to create a personal access token here https://github.com/settings/tokens that has read access to GitHub packages. Then run</p> <pre><code>docker login ghcr.io\n</code></pre> <p>and enter your username and the access token as the password.</p>"},{"location":"deployment/#docker-compose","title":"Docker Compose","text":"<p>The Docker compose file here sets up all three services with the appropriate networking. To run, set the environment variable <code>POSTGRES_PASSWORD</code> to your liking and then run</p> <pre><code>docker compose up\n</code></pre>"},{"location":"integrations/chariot/","title":"Chariot integrations","text":""},{"location":"integrations/chariot/#datasets","title":"Datasets","text":""},{"location":"integrations/chariot/#inference-servers","title":"Inference servers","text":""},{"location":"integrations/yolo/","title":"Ultralytics YOLO Integration","text":""}]}