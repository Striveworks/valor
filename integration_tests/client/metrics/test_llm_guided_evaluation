""" These integration tests should be run with a back end at http://localhost:8000
that is no auth
"""

from datetime import date, datetime, timedelta, timezone

import pytest

from valor import (
    Annotation,
    Client,
    Dataset,
    Datum,
    GroundTruth,
    Label,
    Model,
    Prediction,
)
from valor.enums import EvaluationStatus, TaskType
from valor.exceptions import ClientException


def test_evaluate_image_clf(
    client: Client,
    gt_questions: list[GroundTruth], # TODO Questions and answers should be fed through here. 
    pred_answers: list[Prediction],
    pred_context: list[list[str]], # TODO Is this how we want context passed in? 
    dataset_name: str,
    model_name: str,
):
    dataset = Dataset.create(dataset_name)
    model = Model.create(model_name)

    datum = Datum(uid="uid", metadata={})

    questions = [ # TODO these should be fed in through the parameters above
        "When was Gennady's first fight?",
        "Tell me something noteworthy about Golovkin's early career?",
    ...	
    ]

    llm_outputs = [
        "I can't answer that question with the context provided",
        "By the end of 2008, Golovkin's record stood at 14-0 (11 KO)"
    ]

    context = [
            ["After ending his amateur career in 2005, Golovkin signed a professional deal with the Universum Box-Promotion (UBP) and made his professional debut in May 2006. By the end of 2008, Golovkin's record stood at 14-0 (11 KO) and while he had few wins over boxers regarded as legitimate contenders, he was regarded as one of the best prospects in the world. Golovkin was given...", "...more context about Golovkin..."],
    ...
    ]

    groundtruths = []
    predictions = []

    for index in range(len(questions)):	
        # context is stored in the metadata, while the ask of the LLM itself is stored in the label
        groundtruths.append(
            GroundTruth(
                datum=datum,
                annotations=[
                    Annotation(
                        task_type=TaskType.LLM_EVALUATION,
                        labels=[
                            Label(
                                key=f"question {index}", 
                                value=questions[index]
                                ),
                            ],
                    )
                ],
            )
	    )

        # the prediction stores the LLM's output
        predictions.append(
            Prediction(
                datum=datum,
                annotations=[
                    Annotation(
                        task_type=TaskType.LLM_EVALUATION,
                        labels=[
                            Label(
                                key=f"question {index}", 
                                value=llm_outputs[index]
                            ),
                        ],
                        metadata={
                            'context': context[index] # TODO Is a list a valid format for metadata? 
                                                      # TODO Different models might combine / use context in different ways. However it is probably better to record raw context here? Formatting should be a property of the model? 
                        }
                    )
                ],
            )
        )

    for gt in groundtruths:
        dataset.add_groundtruth(gt)

    dataset.finalize()

    for pred in predictions:
        model.add_prediction(dataset, pred)

    model.finalize_inferences(dataset)

    # the evaluate_llm_output call requires an optional, separate service that takes the LLM's inputs and outputs, runs them through a separate LLM with a predetermined prompt, and returns the metric to Valor for storage.llm_service = LLMService(model="ChatGPT4", api_key=os.getenv("CHATGPT_API_KEY"))

    llm_metrics = ["Coherence", "QAG", "Grammaticality", "Hallucination Rate", "Toxicity", "Bias", "Faithfulness", "Answer Relevance", "Answer Correctness", "Context Precision", "Context Relevance", "Context Recall"]

    eval_job = model.evaluate_llm_output(
        dataset,
        llm_service=llm_service, # alternatively, we could just allow the user to pass in an API URL and token
        filter_by=[],
            metrics=llm_metrics,
    )

    assert eval_job.id

    assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE
    
    metrics = eval_job.metrics
    metadata = eval_job.meta

    expected_metrics = [
        # Coherence output >>>
        {
            "type": "Coherence",
            "value": .78,
            "label": {"key": "question 1", "value": "I can't answer that question with the context provided"}, 
            "parameters": {
                "groundtruths": [
                    {"key": "question 1", "value": "After ending..."}
                ],
                "query": "Given the following context and question, give a score from 1-5 indicating how coherent you believe the following answer is...",
                "context": ...,
            },
        },
        {
            "type": "QAG",
        },
        {
            "type": "Grammaticality",
        }
        {
            "type": "Hallucination Rate",
        },
        {
            "type": "Toxicity",
        },
        {
            "type": "Bias",
        },
        {
            "type": "Faithfulness",
        },
        {
            "type": "Answer Relevance",
        },
        {
            "type": "Answer Correctness",
        },
        {
            "type": "Context Precision",
        },
        {
            "type": "Context Relevance",
        },
        {
            "type": "Context Recall",
        }
    ]	

    for m in metrics:
        assert m in expected_metrics
    for m in expected_metrics:
        assert m in metrics

    assert metrics == expected_metrics

    # Test evaluation metadata. TODO put correct info here.
    expected_metadata = {
        "datums": 3,
        "labels": 8,
        "annotations": 6,
    }

    for key, value in expected_metadata.items():
        assert metadata[key] == value

    assert (
        metadata["duration"] <= 5
    )  # TODO Is this redundant with the timeout parameter in wait_for_completion above? Copied from test_classification.py