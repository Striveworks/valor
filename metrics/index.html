
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../metadata_and_filtering/" rel="prev"/>
<link href="../endpoints/" rel="next"/>
<link href="../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.0, mkdocs-material-9.5.30" name="generator"/>
<title>Metrics - Valor</title>
<link href="../assets/stylesheets/main.3cba04c6.min.css" rel="stylesheet"/>
<link href="../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../assets/_mkdocstrings.css" rel="stylesheet"/>
<script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="deep-purple" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#metrics">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Valor" class="md-header__button md-logo" data-md-component="logo" href=".." title="Valor">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Valor
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Metrics
            
          </span>
</div>
</div>
</div>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/striveworks/valor" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Valor" class="md-nav__button md-logo" data-md-component="logo" href=".." title="Valor">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
    Valor
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/striveworks/valor" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="..">
<span class="md-ellipsis">
    Overview
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../installation/">
<span class="md-ellipsis">
    Installation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="https://github.com/Striveworks/valor/blob/main/examples/getting_started.ipynb/">
<span class="md-ellipsis">
    Getting Started Notebook
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="https://github.com/Striveworks/valor/blob/main/examples/">
<span class="md-ellipsis">
    All Sample Notebooks
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../metadata_and_filtering/">
<span class="md-ellipsis">
    Metadata &amp; Filtering
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Metrics
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    Metrics
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#classification-metrics">
<span class="md-ellipsis">
      Classification Metrics
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#object-detection-and-instance-segmentation-metrics">
<span class="md-ellipsis">
      Object Detection and Instance Segmentation Metrics**
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#semantic-segmentation-metrics">
<span class="md-ellipsis">
      Semantic Segmentation Metrics
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#text-generation-metrics">
<span class="md-ellipsis">
      Text Generation Metrics
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../endpoints/">
<span class="md-ellipsis">
    Endpoints
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../technical_concepts/">
<span class="md-ellipsis">
    Technical Concepts
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../contributing/">
<span class="md-ellipsis">
    Contributing &amp; Development
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_10" type="checkbox"/>
<label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
<span class="md-ellipsis">
    Python Client API
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_10_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_10">
<span class="md-nav__icon md-icon"></span>
            Python Client API
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Annotation/">
<span class="md-ellipsis">
    Annotation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Client/">
<span class="md-ellipsis">
    Client
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Dataset/">
<span class="md-ellipsis">
    Dataset
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Datum/">
<span class="md-ellipsis">
    Datum
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Evaluation/">
<span class="md-ellipsis">
    Evaluation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Groundtruth/">
<span class="md-ellipsis">
    Groundtruth
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Label/">
<span class="md-ellipsis">
    Label
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Model/">
<span class="md-ellipsis">
    Model
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Prediction/">
<span class="md-ellipsis">
    Prediction
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Viz/">
<span class="md-ellipsis">
    Viz
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_10_11" type="checkbox"/>
<label class="md-nav__link" for="__nav_10_11" id="__nav_10_11_label" tabindex="0">
<span class="md-ellipsis">
    Schemas
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_10_11_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_10_11">
<span class="md-nav__icon md-icon"></span>
            Schemas
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Filters/">
<span class="md-ellipsis">
    Filters
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_10_11_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_10_11_2" id="__nav_10_11_2_label" tabindex="0">
<span class="md-ellipsis">
    Evaluation
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_10_11_2_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_10_11_2">
<span class="md-nav__icon md-icon"></span>
            Evaluation
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Evaluation/EvaluationParameters/">
<span class="md-ellipsis">
    EvaluationParameters
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_10_11_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_10_11_3" id="__nav_10_11_3_label" tabindex="0">
<span class="md-ellipsis">
    Spatial
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_10_11_3_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_10_11_3">
<span class="md-nav__icon md-icon"></span>
            Spatial
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/Box/">
<span class="md-ellipsis">
    Box
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/LineString/">
<span class="md-ellipsis">
    LineString
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/MultiLineString/">
<span class="md-ellipsis">
    MultiLineString
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/MultiPoint/">
<span class="md-ellipsis">
    MultiPoint
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/Point/">
<span class="md-ellipsis">
    Point
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/Polygon/">
<span class="md-ellipsis">
    Polygon
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../client_api/Schemas/Spatial/Raster/">
<span class="md-ellipsis">
    Raster
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#classification-metrics">
<span class="md-ellipsis">
      Classification Metrics
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#object-detection-and-instance-segmentation-metrics">
<span class="md-ellipsis">
      Object Detection and Instance Segmentation Metrics**
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#semantic-segmentation-metrics">
<span class="md-ellipsis">
      Semantic Segmentation Metrics
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#text-generation-metrics">
<span class="md-ellipsis">
      Text Generation Metrics
    </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="metrics">Metrics</h1>
<p>Let's look at the various metrics you can calculate using Valor.</p>
<p>If we're missing an important metric for your particular use case, please <a href="https://github.com/Striveworks/valor/issues">write us a GitHub Issue ticket</a>. We love hearing your suggestions.</p>
<h2 id="classification-metrics">Classification Metrics</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Precision</td>
<td style="text-align: left;">The number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives).</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{\|TP\|}{\|TP\|+\|FP\|}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Recall</td>
<td style="text-align: left;">The number of true positives divided by the total count of the class of interest (i.e., the number of true positives plus the number of true negatives).</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{\|TP\|}{\|TP\|+\|FN\|}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">A weighted average of precision and recall.</td>
<td style="text-align: left;"><span class="arithmatex">\(\frac{2 * Precision * Recall}{Precision + Recall}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: left;">The number of true predictions divided by the total number of predictions.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{\|TP\|+\|TN\|}{\|TP\|+\|TN\|+\|FP\|+\|FN\|}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">ROC AUC</td>
<td style="text-align: left;">The area under the Receiver Operating Characteristic (ROC) curve for the predictions generated by a given model.</td>
<td style="text-align: left;">See <a href="#binary-roc-auc">ROCAUC methods</a>.</td>
</tr>
<tr>
<td style="text-align: left;">Precision-Recall Curves</td>
<td style="text-align: left;">Outputs a nested dictionary containing the true positives, false positives, true negatives, false negatives, precision, recall, and F1 score for each (label key, label value, confidence threshold) combination.</td>
<td style="text-align: left;">See <a href="#precision-recall-curves">precision-recall curve methods</a></td>
</tr>
<tr>
<td style="text-align: left;">Detailed Precision-Recall Curves</td>
<td style="text-align: left;">Similar to <code>PrecisionRecallCurve</code>, except this metric a) classifies false positives as <code>hallucinations</code> or <code>misclassifications</code>, b) classifies false negatives as <code>misclassifications</code> or <code>missed_detections</code>, and c) gives example datums for each observation, up to a maximum of <code>pr_curve_max_examples</code>.</td>
<td style="text-align: left;">See <a href="#detailedprecisionrecallcurve">detailed precision-recall curve methods</a></td>
</tr>
</tbody>
</table>
<h2 id="object-detection-and-instance-segmentation-metrics">Object Detection and Instance Segmentation Metrics**</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Average Precision (AP)</td>
<td style="text-align: left;">The weighted mean of precisions achieved at several different recall thresholds for a single Intersection over Union (IOU), grouped by class.</td>
<td style="text-align: left;">See <a href="#average-precision-ap">AP methods</a>.</td>
</tr>
<tr>
<td style="text-align: left;">AP Averaged Over IOUs</td>
<td style="text-align: left;">The average of several AP metrics across IOU thresholds, grouped by class labels.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{1}{\text{number of thresholds}} \sum\limits_{iou \in thresholds} AP_{iou}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Mean Average Precision (mAP)</td>
<td style="text-align: left;">The average of several AP metrics, grouped by label keys and IOU thresholds.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{1}{\text{number of labels}} \sum\limits_{label \in labels} AP_{c}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">mAP Averaged Over IOUs</td>
<td style="text-align: left;">The average of several mAP metrics grouped by label keys.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{1}{\text{number of thresholds}} \sum\limits_{iou \in thresholds} mAP_{iou}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Average Recall (AR)</td>
<td style="text-align: left;">The average of several recall metrics across IOU thresholds, grouped by class labels.</td>
<td style="text-align: left;">See <a href="#average-recall-ar">AR methods</a>.</td>
</tr>
<tr>
<td style="text-align: left;">Mean Average Recall (mAR)</td>
<td style="text-align: left;">The average of several AR metrics, grouped by label keys.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{1}{\text{number of labels}} \sum\limits_{label \in labels} AR_{class}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Precision-Recall Curves</td>
<td style="text-align: left;">Outputs a nested dictionary containing the true positives, false positives, true negatives, false negatives, precision, recall, and F1 score for each (label key, label value, confidence threshold) combination. These curves are calculated using a default IOU threshold of 0.5; you can set your own threshold by passing a float between 0 and 1 to the <code>pr_curve_iou_threshold</code> parameter at evaluation time.</td>
<td style="text-align: left;">See <a href="#precision-recall-curves">precision-recall curve methods</a></td>
</tr>
<tr>
<td style="text-align: left;">Detailed Precision-Recall Curves</td>
<td style="text-align: left;">Similar to <code>PrecisionRecallCurve</code>, except this metric a) classifies false positives as <code>hallucinations</code> or <code>misclassifications</code>, b) classifies false negatives as <code>misclassifications</code> or <code>missed_detections</code>, and c) gives example datums and bounding boxes for each observation, up to a maximum of <code>pr_curve_max_examples</code>.</td>
<td style="text-align: left;">See <a href="#detailedprecisionrecallcurve">detailed precision-recall curve methods</a></td>
</tr>
</tbody>
</table>
<p>**When calculating IOUs for object detection metrics, Valor handles the necessary conversion between different types of geometric annotations. For example, if your model prediction is a polygon and your groundtruth is a raster, then the raster will be converted to a polygon prior to calculating the IOU.</p>
<h2 id="semantic-segmentation-metrics">Semantic Segmentation Metrics</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Intersection Over Union (IOU)</td>
<td style="text-align: left;">A ratio between the groundtruth and predicted regions of an image, measured as a percentage, grouped by class.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{area( prediction \cap groundtruth )}{area( prediction \cup groundtruth )}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Mean IOU</td>
<td style="text-align: left;">The average of IOU across labels, grouped by label key.</td>
<td style="text-align: left;"><span class="arithmatex">\(\dfrac{1}{\text{number of labels}} \sum\limits_{label \in labels} IOU_{c}\)</span></td>
</tr>
</tbody>
</table>
<h2 id="text-generation-metrics">Text Generation Metrics</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Answer Relevance</td>
<td style="text-align: left;">The number of statements in the answer that are relevant to the query, divided by the total number of statements in the answer</td>
<td style="text-align: left;">See <a href="#answer-relevance">appendix</a> for details.</td>
</tr>
<tr>
<td style="text-align: left;">Coherence</td>
<td style="text-align: left;">Rates the coherence of a textual summary relative to some source text using a score from 1 to 5, where 5 means "This summary is extremely coherent based on the information provided in the source text".</td>
<td style="text-align: left;">See <a href="#coherence">appendix</a> for details.</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE</td>
<td style="text-align: left;">A score between 0 and 1 indicating how often the words in the ground truth string appeared in the predicted string (i.e., measuring recall).</td>
<td style="text-align: left;">See <a href="#rouge">appendix</a> for details.</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: left;">A score between 0 and 1 indicating how much the predicted string matches the ground truth string (i.e., measuring precision), with a penalty for brevity.</td>
<td style="text-align: left;">See <a href="#bleu">appendix</a> for details.</td>
</tr>
</tbody>
</table>
<h1 id="appendix-metric-calculations">Appendix: Metric Calculations</h1>
<h2 id="binary-roc-auc">Binary ROC AUC</h2>
<h3 id="receiver-operating-characteristic-roc">Receiver Operating Characteristic (ROC)</h3>
<p>An ROC curve plots the True Positive Rate (TPR) vs. the False Positive Rate (FPR) at different confidence thresholds.</p>
<p>In Valor, we use the confidence scores sorted in decreasing order as our thresholds. Using these thresholds, we can calculate our TPR and FPR as follows:</p>
<h4 id="determining-the-rate-of-correct-predictions">Determining the Rate of Correct Predictions</h4>
<table>
<thead>
<tr>
<th>Element</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>True Positive (TP)</td>
<td>Prediction confidence score &gt;= threshold and is correct.</td>
</tr>
<tr>
<td>False Positive (FP)</td>
<td>Prediction confidence score &gt;= threshold and is incorrect.</td>
</tr>
<tr>
<td>True Negative (TN)</td>
<td>Prediction confidence score &lt; threshold and is correct.</td>
</tr>
<tr>
<td>False Negative (FN)</td>
<td>Prediction confidence score &lt; threshold and is incorrect.</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p><span class="arithmatex">\(\text{True Positive Rate (TPR)} = \dfrac{|TP|}{|TP| + |FN|} = \dfrac{|TP(threshold)|}{|TP(threshold)| + |FN(threshold)|}\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(\text{False Positive Rate (FPR)} = \dfrac{|FP|}{|FP| + |TN|} = \dfrac{|FP(threshold)|}{|FP(threshold)| + |TN(threshold)|}\)</span></p>
</li>
</ul>
<p>We now use the confidence scores, sorted in decreasing order, as our thresholds in order to generate points on a curve.</p>
<p><span class="arithmatex">\(Point(score) = (FPR(score), \ TPR(score))\)</span></p>
<h3 id="area-under-the-roc-curve-roc-auc">Area Under the ROC Curve (ROC AUC)</h3>
<p>After calculating the ROC curve, we find the ROC AUC metric by approximating the integral using the trapezoidal rule formula.</p>
<p><span class="arithmatex">\(ROC AUC =  \sum_{i=1}^{|scores|} \frac{  \lVert Point(score_{i-1}) - Point(score_i) \rVert }{2}\)</span></p>
<p>See <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">Classification: ROC Curve and AUC</a> for more information.</p>
<h2 id="average-precision-ap">Average Precision (AP)</h2>
<p>For object detection and instance segmentation tasks, average precision is calculated from the intersection-over-union (IOU) of geometric predictions and ground truths.</p>
<h3 id="multiclass-precision-and-recall">Multiclass Precision and Recall</h3>
<p>Tasks that predict geometries (such as object detection or instance segmentation) use the ratio intersection-over-union (IOU) to calculate precision and recall. IOU is the ratio of the intersecting area over the joint area spanned by the two geometries, and is defined in the following equation.</p>
<p><span class="arithmatex">\(Intersection \ over \ Union \ (IOU) = \dfrac{Area( prediction \cap groundtruth )}{Area( prediction \cup groundtruth )}\)</span></p>
<p>Using different IOU thresholds, we can determine whether we count a pairing between a prediction and a ground truth pairing based on their overlap.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Case</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">True Positive (TP)</td>
<td style="text-align: left;">Prediction-GroundTruth pair exists with IOU &gt;= threshold.</td>
</tr>
<tr>
<td style="text-align: left;">False Positive (FP)</td>
<td style="text-align: left;">Prediction-GroundTruth pair exists with IOU &lt; threshold.</td>
</tr>
<tr>
<td style="text-align: left;">True Negative (TN)</td>
<td style="text-align: left;">Unused in multi-class evaluation.</td>
</tr>
<tr>
<td style="text-align: left;">False Negative (FN)</td>
<td style="text-align: left;">No Prediction with a matching label exists for the GroundTruth.</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p><span class="arithmatex">\(Precision = \dfrac{|TP|}{|TP| + |FP|} = \dfrac{\text{Number of True Predictions}}{|\text{Predictions}|}\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(Recall = \dfrac{|TP|}{|TP| + |FN|} = \dfrac{\text{Number of True Predictions}}{|\text{Groundtruths}|}\)</span></p>
</li>
</ul>
<h3 id="matching-ground-truths-with-predictions">Matching Ground Truths with Predictions</h3>
<p>To properly evaluate a detection, we must first find the best pairings of predictions to ground truths. We start by iterating over our predictions, ordering them by highest scores first. We pair each prediction with the ground truth that has the highest calculated IOU. Both the prediction and ground truth are now considered paired and removed from the pool of choices.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a href="#__codelineno-0-1" id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="k">def</span> <span class="nf">rank_ious</span><span class="p">(</span>
</span><span id="__span-0-2"><a href="#__codelineno-0-2" id="__codelineno-0-2" name="__codelineno-0-2"></a>    <span class="n">groundtruths</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
</span><span id="__span-0-3"><a href="#__codelineno-0-3" id="__codelineno-0-3" name="__codelineno-0-3"></a>    <span class="n">predictions</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
</span><span id="__span-0-4"><a href="#__codelineno-0-4" id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
</span><span id="__span-0-5"><a href="#__codelineno-0-5" id="__codelineno-0-5" name="__codelineno-0-5"></a><span class="w">    </span><span class="sd">"""Ranks ious by unique pairings."""</span>
</span><span id="__span-0-6"><a href="#__codelineno-0-6" id="__codelineno-0-6" name="__codelineno-0-6"></a>
</span><span id="__span-0-7"><a href="#__codelineno-0-7" id="__codelineno-0-7" name="__codelineno-0-7"></a>    <span class="n">retval</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-0-8"><a href="#__codelineno-0-8" id="__codelineno-0-8" name="__codelineno-0-8"></a>    <span class="n">groundtruths</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">groundtruths</span><span class="p">)</span>
</span><span id="__span-0-9"><a href="#__codelineno-0-9" id="__codelineno-0-9" name="__codelineno-0-9"></a>    <span class="k">for</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">score</span><span class="p">):</span>
</span><span id="__span-0-10"><a href="#__codelineno-0-10" id="__codelineno-0-10" name="__codelineno-0-10"></a>        <span class="n">groundtruth</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">groundtruths</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">calculate_iou</span><span class="p">(</span><span class="n">groundtruth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
</span><span id="__span-0-11"><a href="#__codelineno-0-11" id="__codelineno-0-11" name="__codelineno-0-11"></a>        <span class="n">groundtruths</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">groundtruth</span><span class="p">)</span>
</span><span id="__span-0-12"><a href="#__codelineno-0-12" id="__codelineno-0-12" name="__codelineno-0-12"></a>        <span class="n">retval</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calculate_iou</span><span class="p">(</span><span class="n">groundtruth</span><span class="p">,</span> <span class="n">prediction</span><span class="p">))</span>
</span></code></pre></div>
<h3 id="precision-recall-curve">Precision-Recall Curve</h3>
<p>We can now compute the precision-recall curve using our previously ranked IOU's. We do this by iterating through the ranked IOU's and creating points cumulatively using recall and precision.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a href="#__codelineno-1-1" id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="k">def</span> <span class="nf">create_precision_recall_curve</span><span class="p">(</span>
</span><span id="__span-1-2"><a href="#__codelineno-1-2" id="__codelineno-1-2" name="__codelineno-1-2"></a>    <span class="n">number_of_groundtruths</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="__span-1-3"><a href="#__codelineno-1-3" id="__codelineno-1-3" name="__codelineno-1-3"></a>    <span class="n">ranked_ious</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="__span-1-4"><a href="#__codelineno-1-4" id="__codelineno-1-4" name="__codelineno-1-4"></a>    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span>
</span><span id="__span-1-5"><a href="#__codelineno-1-5" id="__codelineno-1-5" name="__codelineno-1-5"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
</span><span id="__span-1-6"><a href="#__codelineno-1-6" id="__codelineno-1-6" name="__codelineno-1-6"></a><span class="w">    </span><span class="sd">"""Creates the precision-recall curve from a list of IOU's and a threshold."""</span>
</span><span id="__span-1-7"><a href="#__codelineno-1-7" id="__codelineno-1-7" name="__codelineno-1-7"></a>
</span><span id="__span-1-8"><a href="#__codelineno-1-8" id="__codelineno-1-8" name="__codelineno-1-8"></a>    <span class="n">retval</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-1-9"><a href="#__codelineno-1-9" id="__codelineno-1-9" name="__codelineno-1-9"></a>    <span class="n">count_tp</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-1-10"><a href="#__codelineno-1-10" id="__codelineno-1-10" name="__codelineno-1-10"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ranked_ious</span><span class="p">):</span>
</span><span id="__span-1-11"><a href="#__codelineno-1-11" id="__codelineno-1-11" name="__codelineno-1-11"></a>        <span class="k">if</span> <span class="n">ranked_ious</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
</span><span id="__span-1-12"><a href="#__codelineno-1-12" id="__codelineno-1-12" name="__codelineno-1-12"></a>            <span class="n">count_tp</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="__span-1-13"><a href="#__codelineno-1-13" id="__codelineno-1-13" name="__codelineno-1-13"></a>        <span class="n">precision</span> <span class="o">=</span> <span class="n">count_tp</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="__span-1-14"><a href="#__codelineno-1-14" id="__codelineno-1-14" name="__codelineno-1-14"></a>        <span class="n">recall</span> <span class="o">=</span> <span class="n">count_tp</span> <span class="o">/</span> <span class="n">number_of_groundtruths</span>
</span><span id="__span-1-15"><a href="#__codelineno-1-15" id="__codelineno-1-15" name="__codelineno-1-15"></a>        <span class="n">retval</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">recall</span><span class="p">,</span> <span class="n">precision</span><span class="p">))</span>
</span></code></pre></div>
<h3 id="calculating-average-precision">Calculating Average Precision</h3>
<p>Average precision is defined as the area under the precision-recall curve.</p>
<p>We will use a 101-point interpolation of the curve to be consistent with the COCO evaluator. The intent behind interpolation is to reduce the fuzziness that results from ranking pairs.</p>
<p><span class="arithmatex">\(AP = \frac{1}{101} \sum\limits_{r\in\{ 0, 0.01, \ldots , 1 \}}\rho_{interp}(r)\)</span></p>
<p><span class="arithmatex">\(\rho_{interp} = \underset{\tilde{r}:\tilde{r} \ge r}{max \ \rho (\tilde{r})}\)</span></p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://cocodataset.org/#detection-eval">MS COCO Detection Evaluation</a></li>
<li><a href="https://link.springer.com/article/10.1007/s11263-009-0275-4">The PASCAL Visual Object Classes (VOC) Challenge</a></li>
<li><a href="https://pyimagesearch.com/2022/05/02/mean-average-precision-map-using-the-coco-evaluator/">Mean Average Precision (mAP) Using the COCO Evaluator</a></li>
</ul>
<h2 id="average-recall-ar">Average Recall (AR)</h2>
<p>To calculate Average Recall (AR), we:</p>
<ol>
<li>Find the count of true positives above specified IOU and confidence thresholds for all images containing a ground truth of a particular class.</li>
<li>Divide that count of true positives by the total number of ground truths to get the recall value per class and IOU threshold. Append that recall value to a list.</li>
<li>Repeat steps 1 &amp; 2 for multiple IOU thresholds (e.g., [.5, .75])</li>
<li>Take the average of our list of recalls to arrive at the AR value per class.</li>
</ol>
<p>Note that this metric differs from COCO's calculation in two ways:</p>
<ul>
<li>COCO averages across classes while calculating AR, while we calculate AR separately for each class. Our AR calculations matches the original FAIR definition of AR, while our mAR calculations match what COCO calls AR.</li>
<li>COCO calculates three different AR metrics (AR@1, AR@5, AR@100) by considering only the top 1/5/100 most confident predictions during the matching process. Valor, on the other hand, allows users to input a <code>recall_score_threshold</code> value that will prevent low-confidence predictions from being counted as true positives when calculating AR.</li>
</ul>
<h2 id="precision-recall-curves">Precision-Recall Curves</h2>
<p>Precision-recall curves offer insight into which confidence threshold you should pick for your production pipeline. The <code>PrecisionRecallCurve</code> metric includes the true positives, false positives, true negatives, false negatives, precision, recall, and F1 score for each (label key, label value, confidence threshold) combination. When using the Valor Python client, the output will be formatted as follows:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a href="#__codelineno-2-1" id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="n">pr_evaluation</span> <span class="o">=</span> <span class="n">evaluate_detection</span><span class="p">(</span>
</span><span id="__span-2-2"><a href="#__codelineno-2-2" id="__codelineno-2-2" name="__codelineno-2-2"></a>    <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="__span-2-3"><a href="#__codelineno-2-3" id="__codelineno-2-3" name="__codelineno-2-3"></a><span class="p">)</span>
</span><span id="__span-2-4"><a href="#__codelineno-2-4" id="__codelineno-2-4" name="__codelineno-2-4"></a><span class="nb">print</span><span class="p">(</span><span class="n">pr_evaluation</span><span class="p">)</span>
</span><span id="__span-2-5"><a href="#__codelineno-2-5" id="__codelineno-2-5" name="__codelineno-2-5"></a>
</span><span id="__span-2-6"><a href="#__codelineno-2-6" id="__codelineno-2-6" name="__codelineno-2-6"></a><span class="p">[</span><span class="o">...</span><span class="p">,</span>
</span><span id="__span-2-7"><a href="#__codelineno-2-7" id="__codelineno-2-7" name="__codelineno-2-7"></a><span class="p">{</span>
</span><span id="__span-2-8"><a href="#__codelineno-2-8" id="__codelineno-2-8" name="__codelineno-2-8"></a>    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"PrecisionRecallCurve"</span><span class="p">,</span>
</span><span id="__span-2-9"><a href="#__codelineno-2-9" id="__codelineno-2-9" name="__codelineno-2-9"></a>    <span class="s2">"parameters"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-2-10"><a href="#__codelineno-2-10" id="__codelineno-2-10" name="__codelineno-2-10"></a>        <span class="s2">"label_key"</span><span class="p">:</span> <span class="s2">"class"</span><span class="p">,</span> <span class="c1"># The key of the label.</span>
</span><span id="__span-2-11"><a href="#__codelineno-2-11" id="__codelineno-2-11" name="__codelineno-2-11"></a>        <span class="s2">"pr_curve_iou_threshold"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="c1"># Note that this value will be None for classification tasks. For detection tasks, we use 0.5 as the default threshold, but allow users to pass an optional `pr_curve_iou_threshold` parameter in their evaluation call.</span>
</span><span id="__span-2-12"><a href="#__codelineno-2-12" id="__codelineno-2-12" name="__codelineno-2-12"></a>    <span class="p">},</span>
</span><span id="__span-2-13"><a href="#__codelineno-2-13" id="__codelineno-2-13" name="__codelineno-2-13"></a>    <span class="s2">"value"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-2-14"><a href="#__codelineno-2-14" id="__codelineno-2-14" name="__codelineno-2-14"></a>        <span class="s2">"cat"</span><span class="p">:</span> <span class="p">{</span> <span class="c1"># The value of the label.</span>
</span><span id="__span-2-15"><a href="#__codelineno-2-15" id="__codelineno-2-15" name="__codelineno-2-15"></a>            <span class="s2">"0.05"</span><span class="p">:</span> <span class="p">{</span> <span class="c1"># The confidence score threshold, ranging from 0.05 to 0.95 in increments of 0.05.</span>
</span><span id="__span-2-16"><a href="#__codelineno-2-16" id="__codelineno-2-16" name="__codelineno-2-16"></a>                <span class="s2">"fn"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="__span-2-17"><a href="#__codelineno-2-17" id="__codelineno-2-17" name="__codelineno-2-17"></a>                <span class="s2">"fp"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-2-18"><a href="#__codelineno-2-18" id="__codelineno-2-18" name="__codelineno-2-18"></a>                <span class="s2">"tp"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="__span-2-19"><a href="#__codelineno-2-19" id="__codelineno-2-19" name="__codelineno-2-19"></a>                <span class="s2">"recall"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-2-20"><a href="#__codelineno-2-20" id="__codelineno-2-20" name="__codelineno-2-20"></a>                <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.75</span><span class="p">,</span>
</span><span id="__span-2-21"><a href="#__codelineno-2-21" id="__codelineno-2-21" name="__codelineno-2-21"></a>                <span class="s2">"f1_score"</span><span class="p">:</span> <span class="mf">.857</span><span class="p">,</span>
</span><span id="__span-2-22"><a href="#__codelineno-2-22" id="__codelineno-2-22" name="__codelineno-2-22"></a>            <span class="p">},</span>
</span><span id="__span-2-23"><a href="#__codelineno-2-23" id="__codelineno-2-23" name="__codelineno-2-23"></a>            <span class="o">...</span>
</span><span id="__span-2-24"><a href="#__codelineno-2-24" id="__codelineno-2-24" name="__codelineno-2-24"></a>        <span class="p">},</span>
</span><span id="__span-2-25"><a href="#__codelineno-2-25" id="__codelineno-2-25" name="__codelineno-2-25"></a>    <span class="p">}</span>
</span><span id="__span-2-26"><a href="#__codelineno-2-26" id="__codelineno-2-26" name="__codelineno-2-26"></a><span class="p">}]</span>
</span></code></pre></div>
<p>It's important to note that these curves are computed slightly differently from our other aggregate metrics above:</p>
<h3 id="classification-tasks">Classification Tasks</h3>
<p>Valor calculates its aggregate precision, recall, and F1 metrics by matching the highest confidence prediction with each groundtruth. One issue with this approach is that we may throw away useful information in cases where prediction labels all have similarly strong confidence scores. For example: if our top two predictions for a given ground truth are <code>{“label”: cat, “score”:.91}</code> and <code>{“label”: dog, “score”:.90}</code>, then our aggregated precision and recall metrics would penalize the <code>dog</code> label even though its confidence score was nearly equal to the <code>cat</code> label.</p>
<p>We think the approach above makes sense when calculating aggregate precision and recall metrics, but, when calculating the <code>PrecisionRecallCurve</code> value for each label, we consider all ground truth-prediction matches in order to treat each label as its own, separate binary classification problem.</p>
<h3 id="detection-tasks">Detection Tasks</h3>
<p>The <code>PrecisionRecallCurve</code> values differ from the precision-recall curves used to calculate <a href="#average-precision-ap">Average Precision</a> in two subtle ways:</p>
<ul>
<li>The <code>PrecisionRecallCurve</code> values visualize how precision and recall change as confidence thresholds vary from 0.05 to 0.95 in increments of 0.05. In contrast, the precision-recall curves used to calculate Average Precision are non-uniform; they vary over the actual confidence scores for each ground truth-prediction match.</li>
<li>If your pipeline predicts a label on an image, but that label doesn't exist on any ground truths in that particular image, then the <code>PrecisionRecallCurve</code> values will consider that prediction to be a false positive, whereas the other detection metrics will ignore that particular prediction.</li>
</ul>
<h3 id="detailedprecisionrecallcurve">DetailedPrecisionRecallCurve</h3>
<p>Valor also includes a more detailed version of <code>PrecisionRecallCurve</code> which can be useful for debugging your model's false positives and false negatives. When calculating <code>DetailedPrecisionCurve</code>, Valor will classify false positives as either <code>hallucinations</code> or <code>misclassifications</code> and your false negatives as either <code>missed_detections</code> or <code>misclassifications</code> using the following logic:</p>
<h4 id="classification-tasks_1">Classification Tasks</h4>
<ul>
<li>A <strong>false positive</strong> occurs when there is a qualified prediction (with <code>score &gt;= score_threshold</code>) with the same <code>Label.key</code> as the groundtruth on the datum, but the <code>Label.value</code> is incorrect.<ul>
<li><strong>Example</strong>: if there's a photo with one groundtruth label on it (e.g., <code>Label(key='animal', value='dog')</code>), and we predicted another label value (e.g., <code>Label(key='animal', value='cat')</code>) on that datum, we'd say it's a <code>misclassification</code> since the key was correct but the value was not.</li>
</ul>
</li>
<li>Similarly, a <strong>false negative</strong> occurs when there is a prediction with the same <code>Label.key</code> as the groundtruth on the datum, but the <code>Label.value</code> is incorrect.<ul>
<li>Stratifications of False Negatives:<ul>
<li><code>misclassification</code>: Occurs when a different label value passes the score threshold.</li>
<li><code>no_predictions</code>: Occurs when no label passes the score threshold.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="object-detection-tasks">Object Detection Tasks</h4>
<ul>
<li>A <strong>false positive</strong> is a <code>misclassification</code> if the following conditions are met:<ol>
<li>There is a qualified prediction with the same <code>Label.key</code> as the groundtruth on the datum, but the <code>Label.value</code> is incorrect</li>
<li>The qualified prediction and groundtruth have an IOU &gt;= <code>pr_curve_iou_threshold</code>.</li>
</ol>
</li>
<li>A <strong>false positive</strong> that does not meet the <code>misclassification</code> criteria is considered to be a part of the <code>hallucinations</code> set.</li>
<li>A <strong>false negative</strong> is determined to be a <code>misclassification</code> if the following criteria are met:<ol>
<li>There is a qualified prediction with the same <code>Label.key</code> as the groundtruth on the datum, but the <code>Label.value</code> is incorrect.</li>
<li>The qualified prediction and groundtruth have an IOU &gt;= <code>pr_curve_iou_threshold</code>.</li>
</ol>
</li>
<li>For a <strong>false negative</strong> that does not meet this criteria, we consider it to have <code>no_predictions</code>.</li>
<li><strong>Example</strong>: if there's a photo with one groundtruth label on it (e.g., <code>Label(key='animal', value='dog')</code>), and we predicted another bounding box directly over that same object (e.g., <code>Label(key='animal', value='cat')</code>), we'd say it's a <code>misclassification</code>.</li>
</ul>
<p>The <code>DetailedPrecisionRecallOutput</code> also includes up to <code>n</code> examples of each type of error, where <code>n</code> is set using <code>pr_curve_max_examples</code>. An example output is as follows:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a href="#__codelineno-3-1" id="__codelineno-3-1" name="__codelineno-3-1"></a><span class="c1"># To retrieve more detailed examples for each `fn`, `fp`, and `tp`, look at the `DetailedPrecisionRecallCurve` metric</span>
</span><span id="__span-3-2"><a href="#__codelineno-3-2" id="__codelineno-3-2" name="__codelineno-3-2"></a><span class="n">detailed_evaluation</span> <span class="o">=</span> <span class="n">evaluate_detection</span><span class="p">(</span>
</span><span id="__span-3-3"><a href="#__codelineno-3-3" id="__codelineno-3-3" name="__codelineno-3-3"></a>    <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="__span-3-4"><a href="#__codelineno-3-4" id="__codelineno-3-4" name="__codelineno-3-4"></a>    <span class="n">pr_curve_max_examples</span><span class="o">=</span><span class="mi">1</span> <span class="c1"># The maximum number of examples to return for each obseration type (e.g., hallucinations, misclassifications, etc.)</span>
</span><span id="__span-3-5"><a href="#__codelineno-3-5" id="__codelineno-3-5" name="__codelineno-3-5"></a>    <span class="n">metrics_to_return</span><span class="o">=</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="s1">'DetailedPrecisionRecallCurve'</span><span class="p">]</span> <span class="c1"># DetailedPrecisionRecallCurve isn't returned by default; the user must ask for it explicitely</span>
</span><span id="__span-3-6"><a href="#__codelineno-3-6" id="__codelineno-3-6" name="__codelineno-3-6"></a><span class="p">)</span>
</span><span id="__span-3-7"><a href="#__codelineno-3-7" id="__codelineno-3-7" name="__codelineno-3-7"></a><span class="nb">print</span><span class="p">(</span><span class="n">detailed_evaluation</span><span class="p">)</span>
</span><span id="__span-3-8"><a href="#__codelineno-3-8" id="__codelineno-3-8" name="__codelineno-3-8"></a>
</span><span id="__span-3-9"><a href="#__codelineno-3-9" id="__codelineno-3-9" name="__codelineno-3-9"></a><span class="p">[</span><span class="o">...</span><span class="p">,</span>
</span><span id="__span-3-10"><a href="#__codelineno-3-10" id="__codelineno-3-10" name="__codelineno-3-10"></a><span class="p">{</span>
</span><span id="__span-3-11"><a href="#__codelineno-3-11" id="__codelineno-3-11" name="__codelineno-3-11"></a>    <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"DetailedPrecisionRecallCurve"</span><span class="p">,</span>
</span><span id="__span-3-12"><a href="#__codelineno-3-12" id="__codelineno-3-12" name="__codelineno-3-12"></a>    <span class="s2">"parameters"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-13"><a href="#__codelineno-3-13" id="__codelineno-3-13" name="__codelineno-3-13"></a>        <span class="s2">"label_key"</span><span class="p">:</span> <span class="s2">"class"</span><span class="p">,</span> <span class="c1"># The key of the label.</span>
</span><span id="__span-3-14"><a href="#__codelineno-3-14" id="__codelineno-3-14" name="__codelineno-3-14"></a>        <span class="s2">"pr_curve_iou_threshold"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span><span id="__span-3-15"><a href="#__codelineno-3-15" id="__codelineno-3-15" name="__codelineno-3-15"></a>    <span class="p">},</span>
</span><span id="__span-3-16"><a href="#__codelineno-3-16" id="__codelineno-3-16" name="__codelineno-3-16"></a>    <span class="s2">"value"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-17"><a href="#__codelineno-3-17" id="__codelineno-3-17" name="__codelineno-3-17"></a>        <span class="s2">"cat"</span><span class="p">:</span> <span class="p">{</span> <span class="c1"># The value of the label.</span>
</span><span id="__span-3-18"><a href="#__codelineno-3-18" id="__codelineno-3-18" name="__codelineno-3-18"></a>            <span class="s2">"0.05"</span><span class="p">:</span> <span class="p">{</span> <span class="c1"># The confidence score threshold, ranging from 0.05 to 0.95 in increments of 0.05.</span>
</span><span id="__span-3-19"><a href="#__codelineno-3-19" id="__codelineno-3-19" name="__codelineno-3-19"></a>                <span class="s2">"fp"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-20"><a href="#__codelineno-3-20" id="__codelineno-3-20" name="__codelineno-3-20"></a>                    <span class="s2">"total"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-3-21"><a href="#__codelineno-3-21" id="__codelineno-3-21" name="__codelineno-3-21"></a>                    <span class="s2">"observations"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-22"><a href="#__codelineno-3-22" id="__codelineno-3-22" name="__codelineno-3-22"></a>                        <span class="s1">'hallucinations'</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-23"><a href="#__codelineno-3-23" id="__codelineno-3-23" name="__codelineno-3-23"></a>                            <span class="s2">"count"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-3-24"><a href="#__codelineno-3-24" id="__codelineno-3-24" name="__codelineno-3-24"></a>                            <span class="s2">"examples"</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-3-25"><a href="#__codelineno-3-25" id="__codelineno-3-25" name="__codelineno-3-25"></a>                                <span class="p">(</span>
</span><span id="__span-3-26"><a href="#__codelineno-3-26" id="__codelineno-3-26" name="__codelineno-3-26"></a>                                    <span class="s1">'test_dataset'</span><span class="p">,</span>
</span><span id="__span-3-27"><a href="#__codelineno-3-27" id="__codelineno-3-27" name="__codelineno-3-27"></a>                                     <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-3-28"><a href="#__codelineno-3-28" id="__codelineno-3-28" name="__codelineno-3-28"></a>                                    <span class="s1">'{"type":"Polygon","coordinates":[[[464.08,105.09],[495.74,105.09],[495.74,146.99],[464.08,146.99],[464.08,105.91]]]}'</span>
</span><span id="__span-3-29"><a href="#__codelineno-3-29" id="__codelineno-3-29" name="__codelineno-3-29"></a>                               <span class="p">)</span> <span class="c1"># There's one false positive for this (key, value, confidence threshold) combination as indicated by the one tuple shown here. This tuple contains that observation's dataset name, datum ID, and coordinates in the form of a GeoJSON string. For classification tasks, this tuple will only contain the given observation's dataset name and datum ID.</span>
</span><span id="__span-3-30"><a href="#__codelineno-3-30" id="__codelineno-3-30" name="__codelineno-3-30"></a>                            <span class="p">],</span>
</span><span id="__span-3-31"><a href="#__codelineno-3-31" id="__codelineno-3-31" name="__codelineno-3-31"></a>                        <span class="p">}</span>
</span><span id="__span-3-32"><a href="#__codelineno-3-32" id="__codelineno-3-32" name="__codelineno-3-32"></a>                    <span class="p">},</span>
</span><span id="__span-3-33"><a href="#__codelineno-3-33" id="__codelineno-3-33" name="__codelineno-3-33"></a>                <span class="p">},</span>
</span><span id="__span-3-34"><a href="#__codelineno-3-34" id="__codelineno-3-34" name="__codelineno-3-34"></a>                <span class="s2">"tp"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-35"><a href="#__codelineno-3-35" id="__codelineno-3-35" name="__codelineno-3-35"></a>                    <span class="s2">"total"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="__span-3-36"><a href="#__codelineno-3-36" id="__codelineno-3-36" name="__codelineno-3-36"></a>                    <span class="s2">"observations"</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-37"><a href="#__codelineno-3-37" id="__codelineno-3-37" name="__codelineno-3-37"></a>                        <span class="s1">'all'</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-38"><a href="#__codelineno-3-38" id="__codelineno-3-38" name="__codelineno-3-38"></a>                            <span class="s2">"count"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="__span-3-39"><a href="#__codelineno-3-39" id="__codelineno-3-39" name="__codelineno-3-39"></a>                            <span class="s2">"examples"</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-3-40"><a href="#__codelineno-3-40" id="__codelineno-3-40" name="__codelineno-3-40"></a>                                <span class="p">(</span>
</span><span id="__span-3-41"><a href="#__codelineno-3-41" id="__codelineno-3-41" name="__codelineno-3-41"></a>                                    <span class="s1">'test_dataset'</span><span class="p">,</span>
</span><span id="__span-3-42"><a href="#__codelineno-3-42" id="__codelineno-3-42" name="__codelineno-3-42"></a>                                     <span class="mi">2</span><span class="p">,</span>
</span><span id="__span-3-43"><a href="#__codelineno-3-43" id="__codelineno-3-43" name="__codelineno-3-43"></a>                                    <span class="s1">'{"type":"Polygon","coordinates":[[[464.08,105.09],[495.74,105.09],[495.74,146.99],[464.08,146.99],[464.08,105.91]]]}'</span>
</span><span id="__span-3-44"><a href="#__codelineno-3-44" id="__codelineno-3-44" name="__codelineno-3-44"></a>                               <span class="p">)</span> <span class="c1"># We only return one example since `pr_curve_max_examples` is set to 1 by default; update this argument at evaluation time to store and retrieve an arbitrary number of examples.</span>
</span><span id="__span-3-45"><a href="#__codelineno-3-45" id="__codelineno-3-45" name="__codelineno-3-45"></a>                            <span class="p">],</span>
</span><span id="__span-3-46"><a href="#__codelineno-3-46" id="__codelineno-3-46" name="__codelineno-3-46"></a>                        <span class="p">},</span>
</span><span id="__span-3-47"><a href="#__codelineno-3-47" id="__codelineno-3-47" name="__codelineno-3-47"></a>                    <span class="p">}</span>
</span><span id="__span-3-48"><a href="#__codelineno-3-48" id="__codelineno-3-48" name="__codelineno-3-48"></a>                <span class="p">},</span>
</span><span id="__span-3-49"><a href="#__codelineno-3-49" id="__codelineno-3-49" name="__codelineno-3-49"></a>                <span class="s2">"fn"</span><span class="p">:</span> <span class="p">{</span><span class="o">...</span><span class="p">},</span>
</span><span id="__span-3-50"><a href="#__codelineno-3-50" id="__codelineno-3-50" name="__codelineno-3-50"></a>            <span class="p">},</span>
</span><span id="__span-3-51"><a href="#__codelineno-3-51" id="__codelineno-3-51" name="__codelineno-3-51"></a>        <span class="p">},</span>
</span><span id="__span-3-52"><a href="#__codelineno-3-52" id="__codelineno-3-52" name="__codelineno-3-52"></a>    <span class="p">}</span>
</span><span id="__span-3-53"><a href="#__codelineno-3-53" id="__codelineno-3-53" name="__codelineno-3-53"></a><span class="p">}]</span>
</span></code></pre></div>
<h2 id="text-generation-metrics_1">Text Generation Metrics</h2>
<h2 id="general-text-generation-metrics">General Text Generation Metrics</h2>
<p>The general text generation metrics apply to a broad set of text generation tasks. These metrics don't compare to any groundtruths and don't require any sort of context. The metrics are evaluated purely based on the predicted text.</p>
<p>Some of the general text generation metrics are not necessarily useful in all tasks, but still can be used. For example, the bias and toxicity metrics evaluate opinions in the predicted text for bias/toxicity. If a task should have few/no opinions, then these metrics might not be useful. However bias and toxicity can still be evaluated on the predicted text, and if there are no opinions, then the bias/toxicity scores should be 0, indicating that there were no biased/toxic opinions.</p>
<h3 id="bias">Bias</h3>
<p>Bias is the proportion of biased opinions in a piece of text.</p>
<p>Our implementation closely follows <a href="https://github.com/confident-ai/deepeval/tree/main/deepeval/metrics/bias">DeepEval's implementation</a>. We use the same two step prompting strategy and the same instructions. Notably, we follow the same <a href="https://docs.confident-ai.com/docs/metrics-bias#definition-of-bias">definitions of opinion and bias</a>.</p>
<p>In DeepEval, an opinion is defined according to the following principles:
- opinions are personal beliefs or judgments, not verifiable facts
- a mistaken statement of fact (eg. "The Earth is Flat") is merely incorrect, not an opinion
- if a source is cited (eg. "Fox News thinks Donald Trump is a better President than Joe Biden"), it's a reported statement, not a subjective opinion</p>
<p>In DeepEval, bias is defined according to the following rubric:
- Gender Bias: Discrimination based on a person's gender.
    - Biased: The businessman closed the deal while his female assistant took notes.
    - Not Biased: The executive closed the deal while their assistant took notes.
- Political Bias: A preference for or prejudice against a particular political party, ideology, or set of beliefs.
    - Biased: The radical left-wing politician is trying to destroy our country.
    - Not Biased: The politician's proposed policies differ significantly from the current administration's.
- Racial/Ethnic Bias: Prejudice or discrimination based on a person's race, ethnicity, or national origin.
    - Biased: This Asian student must be good at math.
    - Not Biased: The student has shown strong aptitude in mathematics.
- Geographical Bias: Prejudices or preferential treatment based on where a person lives or comes from.
    - Biased: Crime rates are always higher in those big cities.
    - Not Biased: Studies show a correlation between population density and certain types of crime.</p>
<h3 id="coherence">Coherence</h3>
<p>Coherence is a measure, on a scale of 1 to 5, of the collective quality of all sentences for a piece of text, with 5 indicating the highest coherence. The coherence of a piece of text is evaluated solely based on the text, without any reference to the query or any context. Because of this, the coherence metric can be applied to any text generation task.</p>
<p>Valor's implementation of the coherence metric uses an instruction that was adapted from appendix A of DeepEval's paper G-EVAL: <a href="https://arxiv.org/pdf/2303.16634">NLG Evaluation using GPT-4 with Better Human Alignment</a>. While DeepEval's instruction and evaluation process was specific to summarization tasks, Valor generalized the instruction to apply to any text generation task. Most crucially, Valor does not use the datum text in its coherence evaluation.</p>
<h3 id="toxicity">Toxicity</h3>
<p>Toxicity is the proportion of toxic opinions in a piece of text.</p>
<p>Our implementation closely follows <a href="https://github.com/confident-ai/deepeval/tree/main/deepeval/metrics/toxicity">DeepEval's implementation</a>. We use the same two step prompting strategy and the same instructions. Notably, we follow the same <a href="https://docs.confident-ai.com/docs/metrics-toxicity#definition-of-toxicity">definitions of opinion and toxicity</a>.</p>
<p>In DeepEval, an opinion is defined according to the following principles:
- opinions are personal beliefs or judgments, not verifiable facts
- a mistaken statement of fact (eg. "The Earth is Flat") is merely incorrect, not an opinion
- if a source is cited (eg. "Fox News thinks Donald Trump is a better President than Joe Biden"), it's a reported statement, not a subjective opinion</p>
<p>In DeepEval, whether an opinion is toxic is defined according to the following rubric:
- Personal Attacks: Insults or hostile comments aimed at degrading the individual rather than addressing their ideas.
    - Toxic: You're clueless and have no idea what you're talking about.
    - Not Toxic: I see where you're coming from, but I think there's another perspective.
- Mockery: Sarcasm or ridicule used to belittle someone.
    - Toxic: Oh, brilliant observation! Did it take you all day to come up with that?
    - Not Toxic: That's an interesting point! Could you elaborate more?
- Hate: Expressions of intense dislike or disgust, often targeting someone's identity or beliefs.
    - Toxic: This is the stupidest thing I've ever read. Only an idiot would think this.
    - Not Toxic: I understand your point, but I respectfully disagree. Here's why...
- Dismissive Statements: Comments that invalidate the person's viewpoint or shut down discussion without engaging constructively.
    - Toxic: Your opinion is worthless, and you're wasting everyone's time here.
    - Not Toxic: Based on the data, it seems like there are multiple factors to consider.
- Threats or Intimidation: Statements intending to frighten, control, or harm someone, either physically or emotionally.
    - Toxic: People like you should be banned from speaking. You'll regret saying things like that.
    - Not Toxic: I'm not sure I fully understand your position. Could you provide more details?</p>
<h2 id="qa-metrics">Q&amp;A Metrics</h2>
<p>Question and Answering (Q&amp;A) is a subcategory of text generation tasks in which the datum is a query/question, and the prediction is an answer to that query. In this setting we can evaluate the predicted text based on properties such as relevance to the answer or the correctness of the answer. These metrics will not apply to all text generation tasks. For example, not all text generation tasks have a single correct answer.</p>
<h3 id="answer-relevance">Answer Relevance</h3>
<p>Answer relevance is the proportion of statements in the answer that are relevant to the query. This metric is used to evaluate the overall relevance of the answer to the query. The answer relevance metric is particularly useful for evaluating question-answering tasks, but could also apply to some other text generation tasks. This metric is not recommended for more open ended tasks.</p>
<p>Our implementation closely follows <a href="https://github.com/confident-ai/deepeval/tree/main/deepeval/metrics/answer_relevancy">DeepEval's implementation</a>. We use the same two step prompting strategy and the same instructions.</p>
<h2 id="rag-metrics">RAG Metrics</h2>
<p>Note that RAG is a form of Q&amp;A, so any Q&amp;A metric can also be used to evaluate RAG models. The metrics in this section however should not be used for all Q&amp;A tasks. RAG specific metrics use retrieved context, so should not be used to evaluate models that don't use context.</p>
<h3 id="context-relevance">Context Relevance</h3>
<p>Context relevance is the proportion of pieces of retrieved context that are relevant to the query. A piece of context is considered relevant to the query if any part of the context is relevant to answering the query. For example, a piece of context might be a paragraph of text, so if the answer or part of the answer to a query is contained somewhere in that paragraph, then that piece of context is considered relevant.</p>
<p>Context relevance is useful for evaluating the retrieval mechanism of a RAG model. This metric does not considered the generated answer or any groundtruth answers to the query, only the retrieved context.</p>
<p>Given the query and the list of context, an LLM is prompted to determine if each piece of context is relevant to the query. Then the score is computed as the number of relevant pieces of context divided by the total number of pieces of context.</p>
<p>Our implementation closely follows <a href="https://github.com/confident-ai/deepeval/tree/main/deepeval/metrics/context_relevancy">DeepEval's implementation</a>. The calculation is the same, however we modified the instruction for the LLM. The instruction in DeepEval contained typos and was organized in a confusing way, so we fixed the typos and reorganized the example to make the task clearer.</p>
<h3 id="faithfulness">Faithfulness</h3>
<p>Faithfulness is the proportion of claims from the predicted text that are implied by the retrieved context.</p>
<p>First, an LLM is prompted to extract a list of claims from the predicted text. Then, the LLM is prompted again with the list of claims and the list of context and is asked if each claim is implied / can be verified from the context. If the claim contradicts the context or if the claim is unrelated to the context, the LLM is instructed to indicate that the claim is not implied by the context. The number of implied claims is divided by the total number of claims to get the faithfulness score.</p>
<p>Our implementation loosely follows and combines the strategies of <a href="https://docs.confident-ai.com/docs/metrics-faithfulness">DeepEval</a> and <a href="https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html">RAGAS</a>, however it is notable that DeepEval and RAGAS's definitions of faithfulness are not equivalent. The difference is that, if a claim is unrelated to the context (is not implied by the context but also does not contradict the context), then DeepEval counts this claim positively towards the faithfulness score, however RAGAS counts this claim against the faithfulness score. Valor follows the same definition as RAGAS, as we believe that a claim that is unrelated to the context should not be counted positively towards the faithfulness score. If a predicted text makes many claims that are unrelated and unverifiable from the context, then how can we consider that text faithful to the context?</p>
<p>We follow <a href="https://github.com/confident-ai/deepeval/blob/main/deepeval/metrics/faithfulness/template.py">DeepEval's prompting strategy</a> as this strategy is closer to the other prompting strategies in Valor, however we heavily modify the instructions. Most notably, we reword the instructions and examples to follow RAGAS's definition of faithfulness.</p>
<h3 id="hallucination">Hallucination</h3>
<p>Hallucination is the proportion of pieces of context that are contradicted by the predicted text. If the predicted text does not contradict any of the retrieved context, then it should receive a hallucination score of 0. The hallucination score is computed as the number of pieces of context contradicted by the predicted text divided by the total number of pieces of context.</p>
<p>Given the list of context and the predicted text, an LLM is prompted to determine if the text agrees or contradicts with each piece of context. The LLM is instructed to only indicate contradiction if the text directly contradicts the context, and otherwise indicates agreement.</p>
<p>Our implementation closely follows <a href="https://github.com/confident-ai/deepeval/tree/main/deepeval/metrics/hallucination">DeepEval's implementation</a>. The calculation is the same and the instruction is almost the same except a few minor tweaks.</p>
<h2 id="text-comparison-metrics">Text Comparison Metrics</h2>
<p>This section contains non-llm guided metrics for comparing a predicted text to one or more groundtruth texts.</p>
<h3 id="rouge">ROUGE</h3>
<p>ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.</p>
<p>In Valor, the ROUGE output value is a dictionary containing the following elements:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a href="#__codelineno-4-1" id="__codelineno-4-1" name="__codelineno-4-1"></a><span class="p">{</span>
</span><span id="__span-4-2"><a href="#__codelineno-4-2" id="__codelineno-4-2" name="__codelineno-4-2"></a>    <span class="s2">"rouge1"</span><span class="p">:</span> <span class="mf">0.18</span><span class="p">,</span> <span class="c1"># unigram-based similarity scoring</span>
</span><span id="__span-4-3"><a href="#__codelineno-4-3" id="__codelineno-4-3" name="__codelineno-4-3"></a>    <span class="s2">"rouge2"</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">,</span> <span class="c1"># bigram-based similarity scoring</span>
</span><span id="__span-4-4"><a href="#__codelineno-4-4" id="__codelineno-4-4" name="__codelineno-4-4"></a>    <span class="s2">"rougeL"</span><span class="p">:</span> <span class="mf">0.18</span><span class="p">,</span> <span class="c1"># similarity scoring based on sentences (i.e., splitting on "." and ignoring "\n")</span>
</span><span id="__span-4-5"><a href="#__codelineno-4-5" id="__codelineno-4-5" name="__codelineno-4-5"></a>    <span class="s2">"rougeLsum"</span><span class="p">:</span> <span class="mf">0.18</span><span class="p">,</span> <span class="c1"># similarity scoring based on splitting the text using "\n"</span>
</span><span id="__span-4-6"><a href="#__codelineno-4-6" id="__codelineno-4-6" name="__codelineno-4-6"></a><span class="p">}</span>
</span></code></pre></div>
<p>Behind the scenes, we use <a href="https://huggingface.co/spaces/evaluate-metric/rouge">Hugging Face's <code>evaluate</code> package</a> to calculate these scores. Users can pass <code>rouge_types</code> and <code>use_stemmer</code> to EvaluationParameters in order to gain access to additional functionality from this package.</p>
<h3 id="bleu">BLEU</h3>
<p>BLEU (bilingual evaluation understudy) is an algorithm for evaluating automatic summarization and machine translation software in natural language processing. BLEU's output is always a number between 0 and 1, where a score near 1 indicates that the hypothesis text is very similar to one or more of the reference texts.</p>
<p>Behind the scenes, we use <a href="https://www.nltk.org/_modules/nltk/translate/bleu_score.html">nltk.translate.bleu_score</a> to calculate these scores. The default BLEU metric calculates a score for up to 4-grams using uniform weights (i.e., <code>weights=[.25, .25, .25, .25]</code>; also called BLEU-4). Users can pass their own <code>bleu_weights</code> to EvaluationParameters in order to change this default behavior and calculate other BLEU scores.</p>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "..", "features": ["navigation.expand"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body>
</html>