{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/vxjvkhy543l66mrkrtfrb56c0000gn/T/ipykernel_60142/1741203415.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/opt/homebrew/anaconda3/envs/velour_api_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to host at http://0.0.0.0:8000/\n"
     ]
    }
   ],
   "source": [
    "# data from https://github.com/alfredodeza/learn-retrieval-augmented-generation/tree/main\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from valor.enums import TaskType, EvaluationStatus\n",
    "from valor import Annotation, Datum, Dataset, Model, GroundTruth, Label, Client, Prediction, viz, connect\n",
    "\n",
    "NUMBER_OF_RECORDS = 50\n",
    "\n",
    "\n",
    "# get data\n",
    "df = pd.read_csv('./top_rated_wines.csv')\n",
    "df = df[df['variety'].notna()].sample(NUMBER_OF_RECORDS) # remove any NaN values as it blows up serialization\n",
    "len(df)\n",
    "\n",
    "# connet to Valor API\n",
    "connect(\"http://0.0.0.0:8000\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case #1: Evaluating Rankings via Relevant Doc Names\n",
    "\n",
    "If we know in advance which docs are relevant to our request, then it's easy for us to calculate our various metrics. We just have to pass the relevant docs in our `Groundtruth` object, pass the ordered predictions in our `Prediction` object, and run `evaluate_ranking` to get our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chateau Margaux 2015',\n",
       " 'Kistler Vineyards Stone Flat Vineyard Chardonnay 2005',\n",
       " 'Chateau Smith Haut Lafitte (1.5 Liter Futures Pre-Sale) 2019']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick three wines at random to be our \"relevant docs\" for this example\n",
    "relevant_wines = df[:10].loc[:, 'name'].sample(3).to_list()\n",
    "relevant_wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'MRRMetric',\n",
       "  'parameters': {'label_key': 'wine_recommender'},\n",
       "  'value': 0.25},\n",
       " {'type': 'PrecisionAtKMetric',\n",
       "  'parameters': {'k': 3, 'annotation_id': 2},\n",
       "  'value': 0.0,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'first_recommendation'}},\n",
       " {'type': 'RecallAtKMetric',\n",
       "  'parameters': {'k': 3, 'annotation_id': 2},\n",
       "  'value': 0.0,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'first_recommendation'}},\n",
       " {'type': 'APAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [3], 'annotation_id': 2},\n",
       "  'value': 0.0,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'first_recommendation'}},\n",
       " {'type': 'ARAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [3], 'annotation_id': 2},\n",
       "  'value': 0.0,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'first_recommendation'}},\n",
       " {'type': 'mAPAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [3], 'label_key': 'wine_recommender'},\n",
       "  'value': 0.0},\n",
       " {'type': 'mARAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [3], 'label_key': 'wine_recommender'},\n",
       "  'value': 0.0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.create('relevant_wines_dataset')\n",
    "model = Model.create('relevant_wines_model')\n",
    "\n",
    "dataset.add_groundtruth(\n",
    "    GroundTruth(\n",
    "        datum=Datum(uid=\"wines\"),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.RANKING,\n",
    "                labels=[Label(key=\"wine_recommender\", value='first_recommendation')],\n",
    "                ranking=relevant_wines\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "dataset.finalize()\n",
    "\n",
    "# assume that the other predictions were delivered from a recommender system in order\n",
    "model.add_prediction(\n",
    "    dataset, \n",
    "    Prediction(\n",
    "        datum=Datum(uid=\"wines\"),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.RANKING,\n",
    "                labels=[Label(key=\"wine_recommender\", value='first_recommendation')],\n",
    "                ranking=df[:10].loc[:, 'name'].to_list()\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_ranking(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"MRRMetric\", \"PrecisionAtKMetric\", 'RecallAtKMetric', 'APAtKMetric', 'ARAtKMetric', 'mAPAtKMetric', 'mARAtKMetric'],\n",
    "    k_cutoffs=[3],\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, say that we don't know all of the docs which are relevant to our request, but we do know at least two of them are. We can use embeddings to identify other relevant docs, then pass all of those relevant docs into the `ranking` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_embeddings(relevant_embeddings, other_embeddings, similarity_cutoff=.95):\n",
    "    \"\"\"Find all embeddings in a list of other_embeddings that are similar to some set of known relevant_embeddings.\"\"\"\n",
    "    output = []\n",
    "\n",
    "    for embedding in other_embeddings:\n",
    "        intermediate_distances = []\n",
    "        for relevant_embedding in relevant_embeddings:\n",
    "            distance = util.cos_sim(embedding, relevant_embedding)\n",
    "            intermediate_distances.append(distance)\n",
    "        \n",
    "        output.append(max(intermediate_distances).item())\n",
    "    return [i for i, distance in enumerate(output) if distance >= similarity_cutoff]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/velour_api_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>region</th>\n",
       "      <th>variety</th>\n",
       "      <th>rating</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Chateau d'Yquem Sauternes (375ML half-bottle) ...</td>\n",
       "      <td>Sauternes, Bordeaux, France</td>\n",
       "      <td>Collectible</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Discovering Chateau d'Yquem starts with the bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Inglenook Rubicon 2002</td>\n",
       "      <td>Napa Valley, California</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>96.0</td>\n",
       "      <td>\"This is the best Rubicon ever...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Domaine Saint Prefert Chateauneuf-du-Pape Coll...</td>\n",
       "      <td>Chateauneuf-du-Pape, Rhone, France</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>98.0</td>\n",
       "      <td>The tete de cuvee of the domaine, made from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Bouchard Pere &amp; Fils Chambertin Clos de Beze G...</td>\n",
       "      <td>Burgundy, France</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "439   Chateau d'Yquem Sauternes (375ML half-bottle) ...   \n",
       "1173                             Inglenook Rubicon 2002   \n",
       "916   Domaine Saint Prefert Chateauneuf-du-Pape Coll...   \n",
       "217   Bouchard Pere & Fils Chambertin Clos de Beze G...   \n",
       "\n",
       "                                  region      variety  rating  \\\n",
       "439          Sauternes, Bordeaux, France  Collectible    97.0   \n",
       "1173             Napa Valley, California     Red Wine    96.0   \n",
       "916   Chateauneuf-du-Pape, Rhone, France     Red Wine    98.0   \n",
       "217                     Burgundy, France     Red Wine    96.0   \n",
       "\n",
       "                                                  notes  \n",
       "439   Discovering Chateau d'Yquem starts with the bo...  \n",
       "1173                 \"This is the best Rubicon ever...\"  \n",
       "916   The tete de cuvee of the domaine, made from th...  \n",
       "217                                                 95   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# say we know for certain that items 5:9 are relevant to our query, but we we want to expand our search to other relevant docs using embedding distances\n",
    "relevant_docs = df['notes'][5:9].tolist()\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "relevant_doc_embeddings = [encoder.encode(doc)for doc in relevant_docs]\n",
    "other_embeddings = [encoder.encode(doc) for doc in df['notes']]\n",
    "\n",
    "similar_embeddings = find_similar_embeddings(relevant_embeddings=relevant_doc_embeddings, other_embeddings=other_embeddings)\n",
    "df.iloc[similar_embeddings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'MRRMetric',\n",
       "  'parameters': {'label_key': 'wine_recommender'},\n",
       "  'value': 0.16666666666666666},\n",
       " {'type': 'PrecisionAtKMetric',\n",
       "  'parameters': {'k': 10, 'annotation_id': 4},\n",
       "  'value': 0.4,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'second_recommendation'}},\n",
       " {'type': 'RecallAtKMetric',\n",
       "  'parameters': {'k': 10, 'annotation_id': 4},\n",
       "  'value': 1.0,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'second_recommendation'}},\n",
       " {'type': 'APAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [10], 'annotation_id': 4},\n",
       "  'value': 0.4,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'second_recommendation'}},\n",
       " {'type': 'ARAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [10], 'annotation_id': 4},\n",
       "  'value': 1.0,\n",
       "  'label': {'key': 'wine_recommender', 'value': 'second_recommendation'}},\n",
       " {'type': 'mAPAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [10], 'label_key': 'wine_recommender'},\n",
       "  'value': 0.4},\n",
       " {'type': 'mARAtKMetric',\n",
       "  'parameters': {'k_cutoffs': [10], 'label_key': 'wine_recommender'},\n",
       "  'value': 1.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.create('relevant_notes_dataset')\n",
    "model = Model.create('relevant_notes_model')\n",
    "\n",
    "dataset.add_groundtruth(\n",
    "    GroundTruth(\n",
    "        datum=Datum(uid=\"wines\"),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.RANKING,\n",
    "                labels=[Label(key=\"wine_recommender\", value='second_recommendation')],\n",
    "                ranking=df.iloc[similar_embeddings]['notes'].to_list()\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "dataset.finalize()\n",
    "\n",
    "# assume that the other predictions were delivered from a recommender system in order\n",
    "model.add_prediction(\n",
    "    dataset, \n",
    "    Prediction(\n",
    "        datum=Datum(uid=\"wines\"),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.RANKING,\n",
    "                labels=[Label(key=\"wine_recommender\", value='second_recommendation')],\n",
    "                ranking=df.loc[:, 'notes'].to_list()\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_ranking(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"MRRMetric\", \"PrecisionAtKMetric\", 'RecallAtKMetric', 'APAtKMetric', 'ARAtKMetric', 'mAPAtKMetric', 'mARAtKMetric'],\n",
    "    k_cutoffs=[10],\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case #2: Evaluating Rankings via Embeddings\n",
    "NOTE: The code below doesn't run yet as the `embedding` attribute of `Annotation` needs work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create groundtruths and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create groundtruths using documents that we know are relevant to the question \"Where is Capella, and why is it a great region for wines?\"\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "relevant_docs = df['notes'][1:3].tolist()\n",
    "df.drop(df.index[1:3])\n",
    "\n",
    "dataset = Dataset.create(DATASET_NAME)\n",
    "model = Model.create(MODEL_NAME)\n",
    "\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    dataset.add_groundtruth(\n",
    "        GroundTruth(\n",
    "            datum=Datum(uid=\"wines\"),\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.RANKING,\n",
    "                    labels=[Label(key=\"docs related to Capella\", value=f'doc #{i}')],\n",
    "                    metadata={'content': doc},\n",
    "                    embedding=encoder.encode(doc).tolist() # TODO: embedding can't handle nested lists at the moment\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "# create predictions for all of our other records\n",
    "embeddings = [encoder.encode(doc) for doc in df.loc[:, 'notes']] # output is NUMBER_OF_RECORDS x 384 dimensions per record\n",
    "\n",
    "# add the other docs as predictions\n",
    "for i, doc in enumerate(df):\n",
    "    model.add_prediction(\n",
    "        dataset,\n",
    "        Prediction(\n",
    "            datum=Datum(uid=\"wines\"),\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.RANKING,\n",
    "                    labels=[Label(key=\"docs related to Capella\", value=f'doc #{i}')],\n",
    "                    metadata={'content': doc},\n",
    "                    embedding=embeddings[i]\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "model.finalize_inferences(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_job = model.evaluate_ranking(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"MRRMetric\", \"PrecisionAtKMetric\", 'RecallAtKMetric', 'APAtKMetric', 'ARAtKMetric', 'mAPAtKMetric', 'mARAtKMetric'],\n",
    "    k_cutoffs=[3],\n",
    "    similarity_cutoff=.95 # vectors have to be 95% similar to the groundtruth vectors to be considered \"relevant\"\n",
    ")\n",
    "\n",
    "# behind the scenes, Valor should:\n",
    "# - calculate the distance between each prediction and both groundtruths (taking the average of both distances)\n",
    "# - figure out which predictions were \"relevant\" based on the cutoff\n",
    "# - calculate the IR metrics (NOTE: assumes that the annotations are added in the order in which they were recommended)\n",
    "\n",
    "\n",
    "# alternatives\n",
    "# - the user passes a nested array of embeddings to `ranking` (note: this would be a pretty large array to store in Valor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velour_api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
