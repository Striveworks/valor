{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to host at http://localhost:8000/\n"
     ]
    }
   ],
   "source": [
    "from valor import (\n",
    "    Dataset, \n",
    "    Model, \n",
    "    Datum,\n",
    "    GroundTruth, \n",
    "    Prediction, \n",
    "    Annotation, \n",
    "    Label, \n",
    "    Client,\n",
    "    connect,\n",
    ")\n",
    "from valor.schemas import Embedding\n",
    "from valor.enums import EvaluationStatus, MetricType\n",
    "\n",
    "connect(\"http://localhost:8000\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czaloom/valor/.env-velour/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "tiny_imagenet = load_dataset('zh-plus/tiny-imagenet', split='train')\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "dinov2 = AutoModel.from_pretrained('facebook/dinov2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(x) -> Embedding:\n",
    "    inputs = processor(images=x, return_tensors=\"pt\")\n",
    "    outputs = dinov2(**inputs)\n",
    "    retval = (\n",
    "        outputs\n",
    "        .last_hidden_state\n",
    "        .flatten()\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "    retval = retval.tolist()\n",
    "    return Embedding(retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"fish\",\n",
    "    1: \"ground_spider\",\n",
    "    2: \"frog\",\n",
    "    5: \"snake\",\n",
    "    8: \"web_spider\",\n",
    "    19: \"penguin\"\n",
    "}\n",
    "label_ids = classes.keys()\n",
    "text_labels = list(classes.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ground truths and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create(\"tiny-imagenet\")\n",
    "model = Model.create(\"facebook/dinov2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientException",
     "evalue": "{\"name\": \"OperationalError\", \"detail\": \"(psycopg2.errors.ProgramLimitExceeded) vector cannot have more than 16000 dimensions\\nLINE 1: INSERT INTO embedding (value, created_at) VALUES ('[-2.21651...\\n                                                          ^\\n\\n[SQL: INSERT INTO embedding (value, created_at) VALUES (%(value)s, now()) RETURNING embedding.id, embedding.created_at]\\n[parameters: {'value': '[-2.216519832611084,-0.4405917823314667,-0.09975497424602509,-0.6448603272438049,-0.19376243650913239,0.6705418825149536,1.8325440883636475,-1.867935 ... (1902918 characters truncated) ... 888239860535,-1.5801950693130493,2.1737005710601807,-1.536237359046936,-5.610318183898926,1.4012024402618408,-1.9985246658325195,-3.3968770503997803]'}]\\n(Background on this error at: https://sqlalche.me/e/20/e3q8)\", \"timestamp\": 1719450608.231918}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     10\u001b[0m datum \u001b[39m=\u001b[39m Datum(uid\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(idx))\n\u001b[1;32m     11\u001b[0m dataset\u001b[39m.\u001b[39madd_groundtruth(\n\u001b[1;32m     12\u001b[0m     GroundTruth(\n\u001b[1;32m     13\u001b[0m         datum\u001b[39m=\u001b[39mdatum,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m model\u001b[39m.\u001b[39;49madd_prediction(\n\u001b[1;32m     27\u001b[0m     dataset,\n\u001b[1;32m     28\u001b[0m     Prediction(\n\u001b[1;32m     29\u001b[0m         datum\u001b[39m=\u001b[39;49mdatum,\n\u001b[1;32m     30\u001b[0m         annotations\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     31\u001b[0m             Annotation(\n\u001b[1;32m     32\u001b[0m                 embedding\u001b[39m=\u001b[39;49membed(pil_image)\n\u001b[1;32m     33\u001b[0m             )\n\u001b[1;32m     34\u001b[0m         ]\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/coretypes.py:723\u001b[0m, in \u001b[0;36mModel.add_prediction\u001b[0;34m(self, dataset, prediction)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_prediction\u001b[39m(\n\u001b[1;32m    709\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    710\u001b[0m     dataset: Dataset,\n\u001b[1;32m    711\u001b[0m     prediction: Prediction,\n\u001b[1;32m    712\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m    Add a prediction to the model.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39m        The prediction to create.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     Client(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconn)\u001b[39m.\u001b[39;49mcreate_predictions(\n\u001b[1;32m    724\u001b[0m         dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m    725\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    726\u001b[0m         predictions\u001b[39m=\u001b[39;49m[prediction],\n\u001b[1;32m    727\u001b[0m     )\n",
      "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/coretypes.py:1552\u001b[0m, in \u001b[0;36mClient.create_predictions\u001b[0;34m(self, dataset, model, predictions)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     prediction_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mname\n\u001b[1;32m   1551\u001b[0m     predictions_json\u001b[39m.\u001b[39mappend(prediction_dict)\n\u001b[0;32m-> 1552\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconn\u001b[39m.\u001b[39;49mcreate_predictions(predictions_json)\n",
      "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/client.py:368\u001b[0m, in \u001b[0;36mClientConnection.create_predictions\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_predictions\u001b[39m(\u001b[39mself\u001b[39m, predictions: List[\u001b[39mdict\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m    Creates predictions.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m        The predictions to be created.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_post_rel_host(\u001b[39m\"\u001b[39;49m\u001b[39mpredictions\u001b[39;49m\u001b[39m\"\u001b[39;49m, json\u001b[39m=\u001b[39;49mpredictions)\n",
      "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/client.py:280\u001b[0m, in \u001b[0;36mClientConnection._requests_post_rel_host\u001b[0;34m(self, endpoint, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_requests_post_rel_host\u001b[39m(\u001b[39mself\u001b[39m, endpoint: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m    Helper for handling POST requests.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_wrapper(\n\u001b[1;32m    281\u001b[0m         method_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, endpoint\u001b[39m=\u001b[39;49mendpoint, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    282\u001b[0m     )\n",
      "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/client.py:269\u001b[0m, in \u001b[0;36mClientConnection._requests_wrapper\u001b[0;34m(self, method_name, endpoint, ignore_auth, max_retries_on_timeout, initial_timeout, exponential_backoff, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_access_token_from_username_and_password()\n\u001b[1;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m         raise_client_exception(resp)\n\u001b[1;32m    270\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/exceptions.py:176\u001b[0m, in \u001b[0;36mraise_client_exception\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mglobals\u001b[39m()[cls_name](resp)\n\u001b[1;32m    175\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         \u001b[39mraise\u001b[39;00m ClientException(resp)\n\u001b[1;32m    177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, json\u001b[39m.\u001b[39mJSONDecodeError):\n\u001b[1;32m    178\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(resp)\n",
      "\u001b[0;31mClientException\u001b[0m: {\"name\": \"OperationalError\", \"detail\": \"(psycopg2.errors.ProgramLimitExceeded) vector cannot have more than 16000 dimensions\\nLINE 1: INSERT INTO embedding (value, created_at) VALUES ('[-2.21651...\\n                                                          ^\\n\\n[SQL: INSERT INTO embedding (value, created_at) VALUES (%(value)s, now()) RETURNING embedding.id, embedding.created_at]\\n[parameters: {'value': '[-2.216519832611084,-0.4405917823314667,-0.09975497424602509,-0.6448603272438049,-0.19376243650913239,0.6705418825149536,1.8325440883636475,-1.867935 ... (1902918 characters truncated) ... 888239860535,-1.5801950693130493,2.1737005710601807,-1.536237359046936,-5.610318183898926,1.4012024402618408,-1.9985246658325195,-3.3968770503997803]'}]\\n(Background on this error at: https://sqlalche.me/e/20/e3q8)\", \"timestamp\": 1719450608.231918}"
     ]
    }
   ],
   "source": [
    "groundtruths = []\n",
    "predictions = []\n",
    "\n",
    "for idx, image in enumerate(tiny_imagenet):\n",
    "    label_id = image['label']\n",
    "    pil_image = image['image']\n",
    "\n",
    "    # ignore other labels\n",
    "    if label_id in label_ids:\n",
    "        datum = Datum(uid=str(idx))\n",
    "        dataset.add_groundtruth(\n",
    "            GroundTruth(\n",
    "                datum=datum,\n",
    "                annotations=[\n",
    "                    Annotation(\n",
    "                        labels=[\n",
    "                            Label(\n",
    "                                key=\"class\", \n",
    "                                value=classes[label_id]\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        model.add_prediction(\n",
    "            dataset,\n",
    "            Prediction(\n",
    "                datum=datum,\n",
    "                annotations=[\n",
    "                    Annotation(\n",
    "                        embedding=embed(pil_image)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.finalize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = model.evaluate_embeddings(dataset, limit=10)\n",
    "assert job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "cvm = job.metrics[0] if job.metrics[0]['type'] == MetricType.CramerVonMises else job.metrics[1]\n",
    "ks = job.metrics[0] if job.metrics[0]['type'] == MetricType.KolmgorovSmirnov else job.metrics[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(x: dict, labels: list[str]):\n",
    "    col_ix = pd.MultiIndex.from_product([[\"Reference\"], labels])\n",
    "    row_ix = pd.MultiIndex.from_product([[\"Query\"], labels])\n",
    "\n",
    "    statistic_df = pd.DataFrame(x[\"value\"][\"statistics\"])\n",
    "    statistic_df = statistic_df.set_index(row_ix)\n",
    "    statistic_df.columns = col_ix\n",
    "\n",
    "    pvalue_df = pd.DataFrame(x[\"value\"][\"pvalues\"])\n",
    "    pvalue_df = pvalue_df.set_index(row_ix)\n",
    "    pvalue_df.columns = col_ix\n",
    "\n",
    "    return (statistic_df, pvalue_df)\n",
    "\n",
    "cvm_statistics, cvm_pvalues = create_dataframe(cvm, text_labels)\n",
    "ks_statistics, ks_pvalues = create_dataframe(ks, text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_pvalues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-velour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
