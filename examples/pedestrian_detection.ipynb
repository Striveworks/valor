{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979f2844-e485-42a3-a93a-02f188a5438e",
   "metadata": {},
   "source": [
    "# Pedestrian Detection Example\n",
    "\n",
    "This notebook provides an end-to-end example of evaluating and comparing object detection models. It demonstrates velour's ability to use business logic to define metadata, by which metrics can stratified/filtered.\n",
    "\n",
    "\n",
    "We will work through a self-driving car example, where we evaluate model performance on detecting persons in and out of the road. For the dataset we will use Berkeley Deep Drive (https://bdd-data.berkeley.edu/) and we will evaluate against some pretrained torchvision detection models (https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n",
    "\n",
    "*Note: This notebook assumes that the velour service is running on http://localhost:8000. See https://striveworks.github.io/velour/getting_started/ for how set up the velour service. To connect to a different instance of Velour, change the line `client = Client(\"http://localhost:8000\")` in cell 5 accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ef6f8-64d8-4afb-b924-882f8801d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from velour.enums import TaskType\n",
    "from velour import Annotation, Datum, Dataset, Model, GroundTruth, Label, Client, Prediction, viz\n",
    "from velour.schemas import BoundingBox, Point, Raster, BasicPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffebae7-22f6-4805-8094-70668ef419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the BDD files are in these relative locations (or change the paths below)\n",
    "imgs_path = Path(\"bdd100k/images/10k/val/\")\n",
    "masks_path = Path(\"bdd100k/labels/pan_seg/bitmasks/val/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1d71f-0228-4876-86cc-482bc1c39659",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper methods\n",
    "\n",
    "Below we define some helper functions for working with the BDD dataset. See the official BDD documentation for more info: https://doc.bdd100k.com/format.html#segmentation-formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb918f-ecdd-4e77-856a-59e67179b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(fname: str) -> Image.Image:\n",
    "    return Image.open(imgs_path / fname)\n",
    "\n",
    "\n",
    "def load_mask_array(fname: str) -> np.ndarray:\n",
    "    \"\"\" Takes in the filename of an image and returns the mask array.\n",
    "    this is an integer array of shape [H, W, 4] that encodes the classes and instance\n",
    "    ids \n",
    "    \"\"\"\n",
    "    return np.array(Image.open(masks_path / (fname[:-3] + \"png\")))\n",
    "\n",
    "\n",
    "def get_road_binary_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns a binary array of shape [H, W] of which pixels correspond to roads\n",
    "    \"\"\"\n",
    "    # the first channel of the mask is the category, which is 7 for roads\n",
    "    return mask[:, :, 0] == 7\n",
    "\n",
    "\n",
    "def get_person_instances_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns an integer array of shape [H, W] where the non-zero values are pixels\n",
    "    corresponding to people and the value is the instance id (unique to each different person\n",
    "    in the image)\n",
    "    \"\"\"\n",
    "    # category int for person is 31, and the last channel corresponds to instance id\n",
    "    return (mask[:, :, 0] == 31) * mask[:, :, 3]\n",
    "\n",
    "\n",
    "def person_is_in_road(person_bbox: BoundingBox, road_binary_mask: np.ndarray) -> bool:\n",
    "    \"\"\" Determines if a person is in a road or not by checking if there are road\n",
    "    pixels immediately to the left and immediately to the right of the bottom of the\n",
    "    bounding box\n",
    "    \"\"\"\n",
    "    # determine if person and road intersect sufficiently much\n",
    "    road_on_left, road_on_right = False, False\n",
    "    btm_y = min(int(person_bbox.ymax), road_binary_mask.shape[0] - 1)\n",
    "    for offset in range(1, 5):\n",
    "        if road_binary_mask[btm_y, max(int(person_bbox.xmin) - offset, 0)]:\n",
    "            road_on_left = True\n",
    "        if road_binary_mask[btm_y, min(int(person_bbox.xmax) + offset, road_binary_mask.shape[1] - 1)]:\n",
    "            road_on_right = True\n",
    "\n",
    "    return road_on_left and road_on_right\n",
    "\n",
    "\n",
    "def get_person_bboxes(person_instances_mask: np.ndarray) -> list[BoundingBox]:\n",
    "    \"\"\" From the integer mask of person instance ids, return a list of\n",
    "    `Box` objects, representing the bounding boxes for each person.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for instance_id in np.unique(person_instances_mask):\n",
    "        if instance_id == 0:\n",
    "            continue\n",
    "        person_mask = person_instances_mask == instance_id\n",
    "        ys, xs = np.where(person_mask)\n",
    "        ymin, xmin, ymax, xmax = min(ys), min(xs), max(ys), max(xs)\n",
    "\n",
    "        if ymin != ymax and xmin != xmax:\n",
    "            boxes.append(BoundingBox.from_extrema(xmin=xmin, ymin=ymin, xmax=xmax, ymax=ymax))\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8112463-234a-43d8-b4e1-31a2cd7f9a15",
   "metadata": {},
   "source": [
    "## Example image\n",
    "\n",
    "Here we show an example image from the dataset, and draw bounding boxes around each person. We color the box yellow if the person is in the street (according to our definition) and green if it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3881d6-dce6-4fda-85c2-4806f3effc34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = \"9b970e47-51dde695.jpg\"\n",
    "\n",
    "img = load_image(fname)\n",
    "mask = load_mask_array(fname)\n",
    "\n",
    "road_binary_mask = get_road_binary_mask(mask)\n",
    "person_instances_mask = get_person_instances_mask(mask)\n",
    "\n",
    "person_bboxes = get_person_bboxes(person_instances_mask) \n",
    "for person_bbox in person_bboxes:\n",
    "    if person_is_in_road(person_bbox, road_binary_mask):\n",
    "        color = (255, 255, 0)\n",
    "    else:\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "    # img = person_bbox.draw_on_image(img, color=color)\n",
    "    img = viz.draw_bounding_box_on_image(person_bbox, img=img, color=color)\n",
    "\n",
    "road_raster = Raster.from_numpy(road_binary_mask)\n",
    "img = viz.draw_raster_on_image(road_raster, img)\n",
    "\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809ea20-ebac-403a-b3ce-e34b2b6b77fa",
   "metadata": {},
   "source": [
    "## Create the velour dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5b2b4-d05c-491f-868f-83e91918d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253d448-ee34-487c-b813-0be3549d4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset(client, name=\"bdd10k-people-in-roads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c37b8-dddd-41ea-9711-3dc6f1527466",
   "metadata": {},
   "source": [
    "Now we itereate through the images in the dataset and create groundtruth. We store the information of whether or not a bounding box is in the road as *metadata*, in the `\"in_road\"`. This demonstrates velour's ability to incorporate custom defined business logic to evaluate against (the besoke definition of `\"in_road\"`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da14c03-8959-4a1f-931b-506b4b274ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fname in tqdm(os.listdir(imgs_path)):\n",
    "    datum = Datum(uid=fname)\n",
    "    \n",
    "    mask = load_mask_array(fname)\n",
    "    \n",
    "    road_binary_mask = get_road_binary_mask(mask)\n",
    "    person_instances_mask = get_person_instances_mask(mask)\n",
    "    \n",
    "    person_bboxes = get_person_bboxes(person_instances_mask)\n",
    "    \n",
    "    if len(person_bboxes) == 0:\n",
    "        continue\n",
    "    \n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.DETECTION,\n",
    "            bounding_box=person_bbox,\n",
    "            labels=[Label(key=\"class\", value=\"person\")],\n",
    "            metadata={\"in_road\": person_is_in_road(person_bbox, road_binary_mask)}\n",
    "        )\n",
    "        for person_bbox in person_bboxes\n",
    "    ]\n",
    "    \n",
    "    gt = GroundTruth(datum=datum, annotations=annotations)\n",
    "    dset.add_groundtruth(gt)\n",
    "\n",
    "dset.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aaa802-4e00-4ad4-b22f-296fa3a545e1",
   "metadata": {},
   "source": [
    "Get a quick summary of the number of images, bounding boxes, and metadata uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f3772-b48a-4bbb-95d6-d235d9d1d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c2961-39da-46a3-b8c7-07395a3caa29",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4835fc8-0d1e-4121-b606-f0eecf114f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision.models.detection import (\n",
    "    retinanet_resnet50_fpn_v2,\n",
    "    RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn,\n",
    "    FCOS_ResNet50_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e4a8e-4312-4a61-8dc2-043722678e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to GPU if available, otherwise cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "retinanet = retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1).to(device).eval()\n",
    "fcos = fcos_resnet50_fpn(weights=FCOS_ResNet50_FPN_Weights.COCO_V1).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f58528-b867-4fa9-9cb8-c8792d7614bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def detect_people_on_image(net: torch.nn.Module, fname: str, score_thres: float=0.2) -> tuple[list[BoundingBox], list[float]]:\n",
    "    \"\"\" Method to run inference on an image in the BDD dataset, and return output in velour's bounding box format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net\n",
    "        the torchvision detection model to use\n",
    "    name\n",
    "        filename of image to run inference on\n",
    "    score_thres\n",
    "        score threshold to determine if a detection is kept\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        first element of the tuple is a list of all predicted bounding boxes of detected people, and the\n",
    "        second element is a list of the confidence scores of the detections.\n",
    "    \"\"\"\n",
    "    img = load_image(fname)\n",
    "    img_tensor = to_tensor(img).unsqueeze(0).to(device)\n",
    "    out = net(img_tensor)[0]\n",
    "    \n",
    "    score_mask = out[\"scores\"] > score_thres\n",
    "    # filter out only person detections (those have label 1)\n",
    "    label_mask = out[\"labels\"] == 1\n",
    "    \n",
    "    bounding_boxes = [\n",
    "        BoundingBox.from_extrema(xmin=box[0], ymin=box[1], xmax=box[2], ymax=box[3])\n",
    "        for box in out[\"boxes\"][score_mask & label_mask].tolist()\n",
    "    ]\n",
    "    scores = out[\"scores\"][score_mask & label_mask].tolist()\n",
    "    \n",
    "    return bounding_boxes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fb306-f06a-4716-9d43-2afa03ff4f6c",
   "metadata": {},
   "source": [
    "## Example inference\n",
    "\n",
    "Here we show an example inference, with the bounding box detections drawn on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94bdc1-2702-4d8d-8b48-3b8b1b381a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"9b970e47-51dde695.jpg\"\n",
    "person_bboxes, _ = detect_people_on_image(retinanet, fname)\n",
    "\n",
    "img = load_image(fname)\n",
    "for person_bbox in person_bboxes:\n",
    "    img = viz.draw_bounding_box_on_image(person_bbox, img)\n",
    "\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbbd0fc-62ad-4992-961e-54f67e6f0536",
   "metadata": {},
   "source": [
    "## Create the velour models, add predictions, and evaluate\n",
    "\n",
    "Below we define methods for adding inferences to velour and then evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932ad12-3ffd-42a3-a63c-f282ac69bc1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_predictions(velour_model: Model, torch_model: torch.nn.Module) -> None:\n",
    "    \"\"\" This method computes inferences of a torch model on the BDD dataset and then uploads the results to velour. This code\n",
    "    is very similar to the main block of the `Create the velour dataset` section\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    velour_model\n",
    "        the velour model object to associate the inferences with\n",
    "    torch_model\n",
    "        the torchvision detection network\n",
    "    \"\"\"\n",
    "    for datum in tqdm(dset.get_datums()):\n",
    "        fname = datum.uid\n",
    "\n",
    "        mask = load_mask_array(fname)\n",
    "\n",
    "        road_binary_mask = get_road_binary_mask(mask)\n",
    "\n",
    "        person_bboxes, scores = detect_people_on_image(torch_model, datum.uid)\n",
    "\n",
    "        annotations = [\n",
    "            Annotation(\n",
    "                task_type=TaskType.DETECTION,\n",
    "                bounding_box=person_bbox,\n",
    "                labels=[Label(key=\"class\", value=\"person\", score=score)],\n",
    "                metadata={\"in_road\": person_is_in_road(person_bbox, road_binary_mask)}\n",
    "            )\n",
    "            for person_bbox, score in zip(person_bboxes, scores)\n",
    "        ]\n",
    "\n",
    "        pred = Prediction(datum=datum, annotations=annotations)\n",
    "\n",
    "        velour_model.add_prediction(dset, pred)\n",
    "    velour_model.finalize_inferences(dset)\n",
    "\n",
    "\n",
    "def evaluate(velour_model: Model) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" Carries out the evaluation of the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    velour_model\n",
    "        the velour model object to associate the inferences with\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        first element of the tuple is a pandas dataframe giving the metrics for the overall performance of the model. the second\n",
    "        is a pandas dataframe that gives the performance of the model just on detection people in roads\n",
    "    \"\"\"\n",
    "    # compute unfiltered evaluation\n",
    "    reg_eval = velour_model.evaluate_detection(dset)\n",
    "    reg_eval.wait_for_completion()\n",
    "    reg_eval.to_dataframe()\n",
    "    \n",
    "    # evaluate on just people for which the `\"in_road\"` metadata field is `True`\n",
    "    in_road_eval = velour_model.evaluate_detection(dset, filters=[Annotation.metadata[\"in_road\"] == True])\n",
    "    in_road_eval.wait_for_completion()\n",
    "    in_road_eval = in_road_eval.to_dataframe()\n",
    "    \n",
    "    \n",
    "    return reg_eval, in_road_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdae986-6211-41ba-86c6-d0a0253bc34a",
   "metadata": {},
   "source": [
    "Below we evaluate two different models on the dataset (`FCOS Resnet50FPN ` and `Retinanet Resnet50FPN`), both pretrained on coco. We organize the results in a dataframe to carry out model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79630281-6b7c-4882-b34e-af78b813f0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ap_ave_over_ious = {}\n",
    "\n",
    "for model_name, net in [(\"fcos_resnet50_fpn\", fcos), (\"retinanet_resnet50_fpn_v2\", retinanet)]:\n",
    "    \n",
    "    model = Model(client, model_name)\n",
    "    add_predictions(model, net)\n",
    "    \n",
    "    reg_eval, in_road_eval = evaluate(model)\n",
    "    \n",
    "    print(f\"Full evaluation reports for {model_name}\")\n",
    "    \n",
    "    print(\"\\nTotal evaluation\")\n",
    "    print(\"-----------------\")\n",
    "    print(reg_eval)\n",
    "\n",
    "    print(\"\\n\\nEvaluation on people in road\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(in_road_eval)\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    ap_ave_over_ious[model_name] = {\n",
    "        \"total\": reg_eval.loc[\"APAveragedOverIOUs\"].value[dset.name].item(),\n",
    "        \"in_road\": in_road_eval.loc[\"APAveragedOverIOUs\"].value[dset.name].item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b9039-a639-4940-b4ed-4d53bb8e35e3",
   "metadata": {},
   "source": [
    "We now focus on the aggregate metrics `APAveragedOverIOUs`. From the table below we see that score for detecting people overall is very close between the two models (with a slight advantange to `fcos_resnet50_fpn`). However, if we are more concerned with being able to detect people in roads, then the model `retinanet_resnet50_fpn_v2` is clearly the better choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36546cfa-e0ef-4427-8111-a6732f899b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ap_ave_over_ious)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-velour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
