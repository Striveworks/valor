{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0644bdbe-38da-478e-8673-802a5cb59da0",
   "metadata": {},
   "source": [
    "# Evaluating Tabular Classifications\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll walk-through a detailed example of how you can use Valor to evaluate classifications made on a tabular dataset. This example uses `sklearn`'s breast cancer dataset to make a binary prediction about whether a woman has breast cancer, based on a table of descriptive features, such as mean radius and mean texture.\n",
    "\n",
    "For a conceptual introduction to Valor, [check out our project overview](https://striveworks.github.io/valor/). For a higher-level example notebook, [check out our \"Getting Started\" notebook](https://github.com/Striveworks/valor/blob/main/examples/getting_started.ipynb).\n",
    "\n",
    "Before using this notebook, please ensure that the Valor service is running on your machine (for start-up instructions, [click here](https://striveworks.github.io/valor/getting_started/)). To connect to a non-local instance of Valor, update `client = Client(\"http://0.0.0.0:8000\")` in the first code block to point to the correct URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18d9c3",
   "metadata": {},
   "source": [
    "## Defining Our Datasets\n",
    "\n",
    "We start by fetching our dataset, dividing it into test/train splits, and uploading both sets to Valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9daebe8-0bb4-41eb-8359-9cadaa4a7779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The Valor client version (0.27.2.dev37+g6c9eaddf.d20240614) is newer than the Valor API version 0.27.2.dev37+g6c9eaddf\t==========================================================================================\n",
      "\t== Running with a mismatched client != API version may have unexpected results.\n",
      "\t== Please update your client to \u001b[1;0.27.2.dev37+g6c9eaddf\u001b[0;31m to avoid aberrant behavior.\n",
      "\t==========================================================================================\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to host at http://localhost:8000/\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from valor import connect, Client, Dataset, Model, Datum, Annotation, GroundTruth, Prediction, Label\n",
    "from valor.enums import TaskType\n",
    "\n",
    "# connect to the Valor API\n",
    "connect(\"http://localhost:8000\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c72cd1-50f7-4e85-9e25-d0ed35b1d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from sklearn\n",
    "dset = load_breast_cancer()\n",
    "dset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a8d343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), array([0, 1, 1, 1]), array(['malignant', 'benign'], dtype='<U9'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split datasets\n",
    "X, y, target_names = dset[\"data\"], dset[\"target\"], dset[\"target_names\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# show an example input\n",
    "X_train.shape, y_train[:4], target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f5836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset in Valor\n",
    "valor_train_dataset = Dataset.create(\"breast-cancer-train\")\n",
    "\n",
    "# create test dataset in Valor\n",
    "valor_test_dataset = Dataset.create(\"breast-cancer-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85ac3c",
   "metadata": {},
   "source": [
    "### Adding GroundTruths to our Dataset\n",
    "\n",
    "Now that our two datasets exists in Valor, we can add `GroundTruths` to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5311dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format training groundtruths\n",
    "training_groundtruths = [\n",
    "    GroundTruth(\n",
    "        datum=Datum(\n",
    "            uid=f\"train{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                labels=[Label(key=\"class\", value=target_names[t])]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, t in enumerate(y_train)\n",
    "]\n",
    "\n",
    "# format testing groundtruths\n",
    "testing_groundtruths = [\n",
    "    GroundTruth(\n",
    "        datum=Datum(\n",
    "            uid=f\"test{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                labels=[Label(key=\"class\", value=target_names[t])]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, t in enumerate(y_test)\n",
    "]\n",
    "\n",
    "# add the training groundtruths\n",
    "valor_train_dataset.add_groundtruths(training_groundtruths)\n",
    "\n",
    "# add the testing groundtruths\n",
    "valor_test_dataset.add_groundtruths(testing_groundtruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b70427",
   "metadata": {},
   "source": [
    "### Finalizing Our Datasets\n",
    "\n",
    "Lastly, we finalize both datasets to prep them for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e5cddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valor_train_dataset.finalize()\n",
    "valor_test_dataset.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea0e42",
   "metadata": {},
   "source": [
    "## Defining Our Model\n",
    "\n",
    "Now that our `Datasets` have been defined, we can describe our model in Valor using the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f43e61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99966986e-01, 3.30144019e-05],\n",
       "       [2.67294495e-04, 9.99732706e-01],\n",
       "       [2.33815287e-02, 9.76618471e-01],\n",
       "       [7.67578236e-05, 9.99923242e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit an sklearn model to our data\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# get predictions on both of our datasets\n",
    "y_train_probs = pipe.predict_proba(X_train)\n",
    "y_test_probs = pipe.predict_proba(X_test)\n",
    "\n",
    "# show an example output\n",
    "y_train_probs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0380f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our model in Valor\n",
    "valor_model = Model.create(\"breast-cancer-linear-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de269b",
   "metadata": {},
   "source": [
    "### Adding Predictions to Our Model\n",
    "\n",
    "With our model defined in Valor, we can post predictions for each of our `Datasets` to our `Model` object. Each `Prediction` should contain a list of `Labels` describing the prediction and its associated confidence score. Since we're running a classification task, the confidence scores over all prediction classes should sum to (approximately) 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a224345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define our predictions\n",
    "training_predictions = [\n",
    "    Prediction(\n",
    "        datum=Datum(\n",
    "            uid=f\"train{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                labels=[\n",
    "                    Label(\n",
    "                        key=\"class\", \n",
    "                        value=target_names[j],\n",
    "                        score=p,\n",
    "                    )                        \n",
    "                    for j, p in enumerate(prob)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, prob in enumerate(y_train_probs)\n",
    "]\n",
    "\n",
    "testing_predictions = [\n",
    "    Prediction(\n",
    "        datum=Datum(\n",
    "            uid=f\"test{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                labels=[\n",
    "                    Label(\n",
    "                        key=\"class\",\n",
    "                        value=target_names[j],\n",
    "                        score=p,\n",
    "                    )                        \n",
    "                    for j, p in enumerate(prob)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, prob in enumerate(y_test_probs)\n",
    "]\n",
    "\n",
    "# add the train predictions\n",
    "valor_model.add_predictions(valor_train_dataset, training_predictions)\n",
    "\n",
    "# add the test predictions\n",
    "valor_model.add_predictions(valor_test_dataset, testing_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b78f442",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "With our `Dataset` and `Model` defined, we're ready to evaluate our performance and display the results. Note that we use the `wait_for_completion` method since all evaluations run as background tasks; this method ensures that the evaluation finishes before we display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba0e545-4eaa-4f6b-8d62-f3a63018e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>evaluation</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>parameters</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>{\"label_key\": \"class\"}</th>\n",
       "      <th>n/a</th>\n",
       "      <td>0.988263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">\"n/a\"</th>\n",
       "      <th>class: benign</th>\n",
       "      <td>0.990689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class: malignant</th>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Precision</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">\"n/a\"</th>\n",
       "      <th>class: benign</th>\n",
       "      <td>0.981550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class: malignant</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCAUC</th>\n",
       "      <th>{\"label_key\": \"class\"}</th>\n",
       "      <th>n/a</th>\n",
       "      <td>0.997086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Recall</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">\"n/a\"</th>\n",
       "      <th>class: benign</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class: malignant</th>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      value\n",
       "evaluation                                                1\n",
       "type      parameters             label                     \n",
       "Accuracy  {\"label_key\": \"class\"} n/a               0.988263\n",
       "F1        \"n/a\"                  class: benign     0.990689\n",
       "                                 class: malignant  0.984127\n",
       "Precision \"n/a\"                  class: benign     0.981550\n",
       "                                 class: malignant  1.000000\n",
       "ROCAUC    {\"label_key\": \"class\"} n/a               0.997086\n",
       "Recall    \"n/a\"                  class: benign     1.000000\n",
       "                                 class: malignant  0.968750"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval_job = valor_model.evaluate_classification(valor_train_dataset)\n",
    "train_eval_job.wait_for_completion()\n",
    "train_eval_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73626229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label_key': 'class',\n",
       "  'entries': [{'prediction': 'benign', 'groundtruth': 'benign', 'count': 266},\n",
       "   {'prediction': 'benign', 'groundtruth': 'malignant', 'count': 5},\n",
       "   {'prediction': 'malignant', 'groundtruth': 'malignant', 'count': 155}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval_job.confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e447e7-0da4-49ae-af0a-8baa1446b4e7",
   "metadata": {},
   "source": [
    "As a brief sanity check, we can check Valor's outputs against `sklearn's` own classification report. We see that the two results are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347c180e-9913-4aa4-994e-de507da32d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant   1.000000  0.968750  0.984127       160\n",
      "      benign   0.981550  1.000000  0.990689       266\n",
      "\n",
      "    accuracy                       0.988263       426\n",
      "   macro avg   0.990775  0.984375  0.987408       426\n",
      "weighted avg   0.988479  0.988263  0.988224       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_preds = pipe.predict(X_train)\n",
    "print(classification_report(y_train, y_train_preds, digits=6, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-valor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
