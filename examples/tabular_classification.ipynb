{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0644bdbe-38da-478e-8673-802a5cb59da0",
   "metadata": {},
   "source": [
    "# Tabular Classification Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll walk-through a detailed example of how you can use Velour to evaluate classifications made on a tabular dataset. We'll use `sklearn's` breast cancer dataset to make a binary prediction about whether a woman has breast cancer based on a table of descriptive features (e.g., mean radius, mean texture, etc.). \n",
    "\n",
    "For a conceptual introduction to Velour, [check out our project overview](https://striveworks.github.io/velour/). For a higher-level example notebook, [check out our \"Getting Started\" notebook](https://github.com/Striveworks/velour/blob/main/examples/getting_started.ipynb).\n",
    "\n",
    "Before using this notebook, please ensure that the Velour service is running on your machine (for start-up instructions, [click here](https://striveworks.github.io/velour/getting_started/)). To connect to a non-local instance of Velour, update `client = Client(\"http://0.0.0.0:8000\")` in the first code block to point to the correct URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18d9c3",
   "metadata": {},
   "source": [
    "## Defining Our Datasets\n",
    "\n",
    "We start by fetching our dataset, dividing it into test/train splits, and uploading both sets to Velour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9daebe8-0bb4-41eb-8359-9cadaa4a7779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The Velour client isn't versioned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to host at http://localhost:8000/\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from velour import Dataset, Model, Datum, Annotation, GroundTruth, Prediction, Label\n",
    "from velour.enums import TaskType\n",
    "from velour.client import Client\n",
    "\n",
    "# connect to Velour API\n",
    "client = Client(\"http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c72cd1-50f7-4e85-9e25-d0ed35b1d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from sklearn\n",
    "dset = load_breast_cancer()\n",
    "dset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a8d343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), array([0, 1, 0, 0]), array(['malignant', 'benign'], dtype='<U9'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split datasets\n",
    "X, y, target_names = dset[\"data\"], dset[\"target\"], dset[\"target_names\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# show an example input\n",
    "X_train.shape, y_train[:4], target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f5836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset in Velour\n",
    "velour_train_dataset = Dataset(client, \"breast-cancer-train\", delete_if_exists=True)\n",
    "\n",
    "# create test dataset in Velour\n",
    "velour_test_dataset = Dataset(client, \"breast-cancer-test\", delete_if_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85ac3c",
   "metadata": {},
   "source": [
    "### Adding GroundTruths to our Dataset\n",
    "\n",
    "Now that our two datasets exists in Velour, we can add `GroundTruths` to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5311dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 426/426 [00:07<00:00, 57.55it/s]\n",
      "100%|██████████| 143/143 [00:02<00:00, 57.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# format training groundtruths\n",
    "training_groundtruths = [\n",
    "    GroundTruth(\n",
    "        datum=Datum(\n",
    "            uid=f\"train{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.CLASSIFICATION,\n",
    "                labels=[Label(key=\"class\", value=target_names[t])]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, t in enumerate(y_train)\n",
    "]\n",
    "\n",
    "# format testing groundtruths\n",
    "testing_groundtruths = [\n",
    "    GroundTruth(\n",
    "        datum=Datum(\n",
    "            uid=f\"test{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.CLASSIFICATION,\n",
    "                labels=[Label(key=\"class\", value=target_names[t])]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, t in enumerate(y_test)\n",
    "]\n",
    "\n",
    "# add the training groundtruths\n",
    "for gt in tqdm(training_groundtruths):\n",
    "    velour_train_dataset.add_groundtruth(gt)\n",
    "\n",
    "# add the testing groundtruths\n",
    "for gt in tqdm(testing_groundtruths):\n",
    "    velour_test_dataset.add_groundtruth(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b70427",
   "metadata": {},
   "source": [
    "### Finalizing Our Datasets\n",
    "\n",
    "Lastly, we finalize both datasets to prep them for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e5cddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velour_train_dataset.finalize()\n",
    "velour_test_dataset.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea0e42",
   "metadata": {},
   "source": [
    "## Defining Our Model\n",
    "\n",
    "Now that our `Datasets` have been defined, we can describe our model in Velour using the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f43e61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 5.42543127e-12],\n",
       "       [3.10366665e-04, 9.99689633e-01],\n",
       "       [7.88809064e-01, 2.11190936e-01],\n",
       "       [9.99195106e-01, 8.04893945e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit an sklearn model to our data\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# get predictions on both of our datasets\n",
    "y_train_probs = pipe.predict_proba(X_train)\n",
    "y_test_probs = pipe.predict_proba(X_test)\n",
    "\n",
    "# show an example output\n",
    "y_train_probs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0380f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our model in Velour\n",
    "velour_model = Model(client, \"breast-cancer-linear-model\", delete_if_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de269b",
   "metadata": {},
   "source": [
    "### Adding Predictions to Our Model\n",
    "\n",
    "With our model defined in Velour, we can post predictions for each of our `Datasets` to our `Model` object. Each `Prediction` should contain a list of `Labels` describing the prediction and its associated confidence score. Since we're running a classification task, the confidence scores over all prediction classes should sum to (approximately) 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a224345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 426/426 [00:08<00:00, 51.84it/s]\n",
      "100%|██████████| 143/143 [00:02<00:00, 55.27it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define our predictions\n",
    "training_predictions = [\n",
    "    Prediction(\n",
    "        datum=Datum(\n",
    "            dataset=velour_train_dataset.name,\n",
    "            uid=f\"train{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.CLASSIFICATION,\n",
    "                labels=[\n",
    "                    Label(\n",
    "                        key=\"class\", \n",
    "                        value=target_names[j],\n",
    "                        score=p,\n",
    "                    )                        \n",
    "                    for j, p in enumerate(prob)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, prob in enumerate(y_train_probs)\n",
    "]\n",
    "\n",
    "testing_predictions = [\n",
    "    Prediction(\n",
    "        datum=Datum(\n",
    "            dataset=velour_test_dataset.name,\n",
    "            uid=f\"test{i}\",\n",
    "        ),\n",
    "        annotations=[\n",
    "            Annotation(\n",
    "                task_type=TaskType.CLASSIFICATION,\n",
    "                labels=[\n",
    "                    Label(\n",
    "                        key=\"class\",\n",
    "                        value=target_names[j],\n",
    "                        score=p,\n",
    "                    )                        \n",
    "                    for j, p in enumerate(prob)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for i, prob in enumerate(y_test_probs)\n",
    "]\n",
    "\n",
    "# add the train predictions\n",
    "for pd in tqdm(training_predictions):\n",
    "    velour_model.add_prediction(velour_train_dataset, pd)\n",
    "\n",
    "# add the test predictions\n",
    "for pd in tqdm(testing_predictions):\n",
    "    velour_model.add_prediction(velour_test_dataset, pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c9943",
   "metadata": {},
   "source": [
    "### Finalizing Our Model\n",
    "\n",
    "Finally, we finalize our `Model` to prep it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f7cfd2-4784-466d-90b8-32c813fb7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "velour_model.finalize_inferences(velour_train_dataset)\n",
    "velour_model.finalize_inferences(velour_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b78f442",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "With our `Dataset` and `Model` defined, we're ready to evaluate our performance and display the results. Note that we use the `wait_for_completion` method since all evaluations run as background tasks; this method ensures that the evaluation finishes before we display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba0e545-4eaa-4f6b-8d62-f3a63018e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>breast-cancer-train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>parameters</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>{\"label_key\": \"class\"}</th>\n",
       "      <th>n/a</th>\n",
       "      <td>0.988263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">\"n/a\"</th>\n",
       "      <th>class: benign</th>\n",
       "      <td>0.990403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class: malignant</th>\n",
       "      <td>0.984894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Precision</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">\"n/a\"</th>\n",
       "      <th>class: benign</th>\n",
       "      <td>0.984733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class: malignant</th>\n",
       "      <td>0.993902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCAUC</th>\n",
       "      <th>{\"label_key\": \"class\"}</th>\n",
       "      <th>n/a</th>\n",
       "      <td>0.997411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Recall</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">\"n/a\"</th>\n",
       "      <th>class: benign</th>\n",
       "      <td>0.996139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class: malignant</th>\n",
       "      <td>0.976048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                value\n",
       "dataset                                           breast-cancer-train\n",
       "type      parameters             label                               \n",
       "Accuracy  {\"label_key\": \"class\"} n/a                         0.988263\n",
       "F1        \"n/a\"                  class: benign               0.990403\n",
       "                                 class: malignant            0.984894\n",
       "Precision \"n/a\"                  class: benign               0.984733\n",
       "                                 class: malignant            0.993902\n",
       "ROCAUC    {\"label_key\": \"class\"} n/a                         0.997411\n",
       "Recall    \"n/a\"                  class: benign               0.996139\n",
       "                                 class: malignant            0.976048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval_job = velour_model.evaluate_classification(velour_train_dataset)\n",
    "train_eval_job.wait_for_completion()\n",
    "results = train_eval_job.results()\n",
    "\n",
    "results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73626229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label_key': 'class',\n",
       "  'entries': [{'prediction': 'benign', 'groundtruth': 'benign', 'count': 258},\n",
       "   {'prediction': 'benign', 'groundtruth': 'malignant', 'count': 4},\n",
       "   {'prediction': 'malignant', 'groundtruth': 'benign', 'count': 1},\n",
       "   {'prediction': 'malignant', 'groundtruth': 'malignant', 'count': 163}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e447e7-0da4-49ae-af0a-8baa1446b4e7",
   "metadata": {},
   "source": [
    "As a brief sanity check, we can check Velour's outputs against `sklearn's` own classification report. We see that the two results are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347c180e-9913-4aa4-994e-de507da32d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant   0.993902  0.976048  0.984894       167\n",
      "      benign   0.984733  0.996139  0.990403       259\n",
      "\n",
      "    accuracy                       0.988263       426\n",
      "   macro avg   0.989318  0.986093  0.987649       426\n",
      "weighted avg   0.988327  0.988263  0.988244       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_preds = pipe.predict(X_train)\n",
    "print(classification_report(y_train, y_train_preds, digits=6, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-velour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
