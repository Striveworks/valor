{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Velour Dataset Integrations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Velour is crafted to seamlessly integrate with your existing workflows.\n",
    "\n",
    "Our client is equipped to handle a wide array of tasks, including classification, object detection, and semantic segmentation.\n",
    "\n",
    "When it comes to integration, think of it as creating a dataloader. The Velour datasets are structured in a hierarchy of standard types that abstract into the datatypes you provide. \n",
    "\n",
    "At the top level, you'll find `velour.Dataset` which contains a list of `velour.GroundTruth`, which, in turn, consist of `velour.Datum` and `velour.Annotation`.\n",
    "\n",
    "The ultimate objective is to effortlessly map all available metadata into Velour, empowering you to utilize it later for precise evaluations and effective stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from velour import (\n",
    "    Client,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Datum,\n",
    "    Annotation,\n",
    "    GroundTruth, \n",
    "    Prediction,\n",
    "    Label,\n",
    ")\n",
    "from velour.schemas import (\n",
    "    BoundingBox, \n",
    "    Polygon, \n",
    "    BasicPolygon, \n",
    "    Point,\n",
    ")\n",
    "from velour.enums import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully connected to http://0.0.0.0:8000/.\n"
     ]
    }
   ],
   "source": [
    "client = Client(\"http://0.0.0.0:8000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    client=client,   \n",
    "    name=\"myDataset\",\n",
    "    metadata={        # optional, metadata can take `str`, `int`, `float` value types.\n",
    "        \"some_string\": \"hello_world\",\n",
    "        \"some_number\": 1234,\n",
    "        \"a_different_number\": 1.234,\n",
    "    },\n",
    "    geospatial=None,  # optional, define a GeoJSON\n",
    "    reset=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create GroundTruths**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifications = [\n",
    "    {\"path\": \"a/b/c/img1.png\", \"annotations\": [{\"class_label\": \"dog\"}]},\n",
    "    {\"path\": \"a/b/c/img2.png\", \"annotations\": [{\"class_label\": \"cat\"}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundtruth_from_image_classification_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(key=key, value=value)\n",
    "                for label in element[\"annotations\"]\n",
    "                for key, value in label.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in image_classifications:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_image_classification_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Object Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detections = [\n",
    "    {\"path\": \"a/b/c/img3.png\", \"annotations\": [{\"class_label\": \"dog\", \"bbox\": {\"xmin\": 16, \"ymin\": 130, \"xmax\": 70, \"ymax\": 150}}, {\"class_label\": \"person\", \"bbox\": {\"xmin\": 89, \"ymin\": 10, \"xmax\": 97, \"ymax\": 110}}]},\n",
    "    {\"path\": \"a/b/c/img4.png\", \"annotations\": [{\"class_label\": \"cat\", \"bbox\": {\"xmin\": 500, \"ymin\": 220, \"xmax\": 530, \"ymax\": 260}}]},\n",
    "    {\"path\": \"a/b/c/img5.png\", \"annotations\": []}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundtruth_from_object_detection_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"] \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.DETECTION,\n",
    "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
    "            bounding_box=BoundingBox.from_extrema(\n",
    "                xmin=annotation[\"bbox\"][\"xmin\"],\n",
    "                xmax=annotation[\"bbox\"][\"xmax\"],\n",
    "                ymin=annotation[\"bbox\"][\"ymin\"],\n",
    "                ymax=annotation[\"bbox\"][\"ymax\"],\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'person', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'annotations': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czaloom/velour/.env-velour/lib/python3.10/site-packages/velour/coretypes.py:808: UserWarning: GroundTruth for datum with uid `img5` contains no annotations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for element in object_detections:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_object_detection_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Semantic Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_segmentations = [\n",
    "    {\"path\": \"a/b/c/img6.png\", \"annotations\": [{\"class_label\": \"dog\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}]]}]},\n",
    "    {\"path\": \"a/b/c/img7.png\", \"annotations\": [{\"class_label\": \"cat\", \"contour\": [[{\"x\": 97.2, \"y\": 40.2}, {\"x\": 33.33, \"y\": 44.3}, {\"x\": 10.9, \"y\": 18.7}]]}]},\n",
    "    {\"path\": \"a/b/c/img8.png\", \"annotations\": [{\"class_label\": \"car\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}], [{\"x\": 60.0, \"y\": 15.5}, {\"x\": 70.9, \"y\": 50.2}, {\"x\": 75.9, \"y\": 28.4}]]}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundtruth_from_image_segmentation_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"] \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.SEGMENTATION,\n",
    "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
    "            polygon=Polygon(\n",
    "                boundary=BasicPolygon(\n",
    "                    points=[\n",
    "                        Point(p[\"x\"], p[\"y\"])\n",
    "                        for p in annotation[\"contour\"][0]\n",
    "                    ],\n",
    "                ),\n",
    "                holes=[\n",
    "                    BasicPolygon(\n",
    "                        points=[\n",
    "                            Point(p[\"x\"], p[\"y\"])\n",
    "                            for p in hole\n",
    "                        ]\n",
    "                    )\n",
    "                    for hole in annotation[\"contour\"][1:]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation[\"contour\"]) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in image_segmentations:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_image_segmentation_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifications = [\n",
    "    {\"path\": \"a/b/c/text1.txt\", \"annotations\": [{\"sentiment\": {\"context\": \"Is the content of this product review postive?\", \"label\": \"positive\"}}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundtruth_from_text_classification_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"],\n",
    "            \"context\": element[\"annotations\"][0][\"sentiment\"][\"context\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(\n",
    "                    key=\"label\", \n",
    "                    value=element[\"annotations\"][0][\"sentiment\"][\"label\"]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in text_classifications:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_text_classification_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Token Classification**\n",
    "\n",
    "This example is nuanced because the way datums are communicated is different between the source dictionary and Velour.\n",
    "\n",
    "Specifically, the location of the token is considered the `Datum` as it encodes what the subject of comparison is. We can apply this by encoding the positions into the `Datum` uid. As long as the model shares the same tokenizer as the dataset we can guarantee that this will lead to proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_classifications = [\n",
    "    {\"path\": \"a/b/c/text2.text\", \"annotations\": [{\"token_classification\": {\"start_position\": 19, \"end_position\": 22, \"label\": \"place\"}}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groundtruth_from_token_classification_dict(element: dict):\n",
    "\n",
    "    filename = Path(element[\"path\"]).stem\n",
    "    start_position = element[\"annotations\"][0][\"token_classification\"][\"start_position\"]\n",
    "    end_position = element[\"annotations\"][0][\"token_classification\"][\"end_position\"]\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=f\"{filename}_{start_position}_{end_position}\",\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"],\n",
    "            \"filename\": filename,\n",
    "            \"start_position\": start_position,\n",
    "            \"end_position\": end_position,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(\n",
    "                    key=\"label\", \n",
    "                    value=element[\"annotations\"][0][\"token_classification\"]['label']\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'text2_19_22', 'metadata': {'path': 'a/b/c/text2.text', 'filename': 'text2', 'start_position': 19, 'end_position': 22}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'place', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in token_classifications:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_token_classification_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Datatset Finalization**\n",
    "\n",
    "Now that we are done creating `GroundTruth` objects we can move on to finalizing the `Dataset`. This is required for evaluation but not for `Model` creation so it could also be done later. Velour makes this a requirement for traceability reasons, you can be certain that a `Dataset` that was finalized weeks ago will still be the same today.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.finalize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    client=client,\n",
    "    name=\"myModel\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "        \"some_number\": 4321,\n",
    "    },\n",
    "    geospatial=None,\n",
    "    reset=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Predictions**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classifications = [\n",
    "    {\"path\": \"a/b/c/img1.png\", \"annotations\": [{\"class_label\": \"dog\"}]},\n",
    "    {\"path\": \"a/b/c/img2.png\", \"annotations\": [{\"class_label\": \"cat\"}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_from_image_classification_dict(element: dict) -> Prediction:\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(key=key, value=value, score=)\n",
    "                for label in element[\"annotations\"]\n",
    "                for key, value in label.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in image_classifications:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_image_classification_dict(element)\n",
    "\n",
    "    # add prediction to dataset\n",
    "    model.add_prediction(prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Object Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detections = [\n",
    "    {\"path\": \"a/b/c/img3.png\", \"annotations\": [{\"class_label\": \"dog\", \"bbox\": {\"xmin\": 16, \"ymin\": 130, \"xmax\": 70, \"ymax\": 150}}, {\"class_label\": \"person\", \"bbox\": {\"xmin\": 89, \"ymin\": 10, \"xmax\": 97, \"ymax\": 110}}]},\n",
    "    {\"path\": \"a/b/c/img4.png\", \"annotations\": [{\"class_label\": \"cat\", \"bbox\": {\"xmin\": 500, \"ymin\": 220, \"xmax\": 530, \"ymax\": 260}}]},\n",
    "    {\"path\": \"a/b/c/img5.png\", \"annotations\": []}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_from_object_detection_dict(element: dict) -> Prediction:\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"] \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.DETECTION,\n",
    "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"], score=)],\n",
    "            bounding_box=BoundingBox.from_extrema(\n",
    "                xmin=annotation[\"bbox\"][\"xmin\"],\n",
    "                xmax=annotation[\"bbox\"][\"xmax\"],\n",
    "                ymin=annotation[\"bbox\"][\"ymin\"],\n",
    "                ymax=annotation[\"bbox\"][\"ymax\"],\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'person', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'annotations': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czaloom/velour/.env-velour/lib/python3.10/site-packages/velour/coretypes.py:808: UserWarning: GroundTruth for datum with uid `img5` contains no annotations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for element in object_detections:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_object_detection_dict(element)\n",
    "\n",
    "    # add prediction to model\n",
    "    model.add_prediction(prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Semantic Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_segmentations = [\n",
    "    {\"path\": \"a/b/c/img6.png\", \"annotations\": [{\"class_label\": \"dog\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}]]}]},\n",
    "    {\"path\": \"a/b/c/img7.png\", \"annotations\": [{\"class_label\": \"cat\", \"contour\": [[{\"x\": 97.2, \"y\": 40.2}, {\"x\": 33.33, \"y\": 44.3}, {\"x\": 10.9, \"y\": 18.7}]]}]},\n",
    "    {\"path\": \"a/b/c/img8.png\", \"annotations\": [{\"class_label\": \"car\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}], [{\"x\": 60.0, \"y\": 15.5}, {\"x\": 70.9, \"y\": 50.2}, {\"x\": 75.9, \"y\": 28.4}]]}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_from_image_segmentation_dict(element: dict) -> Prediction:\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"] \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.SEGMENTATION,\n",
    "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
    "            polygon=Polygon(\n",
    "                boundary=BasicPolygon(\n",
    "                    points=[\n",
    "                        Point(p[\"x\"], p[\"y\"])\n",
    "                        for p in annotation[\"contour\"][0]\n",
    "                    ],\n",
    "                ),\n",
    "                holes=[\n",
    "                    BasicPolygon(\n",
    "                        points=[\n",
    "                            Point(p[\"x\"], p[\"y\"])\n",
    "                            for p in hole\n",
    "                        ]\n",
    "                    )\n",
    "                    for hole in annotation[\"contour\"][1:]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation[\"contour\"]) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in image_segmentations:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_image_segmentation_dict(element)\n",
    "\n",
    "    # add prediction to model\n",
    "    model.add_prediction(prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifications = [\n",
    "    {\"path\": \"a/b/c/text1.txt\", \"annotations\": [{\"sentiment\": {\"context\": \"Is the content of this product review postive?\", \"label\": \"positive\"}}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_from_text_classification_dict(element: dict) -> Prediction:\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"],\n",
    "            \"context\": element[\"annotations\"][0][\"sentiment\"][\"context\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(\n",
    "                    key=\"label\", \n",
    "                    value=element[\"annotations\"][0][\"sentiment\"][\"label\"],\n",
    "                    score=,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in text_classifications:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_text_classification_dict(element)\n",
    "\n",
    "    # add prediction to model\n",
    "    model.add_prediction(prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Token Classification**\n",
    "\n",
    "This example is nuanced because the way datums are communicated is different between the source dictionary and Velour.\n",
    "\n",
    "Specifically, the location of the token is considered the `Datum` as it encodes what the subject of comparison is. We can apply this by encoding the positions into the `Datum` uid. As long as the model shares the same tokenizer as the dataset we can guarantee that this will lead to proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_classifications = [\n",
    "    {\"path\": \"a/b/c/text2.text\", \"annotations\": [{\"token_classification\": {\"start_position\": 19, \"end_position\": 22, \"label\": \"place\"}}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_from_token_classification_dict(element: dict) -> Predicton:\n",
    "\n",
    "    filename = Path(element[\"path\"]).stem\n",
    "    start_position = element[\"annotations\"][0][\"token_classification\"][\"start_position\"]\n",
    "    end_position = element[\"annotations\"][0][\"token_classification\"][\"end_position\"]\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=f\"{filename}_{start_position}_{end_position}\",\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"],\n",
    "            \"filename\": filename,\n",
    "            \"start_position\": start_position,\n",
    "            \"end_position\": end_position,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(\n",
    "                    key=\"label\", \n",
    "                    value=element[\"annotations\"][0][\"token_classification\"]['label'],\n",
    "                    score=,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'text2_19_22', 'metadata': {'path': 'a/b/c/text2.text', 'filename': 'text2', 'start_position': 19, 'end_position': 22}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'place', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for element in token_classifications:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_token_classification_dict(element)\n",
    "\n",
    "    # add prediction to dataset\n",
    "    model.add_prediction(prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Finalization**\n",
    "\n",
    "Now that we are done creating `Prediction` objects we can move on to finalizing the `Model` over the existing `Dataset`. This is required for evaluation for traceability reasons, you can be certain that an inference run will be as useful tomorrow as it is today.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.finalize(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploring the Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'name': 'coco2017-panoptic',\n",
       "  'metadata': {'url': 'http://cocodataset.org',\n",
       "   'year': 2018.0,\n",
       "   'version': '1.0',\n",
       "   'licenses': \"[{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/', 'id': 1, 'name': 'Attribution-NonCommercial-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nc/2.0/', 'id': 2, 'name': 'Attribution-NonCommercial License'}, {'url': 'http://creativecommons.org/licenses/by-nc-nd/2.0/', 'id': 3, 'name': 'Attribution-NonCommercial-NoDerivs License'}, {'url': 'http://creativecommons.org/licenses/by/2.0/', 'id': 4, 'name': 'Attribution License'}, {'url': 'http://creativecommons.org/licenses/by-sa/2.0/', 'id': 5, 'name': 'Attribution-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nd/2.0/', 'id': 6, 'name': 'Attribution-NoDerivs License'}, {'url': 'http://flickr.com/commons/usage/', 'id': 7, 'name': 'No known copyright restrictions'}, {'url': 'http://www.usa.gov/copyright.shtml', 'id': 8, 'name': 'United States Government Work'}]\",\n",
       "   'contributor': 'https://arxiv.org/abs/1801.00868',\n",
       "   'description': 'COCO 2018 Panoptic Dataset',\n",
       "   'date_created': '2018-06-01 00:00:00.0'},\n",
       "  'geospatial': {}},\n",
       " {'id': 22,\n",
       "  'name': 'myDataset',\n",
       "  'metadata': {'some_number': 1234.0,\n",
       "   'some_string': 'hello_world',\n",
       "   'a_different_number': 1.234},\n",
       "  'geospatial': {}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'name': 'yolov8n-seg', 'metadata': {}, 'geospatial': {}},\n",
       " {'id': 2,\n",
       "  'name': 'myModel',\n",
       "  'metadata': {'foo': 'bar', 'some_number': 4321.0},\n",
       "  'geospatial': {}}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploring the Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.get_datums()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'text2_19_22', 'metadata': {'path': 'a/b/c/text2.text', 'filename': 'text2', 'end_position': 22.0, 'start_position': 19.0}, 'geospatial': {}}\n"
     ]
    }
   ],
   "source": [
    "for datum in dataset.get_datums():\n",
    "    print(datum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.get_groundtruth(uid: str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'person', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'annotations': []}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': None}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': None}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'text2_19_22', 'metadata': {'path': 'a/b/c/text2.text', 'filename': 'text2', 'end_position': 22.0, 'start_position': 19.0}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'place', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for datum in dataset.get_datums():\n",
    "    groundtruth = dataset.get_groundtruth(datum.uid)\n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.get_labels()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'class_label', 'value': 'car', 'score': None}\n",
      "{'key': 'label', 'value': 'place', 'score': None}\n",
      "{'key': 'class_label', 'value': 'cat', 'score': None}\n",
      "{'key': 'class_label', 'value': 'dog', 'score': None}\n",
      "{'key': 'label', 'value': 'positive', 'score': None}\n",
      "{'key': 'class_label', 'value': 'person', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "for label in dataset.get_labels():\n",
    "    print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-velour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
