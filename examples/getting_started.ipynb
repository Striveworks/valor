{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Velour\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Velour is a centralized evaluation store which makes it easy to measure, explore, and rank model performance. Velour empowers data scientists and engineers to evaluate the performance of their machine learning pipelines and use those evaluations to make better modeling decisions in the future. For a conceptual introduction to Velour, [check out our project overview](https://striveworks.github.io/velour/).\n",
    "\n",
    "In this notebook, we'll introduce Velour's high-level abstractions and walk through a computer vision-oriented example of how you can use Velour to evaluate model performance. For task-specific examples, please see our follow-up notebooks below:\n",
    "- [Tabular classification](https://github.com/Striveworks/velour/blob/main/examples/classification/tabular.ipynb)\n",
    "- [Object detection](https://github.com/Striveworks/velour/blob/main/examples/object-detection/coco-yolo.ipynb)\n",
    "- [Semantic segmentation](https://github.com/Striveworks/velour/blob/main/examples/semantic-segmentation/coco-yolo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Velour is equipped to handle a wide variety of supervised learning tasks thanks to its six core abstractions. We can think of these abstractions as being split into two categories:\n",
    "- **Dataset**: When describing our actual dataset, we define a `Dataset` containing a list of `GroundTruths` which, in turn, are made up of `Datums` and `Annotations`.\n",
    "- **Model**: When describing our model outputs, we define a `Model` containing a list of `Predictions` which, in turn, are also made up of `Datums` and `Annotations`. We then link our `Model` to a `Dataset` when finalizing the model.\n",
    "\n",
    "After we define both our dataset inputs and model outputs, Velour will make it easy to calculate and store our evaluation metrics. Let's start by describing our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we import all needed packages and connect to our Velour API. For instructions on setting up your API, please see [our docs here](https://striveworks.github.io/velour/getting_started/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The Velour client isn't versioned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to host at http://0.0.0.0:8000/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from velour import (\n",
    "    Client,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Datum,\n",
    "    Annotation,\n",
    "    GroundTruth, \n",
    "    Prediction,\n",
    "    Label,\n",
    ")\n",
    "from velour.schemas import (\n",
    "    BoundingBox, \n",
    "    Polygon, \n",
    "    BasicPolygon, \n",
    "    Point,\n",
    ")\n",
    "from velour.enums import TaskType\n",
    "\n",
    "client = Client(\"http://0.0.0.0:8000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our `Dataset` in Velour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    client=client,   \n",
    "    name=\"myDataset\",\n",
    "    metadata={        # optional, metadata can take `str`, `int`, `float` value types.\n",
    "        \"some_string\": \"hello_world\",\n",
    "        \"some_number\": 1234,\n",
    "        \"a_different_number\": 1.234,\n",
    "    },\n",
    "    geospatial=None,  # optional, define a GeoJSON\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe the various objects in our `Dataset`, we'll associate a list of `GroundTruths` (made up of `Annotations` and `Datums`) to the `Dataset` we defined above. Note that Velour doesn't actually store any images, and that the `Annotations` we use will vary by our task type (i.e., object detection, semantic segmentation, etc.). For demonstrative purposes, we'll create `GroundTruths` for four different learning tasks in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Object Detection GroundTruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'person', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'annotations': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nthorlind/git/sw/velour/client/velour/coretypes.py:808: UserWarning: GroundTruth for datum with uid `img5` contains no annotations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def create_groundtruth_from_object_detection_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"] \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.DETECTION,\n",
    "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
    "            bounding_box=BoundingBox.from_extrema(\n",
    "                xmin=annotation[\"bbox\"][\"xmin\"],\n",
    "                xmax=annotation[\"bbox\"][\"xmax\"],\n",
    "                ymin=annotation[\"bbox\"][\"ymin\"],\n",
    "                ymax=annotation[\"bbox\"][\"ymax\"],\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "image_object_detections = [\n",
    "    {\"path\": \"a/b/c/img3.png\", \"annotations\": [{\"class_label\": \"dog\", \"bbox\": {\"xmin\": 16, \"ymin\": 130, \"xmax\": 70, \"ymax\": 150}}, {\"class_label\": \"person\", \"bbox\": {\"xmin\": 89, \"ymin\": 10, \"xmax\": 97, \"ymax\": 110}}]},\n",
    "    {\"path\": \"a/b/c/img4.png\", \"annotations\": [{\"class_label\": \"cat\", \"bbox\": {\"xmin\": 500, \"ymin\": 220, \"xmax\": 530, \"ymax\": 260}}]},\n",
    "    {\"path\": \"a/b/c/img5.png\", \"annotations\": []}\n",
    "]\n",
    "\n",
    "\n",
    "for element in image_object_detections:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_object_detection_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Image Classification GroundTruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "def create_groundtruth_from_image_classification_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(key=key, value=value)\n",
    "                for label in element[\"annotations\"]\n",
    "                for key, value in label.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "image_classifications = [\n",
    "    {\"path\": \"a/b/c/img1.png\", \"annotations\": [{\"class_label\": \"dog\"}]},\n",
    "    {\"path\": \"a/b/c/img2.png\", \"annotations\": [{\"class_label\": \"cat\"}]}\n",
    "]\n",
    "\n",
    "for element in image_classifications:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_image_classification_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Image Segmentation GroundTruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "def create_groundtruth_from_image_segmentation_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"] \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.SEGMENTATION,\n",
    "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
    "            polygon=Polygon(\n",
    "                boundary=BasicPolygon(\n",
    "                    points=[\n",
    "                        Point(p[\"x\"], p[\"y\"])\n",
    "                        for p in annotation[\"contour\"][0]\n",
    "                    ],\n",
    "                ),\n",
    "                holes=[\n",
    "                    BasicPolygon(\n",
    "                        points=[\n",
    "                            Point(p[\"x\"], p[\"y\"])\n",
    "                            for p in hole\n",
    "                        ]\n",
    "                    )\n",
    "                    for hole in annotation[\"contour\"][1:]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation[\"contour\"]) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "image_segmentations = [\n",
    "    {\"path\": \"a/b/c/img6.png\", \"annotations\": [{\"class_label\": \"dog\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}]]}]},\n",
    "    {\"path\": \"a/b/c/img7.png\", \"annotations\": [{\"class_label\": \"cat\", \"contour\": [[{\"x\": 97.2, \"y\": 40.2}, {\"x\": 33.33, \"y\": 44.3}, {\"x\": 10.9, \"y\": 18.7}]]}]},\n",
    "    {\"path\": \"a/b/c/img8.png\", \"annotations\": [{\"class_label\": \"car\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}], [{\"x\": 60.0, \"y\": 15.5}, {\"x\": 70.9, \"y\": 50.2}, {\"x\": 75.9, \"y\": 28.4}]]}]}\n",
    "]\n",
    "\n",
    "for element in image_segmentations:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_image_segmentation_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Classification GroundTruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "def create_groundtruth_from_text_classification_dict(element: dict):\n",
    "    \n",
    "    # create Datum using filename, save the full filepath into metadata\n",
    "    datum = Datum(\n",
    "        uid=Path(element[\"path\"]).stem,\n",
    "        metadata={\n",
    "            \"path\": element[\"path\"],\n",
    "            \"context\": element[\"annotations\"][0][\"sentiment\"][\"context\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(\n",
    "                    key=\"label\", \n",
    "                    value=element[\"annotations\"][0][\"sentiment\"][\"label\"]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return GroundTruth\n",
    "    return GroundTruth(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "text_classifications = [\n",
    "    {\"path\": \"a/b/c/text1.txt\", \"annotations\": [{\"sentiment\": {\"context\": \"Is the content of this product review postive?\", \"label\": \"positive\"}}]}\n",
    "]\n",
    "\n",
    "for element in text_classifications:\n",
    "    # create groundtruth\n",
    "    groundtruth = create_groundtruth_from_text_classification_dict(element)\n",
    "\n",
    "    # add groundtruth to dataset\n",
    "    dataset.add_groundtruth(groundtruth)\n",
    "    \n",
    "    print(groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing Our Dataset\n",
    "\n",
    "Now that we've created all of our `GroundTruth` objects, we finalize our `Dataset` such that it's ready for evaluation. Velour makes finalization a requirement for traceability purposes: we want you to feel confident that a finalized `Dataset` or `Model` won't change over any length of time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.finalize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Model\n",
    "\n",
    "Now that we've described our dataset, the next step is to define our model and subsequent predictions. Again, for demonstrative purposes, we'll define predictions for four separate task types in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    client=client,\n",
    "    name=\"myModel\",\n",
    "    metadata={\n",
    "        \"foo\": \"bar\",\n",
    "        \"some_number\": 4321,\n",
    "    },\n",
    "    geospatial=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Object Detection Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.8}, {'key': 'class_label', 'value': 'cat', 'score': 0.1}, {'key': 'class_label', 'value': 'person', 'score': 0.1}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.05}, {'key': 'class_label', 'value': 'cat', 'score': 0.05}, {'key': 'class_label', 'value': 'person', 'score': 0.9}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.8}, {'key': 'class_label', 'value': 'cat', 'score': 0.1}, {'key': 'class_label', 'value': 'person', 'score': 0.1}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nthorlind/git/sw/velour/client/velour/coretypes.py:1103: UserWarning: Prediction for datum with uid `img5` contains no annotations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# populate a dictionary mapping Datum UIDs to datums for all of the datums in our dataset\n",
    "datums_by_uid = {\n",
    "    datum.uid: datum\n",
    "    for datum in dataset.get_datums()\n",
    "}\n",
    "\n",
    "def create_prediction_from_object_detection_dict(element: dict, datums_by_uid:dict) -> Prediction:\n",
    "    \n",
    "    # get datum from dataset using filename\n",
    "    uid=Path(element[\"path\"]).stem\n",
    "    datum = datums_by_uid[uid]\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.DETECTION,\n",
    "            labels=[\n",
    "                Label(key=\"class_label\", value=label[\"class_label\"], score=label[\"score\"])\n",
    "                for label in annotation[\"labels\"]\n",
    "            ],\n",
    "            bounding_box=BoundingBox.from_extrema(\n",
    "                xmin=annotation[\"bbox\"][\"xmin\"],\n",
    "                xmax=annotation[\"bbox\"][\"xmax\"],\n",
    "                ymin=annotation[\"bbox\"][\"ymin\"],\n",
    "                ymax=annotation[\"bbox\"][\"ymax\"],\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "object_detections = [\n",
    "    {\"path\": \"a/b/c/img3.png\", \"annotations\": [\n",
    "        {\"labels\": [{\"class_label\": \"dog\", \"score\": 0.8}, {\"class_label\": \"cat\", \"score\": 0.1}, {\"class_label\": \"person\", \"score\": 0.1}], \"bbox\": {\"xmin\": 16, \"ymin\": 130, \"xmax\": 70, \"ymax\": 150}}, \n",
    "        {\"labels\": [{\"class_label\": \"dog\", \"score\": 0.05}, {\"class_label\": \"cat\", \"score\": 0.05}, {\"class_label\": \"person\", \"score\": 0.9}], \"bbox\": {\"xmin\": 89, \"ymin\": 10, \"xmax\": 97, \"ymax\": 110}}\n",
    "    ]},\n",
    "    {\"path\": \"a/b/c/img4.png\", \"annotations\": [\n",
    "        {\"labels\": [{\"class_label\": \"dog\", \"score\": 0.8}, {\"class_label\": \"cat\", \"score\": 0.1}, {\"class_label\": \"person\", \"score\": 0.1}], \"bbox\": {\"xmin\": 500, \"ymin\": 220, \"xmax\": 530, \"ymax\": 260}}\n",
    "    ]},\n",
    "    {\"path\": \"a/b/c/img5.png\", \"annotations\": []}\n",
    "]\n",
    "\n",
    "for element in object_detections:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_object_detection_dict(element, datums_by_uid=datums_by_uid)\n",
    "\n",
    "    # add prediction to model\n",
    "    model.add_prediction(dataset, prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Image Classification Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.9}, {'key': 'class_label', 'value': 'cat', 'score': 0.1}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.1}, {'key': 'class_label', 'value': 'cat', 'score': 0.9}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "def create_prediction_from_image_classification_dict(element: dict, datums_by_uid:dict) -> Prediction:\n",
    "    \n",
    "    # get datum from dataset using filename\n",
    "    uid=Path(element[\"path\"]).stem\n",
    "    datum = datums_by_uid[uid]\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(key=\"class_label\", value=label[\"class_label\"], score=label[\"score\"])\n",
    "                for label in element[\"annotations\"]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        model=model,\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "image_classifications = [\n",
    "    {\"path\": \"a/b/c/img1.png\", \"annotations\": [{\"class_label\": \"dog\", \"score\": 0.9}, {\"class_label\": \"cat\", \"score\": 0.1}]},\n",
    "    {\"path\": \"a/b/c/img2.png\", \"annotations\": [{\"class_label\": \"dog\", \"score\": 0.1}, {\"class_label\": \"cat\", \"score\": 0.9}]}\n",
    "]\n",
    "\n",
    "for element in image_classifications:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_image_classification_dict(element, datums_by_uid=datums_by_uid)\n",
    "\n",
    "    # add prediction to dataset\n",
    "    model.add_prediction(dataset, prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Image Segmentation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': []}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "def create_prediction_from_image_segmentation_dict(element: dict, datums_by_uid: dict) -> Prediction:\n",
    "    \n",
    "    # get datum from dataset using filename\n",
    "    uid=Path(element[\"path\"]).stem\n",
    "    datum = datums_by_uid[uid]\n",
    "\n",
    "\n",
    "    # create Annotations\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.SEGMENTATION,\n",
    "            labels=[\n",
    "                Label(key=\"class_label\", value=annotation[\"class_label\"])\n",
    "            ],\n",
    "            polygon=Polygon(\n",
    "                boundary=BasicPolygon(\n",
    "                    points=[\n",
    "                        Point(p[\"x\"], p[\"y\"])\n",
    "                        for p in annotation[\"contour\"][0]\n",
    "                    ],\n",
    "                ),\n",
    "                holes=[\n",
    "                    BasicPolygon(\n",
    "                        points=[\n",
    "                            Point(p[\"x\"], p[\"y\"])\n",
    "                            for p in hole\n",
    "                        ]\n",
    "                    )\n",
    "                    for hole in annotation[\"contour\"][1:]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for annotation in element[\"annotations\"]\n",
    "        if len(annotation[\"contour\"]) > 0\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "image_segmentations = [\n",
    "    {\n",
    "        \"path\": \"a/b/c/img6.png\", \n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"class_label\": \"dog\",\n",
    "                \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}]]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"a/b/c/img7.png\", \n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"class_label\": \"cat\",\n",
    "                \"contour\": [[{\"x\": 97.2, \"y\": 40.2}, {\"x\": 33.33, \"y\": 44.3}, {\"x\": 10.9, \"y\": 18.7}]]\n",
    "            }\n",
    "        ]   \n",
    "    },\n",
    "    {\n",
    "        \"path\": \"a/b/c/img8.png\", \n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"class_label\": \"car\",\n",
    "                \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}], [{\"x\": 60.0, \"y\": 15.5}, {\"x\": 70.9, \"y\": 50.2}, {\"x\": 75.9, \"y\": 28.4}]]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for element in image_segmentations:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_image_segmentation_dict(element, datums_by_uid=datums_by_uid)\n",
    "\n",
    "    # add prediction to model\n",
    "    model.add_prediction(dataset, prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Classification Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': 0.8}, {'key': 'label', 'value': 'negative', 'score': 0.2}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "def create_prediction_from_text_classification_dict(element: dict, datums_by_uid:dict) -> Prediction:\n",
    "    \n",
    "    # get datum from dataset using filename\n",
    "    uid=Path(element[\"path\"]).stem\n",
    "    datum = datums_by_uid[uid]\n",
    "\n",
    "    # create Annotation\n",
    "    annotations = [\n",
    "        Annotation(\n",
    "            task_type=TaskType.CLASSIFICATION,\n",
    "            labels=[\n",
    "                Label(\n",
    "                    key=\"label\", \n",
    "                    value=label[\"label\"],\n",
    "                    score=label[\"score\"],\n",
    "                )\n",
    "                for label in element[\"annotations\"][0][\"sentiment\"][\"labels\"]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # create and return Prediction\n",
    "    return Prediction(\n",
    "        datum=datum,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "\n",
    "text_classifications = [\n",
    "    {\n",
    "        \"path\": \"a/b/c/text1.txt\",\n",
    "        \"annotations\": [\n",
    "            {\"sentiment\": \n",
    "                {\n",
    "                    \"context\": \"Is the content of this product review postive?\", \n",
    "                    \"labels\": [\n",
    "                        {\"label\": \"positive\", \"score\": 0.8},\n",
    "                        {\"label\": \"negative\", \"score\": 0.2}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for element in text_classifications:\n",
    "    # create prediction\n",
    "    prediction = create_prediction_from_text_classification_dict(element, datums_by_uid=datums_by_uid)\n",
    "\n",
    "    # add prediction to model\n",
    "    model.add_prediction(dataset, prediction)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing Our Model\n",
    "\n",
    "Now that we've created all of our `Prediction` objects, we finalize our `Model` such that it's ready for evaluation. When finalizing our `Model`, we pass in the `Dataset` object that we want to link it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.finalize_inferences(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Our Objects\n",
    "\n",
    "Now that we've finalized our `Dataset` and `Model`, we can explore all of the objects stored in Velour before running our evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'name': 'myDataset',\n",
       "  'metadata': {'some_number': 1234.0,\n",
       "   'some_string': 'hello_world',\n",
       "   'a_different_number': 1.234},\n",
       "  'geospatial': {}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'name': 'myModel',\n",
       "  'metadata': {'foo': 'bar', 'some_number': 4321.0},\n",
       "  'geospatial': {}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}\n",
      "{'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}\n"
     ]
    }
   ],
   "source": [
    "for datum in dataset.get_datums():\n",
    "    print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'person', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'annotations': []}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': None}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': None}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for datum in dataset.get_datums():\n",
    "    groundtruth = dataset.get_groundtruth(datum)\n",
    "    print(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'class_label', 'value': 'person', 'score': None}\n",
      "{'key': 'class_label', 'value': 'car', 'score': None}\n",
      "{'key': 'class_label', 'value': 'dog', 'score': None}\n",
      "{'key': 'label', 'value': 'positive', 'score': None}\n",
      "{'key': 'class_label', 'value': 'cat', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "for label in dataset.get_labels():\n",
    "    print(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datum': {'dataset': 'myDataset', 'uid': 'img3', 'metadata': {'path': 'a/b/c/img3.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.8}, {'key': 'class_label', 'value': 'person', 'score': 0.1}, {'key': 'class_label', 'value': 'cat', 'score': 0.1}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 16.0, 'y': 130.0}, {'x': 70.0, 'y': 130.0}, {'x': 70.0, 'y': 150.0}, {'x': 16.0, 'y': 150.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.05}, {'key': 'class_label', 'value': 'person', 'score': 0.9}, {'key': 'class_label', 'value': 'cat', 'score': 0.05}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 89.0, 'y': 10.0}, {'x': 97.0, 'y': 10.0}, {'x': 97.0, 'y': 110.0}, {'x': 89.0, 'y': 110.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img4', 'metadata': {'path': 'a/b/c/img4.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.8}, {'key': 'class_label', 'value': 'person', 'score': 0.1}, {'key': 'class_label', 'value': 'cat', 'score': 0.1}], 'metadata': {}, 'bounding_box': {'polygon': {'points': [{'x': 500.0, 'y': 220.0}, {'x': 530.0, 'y': 220.0}, {'x': 530.0, 'y': 260.0}, {'x': 500.0, 'y': 260.0}]}}, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img5', 'metadata': {'path': 'a/b/c/img5.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': []}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img1', 'metadata': {'path': 'a/b/c/img1.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.9}, {'key': 'class_label', 'value': 'cat', 'score': 0.1}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img2', 'metadata': {'path': 'a/b/c/img2.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': 0.1}, {'key': 'class_label', 'value': 'cat', 'score': 0.9}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img6', 'metadata': {'path': 'a/b/c/img6.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': None}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img7', 'metadata': {'path': 'a/b/c/img7.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 97.2, 'y': 40.2}, {'x': 33.33, 'y': 44.3}, {'x': 10.9, 'y': 18.7}]}, 'holes': None}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'img8', 'metadata': {'path': 'a/b/c/img8.png'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'semantic-segmentation', 'labels': [{'key': 'class_label', 'value': 'car', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': {'boundary': {'points': [{'x': 10.0, 'y': 15.5}, {'x': 20.9, 'y': 50.2}, {'x': 25.9, 'y': 28.4}]}, 'holes': [{'points': [{'x': 60.0, 'y': 15.5}, {'x': 70.9, 'y': 50.2}, {'x': 75.9, 'y': 28.4}]}]}, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n",
      "{'datum': {'dataset': 'myDataset', 'uid': 'text1', 'metadata': {'path': 'a/b/c/text1.txt', 'context': 'Is the content of this product review postive?'}, 'geospatial': {}}, 'model': 'myModel', 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'label', 'value': 'positive', 'score': 0.8}, {'key': 'label', 'value': 'negative', 'score': 0.2}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'multipolygon': None, 'raster': None, 'jsonb': None}]}\n"
     ]
    }
   ],
   "source": [
    "for datum in dataset.get_datums():\n",
    "    print(model.get_prediction(datum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'class_label', 'value': 'person', 'score': None}\n",
      "{'key': 'class_label', 'value': 'car', 'score': None}\n",
      "{'key': 'class_label', 'value': 'dog', 'score': None}\n",
      "{'key': 'label', 'value': 'positive', 'score': None}\n",
      "{'key': 'label', 'value': 'negative', 'score': None}\n",
      "{'key': 'class_label', 'value': 'cat', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "for label in model.get_labels():\n",
    "    print(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "Finally, we'll use our Velour abstractions to evaluate model performance. For more detailed, task-specific examples, see our follow-up notebooks at the links below:\n",
    "- [Tabular classification](https://github.com/Striveworks/velour/blob/main/examples/classification/tabular.ipynb)\n",
    "- [Object detection](https://github.com/Striveworks/velour/blob/main/examples/object-detection/coco-yolo.ipynb)\n",
    "- [Semantic segmentation](https://github.com/Striveworks/velour/blob/main/examples/semantic-segmentation/coco-yolo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(dataset='myDataset', model='myModel', settings=EvaluationSettings(parameters=DetectionParameters(iou_thresholds_to_compute=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95], iou_thresholds_to_return=[0.5, 0.75]), filters=Filter(dataset_names=None, dataset_metadata=None, dataset_geospatial=None, model_names=None, model_metadata=None, model_geospatial=None, datum_uids=None, datum_metadata=None, datum_geospatial=None, task_types=None, annotation_types=None, annotation_geometric_area=None, annotation_metadata=None, annotation_geospatial=None, prediction_scores=None, labels=None, label_ids=None, label_keys=None)), job_id=1, status='done', metrics=[{'type': 'AP', 'parameters': {'iou': 0.5}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'person'}}, {'type': 'AP', 'parameters': {'iou': 0.5}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'dog'}}, {'type': 'AP', 'parameters': {'iou': 0.5}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'cat'}}, {'type': 'AP', 'parameters': {'iou': 0.75}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'person'}}, {'type': 'AP', 'parameters': {'iou': 0.75}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'dog'}}, {'type': 'AP', 'parameters': {'iou': 0.75}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'cat'}}, {'type': 'mAP', 'parameters': {'iou': 0.5}, 'value': 1.0}, {'type': 'mAP', 'parameters': {'iou': 0.75}, 'value': 1.0}, {'type': 'APAveragedOverIOUs', 'parameters': {'ious': [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'person'}}, {'type': 'APAveragedOverIOUs', 'parameters': {'ious': [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'dog'}}, {'type': 'APAveragedOverIOUs', 'parameters': {'ious': [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]}, 'value': 1.0, 'label': {'key': 'class_label', 'value': 'cat'}}, {'type': 'mAPAveragedOverIOUs', 'parameters': {'ious': [0.5, 0.55, 0.6, 0.7, 0.65, 0.75, 0.8, 0.85, 0.9, 0.95]}, 'value': 1.0}], confusion_matrices=[])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_objdet = model.evaluate_detection(dataset)\n",
    "eval_objdet.wait_for_completion()\n",
    "eval_objdet.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifications\n",
    "\n",
    "Note that running the code below evaluates both our text classifications as well as our image classifications. If we only wanted to evaluate one type of classification task, we could use `evaluation_classification`'s `filters` argument to specify which type of labels to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(dataset='myDataset', model='myModel', settings=EvaluationSettings(parameters=None, filters=None), job_id=2, status='done', metrics=[{'type': 'Accuracy', 'parameters': {'label_key': 'class_label'}, 'value': 1.0}, {'type': 'ROCAUC', 'parameters': {'label_key': 'class_label'}, 'value': 1.0}, {'type': 'Precision', 'value': 1.0, 'label': {'key': 'class_label', 'value': 'dog'}}, {'type': 'Recall', 'value': 1.0, 'label': {'key': 'class_label', 'value': 'dog'}}, {'type': 'F1', 'value': 1.0, 'label': {'key': 'class_label', 'value': 'dog'}}, {'type': 'Precision', 'value': 1.0, 'label': {'key': 'class_label', 'value': 'cat'}}, {'type': 'Recall', 'value': 1.0, 'label': {'key': 'class_label', 'value': 'cat'}}, {'type': 'F1', 'value': 1.0, 'label': {'key': 'class_label', 'value': 'cat'}}, {'type': 'Accuracy', 'parameters': {'label_key': 'label'}, 'value': 1.0}, {'type': 'ROCAUC', 'parameters': {'label_key': 'label'}, 'value': 1.0}, {'type': 'Precision', 'value': 1.0, 'label': {'key': 'label', 'value': 'positive'}}, {'type': 'Recall', 'value': 1.0, 'label': {'key': 'label', 'value': 'positive'}}, {'type': 'F1', 'value': 1.0, 'label': {'key': 'label', 'value': 'positive'}}, {'type': 'Precision', 'value': -1.0, 'label': {'key': 'label', 'value': 'negative'}}, {'type': 'Recall', 'value': -1.0, 'label': {'key': 'label', 'value': 'negative'}}, {'type': 'F1', 'value': -1.0, 'label': {'key': 'label', 'value': 'negative'}}], confusion_matrices=[{'label_key': 'class_label', 'entries': [{'prediction': 'cat', 'groundtruth': 'cat', 'count': 1}, {'prediction': 'dog', 'groundtruth': 'dog', 'count': 1}]}, {'label_key': 'label', 'entries': [{'prediction': 'positive', 'groundtruth': 'positive', 'count': 1}]}])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_clf = model.evaluate_classification(dataset)\n",
    "eval_clf.wait_for_completion()\n",
    "eval_clf.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(dataset='myDataset', model='myModel', settings=EvaluationSettings(parameters=None, filters=Filter(dataset_names=None, dataset_metadata=None, dataset_geospatial=None, model_names=None, model_metadata=None, model_geospatial=None, datum_uids=None, datum_metadata=None, datum_geospatial=None, task_types=None, annotation_types=None, annotation_geometric_area=None, annotation_metadata=None, annotation_geospatial=None, prediction_scores=None, labels=None, label_ids=None, label_keys=None)), job_id=3, status='done', metrics=[{'type': 'mIOU', 'value': -1.0}], confusion_matrices=[])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_semseg = model.evaluate_segmentation(dataset)\n",
    "eval_semseg.wait_for_completion()\n",
    "eval_semseg.results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-velour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
