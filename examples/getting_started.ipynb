{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Valor\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Valor is a centralized evaluation store which makes it easy to measure, explore, and rank model performance. Valor empowers data scientists and engineers to evaluate the performance of their machine learning pipelines and use those evaluations to make better modeling decisions in the future. For a conceptual introduction to Valor, [check out our project overview](https://striveworks.github.io/valor/).\n",
        "\n",
        "In this notebook, we'll introduce Valor's high-level abstractions and walk through a computer vision-oriented example of how you can use Valor to evaluate model performance. For task-specific examples, please see our follow-up notebooks below:\n",
        "- [Tabular classification](https://github.com/Striveworks/valor/blob/main/examples/classification/tabular.ipynb)\n",
        "- [Object detection](https://github.com/Striveworks/valor/blob/main/examples/object-detection/coco-yolo.ipynb)\n",
        "- [Semantic segmentation](https://github.com/Striveworks/valor/blob/main/examples/semantic-segmentation/coco-yolo.ipynb)\n",
        "\n",
        "Before using this notebook, please ensure that the Valor service is running on your machine (for start-up instructions, [click here](https://striveworks.github.io/valor/getting_started/)). To connect to a non-local instance of Valor, update `client = Client(\"http://0.0.0.0:8000\")` in the first code block to point to the correct URL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## High-Level Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Valor is equipped to handle a wide variety of supervised learning tasks thanks to its six core abstractions. We can think of these abstractions as being split into two categories:\n",
        "- **Dataset**: When describing our actual dataset, we define a `Dataset` containing a list of `GroundTruths` which, in turn, are made up of `Datums` and `Annotations`.\n",
        "- **Model**: When describing our model outputs, we define a `Model` containing a list of `Predictions` which, in turn, are also made up of `Datums` and `Annotations`. We then link our `Model` to a `Dataset` when finalizing the model.\n",
        "\n",
        "After we define both our dataset inputs and model outputs, Valor will make it easy to calculate and store our evaluation metrics. Let's start by describing our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Our Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, we import all needed packages and connect to our Valor API. For instructions on setting up your API, please see [our docs here](https://striveworks.github.io/valor/getting_started/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:The Valor client isn't versioned.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully connected to host at http://0.0.0.0:8000/\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from valor import (\n",
        "    connect,\n",
        "    Client,\n",
        "    Dataset,\n",
        "    Model,\n",
        "    Datum,\n",
        "    Annotation,\n",
        "    GroundTruth, \n",
        "    Prediction,\n",
        "    Label,\n",
        ")\n",
        "from valor.schemas import (\n",
        "    BoundingBox, \n",
        "    Polygon,\n",
        "    Raster,\n",
        ")\n",
        "from valor.enums import TaskType\n",
        "\n",
        "# connect to the Valor API\n",
        "connect(\"http://0.0.0.0:8000\")\n",
        "client = Client()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define our `Dataset` in Valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Dataset.create(  \n",
        "    name=\"myDataset\",\n",
        "    metadata={        # optional, metadata can take `str`, `int`, `float` value types.\n",
        "        \"some_string\": \"hello_world\",\n",
        "        \"some_number\": 1234,\n",
        "        \"a_different_number\": 1.234,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To describe the various objects in our `Dataset`, we'll associate a list of `GroundTruths` (made up of `Annotations` and `Datums`) to the `Dataset` we defined above. Note that Valor doesn't actually store any images, and that the `Annotations` we use will vary by our task type (i.e., object detection, semantic segmentation, etc.). For demonstrative purposes, we'll create `GroundTruths` for four different learning tasks in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Object Detection GroundTruths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'datum': {'uid': 'img3', 'metadata': {'path': {'type': 'string', 'value': 'a/b/c/img3.png'}}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': [[(16, 130), (70, 130), (70, 150), (16, 150), (16, 130)]], 'polygon': None, 'raster': None, 'embedding': None}, {'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'person', 'score': None}], 'metadata': {}, 'bounding_box': [[(89, 10), (97, 10), (97, 110), (89, 110), (89, 10)]], 'polygon': None, 'raster': None, 'embedding': None}]}\n",
            "{'datum': {'uid': 'img4', 'metadata': {'path': {'type': 'string', 'value': 'a/b/c/img4.png'}}}, 'annotations': [{'task_type': 'object-detection', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': [[(500, 220), (530, 220), (530, 260), (500, 260), (500, 220)]], 'polygon': None, 'raster': None, 'embedding': None}]}\n",
            "{'datum': {'uid': 'img5', 'metadata': {'path': {'type': 'string', 'value': 'a/b/c/img5.png'}}}, 'annotations': []}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/czaloom/valor/.env-velour/lib/python3.10/site-packages/valor/coretypes.py:1241: UserWarning: GroundTruth for datum with uid `img5` contains no annotations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def create_groundtruth_from_object_detection_dict(element: dict):\n",
        "    \n",
        "    # create Datum using filename, save the full filepath into metadata\n",
        "    datum = Datum(\n",
        "        uid=Path(element[\"path\"]).stem,\n",
        "        metadata={\n",
        "            \"path\": element[\"path\"] \n",
        "        }\n",
        "    )\n",
        "\n",
        "    # create Annotations\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.OBJECT_DETECTION,\n",
        "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
        "            bounding_box=BoundingBox.from_extrema(\n",
        "                xmin=annotation[\"bbox\"][\"xmin\"],\n",
        "                xmax=annotation[\"bbox\"][\"xmax\"],\n",
        "                ymin=annotation[\"bbox\"][\"ymin\"],\n",
        "                ymax=annotation[\"bbox\"][\"ymax\"],\n",
        "            )\n",
        "        )\n",
        "        for annotation in element[\"annotations\"]\n",
        "        if len(annotation) > 0\n",
        "    ]\n",
        "\n",
        "    # create and return GroundTruth\n",
        "    return GroundTruth(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "image_object_detections = [\n",
        "    {\"path\": \"a/b/c/img3.png\", \"annotations\": [{\"class_label\": \"dog\", \"bbox\": {\"xmin\": 16, \"ymin\": 130, \"xmax\": 70, \"ymax\": 150}}, {\"class_label\": \"person\", \"bbox\": {\"xmin\": 89, \"ymin\": 10, \"xmax\": 97, \"ymax\": 110}}]},\n",
        "    {\"path\": \"a/b/c/img4.png\", \"annotations\": [{\"class_label\": \"cat\", \"bbox\": {\"xmin\": 500, \"ymin\": 220, \"xmax\": 530, \"ymax\": 260}}]},\n",
        "    {\"path\": \"a/b/c/img5.png\", \"annotations\": []}\n",
        "]\n",
        "\n",
        "\n",
        "for element in image_object_detections:\n",
        "    # create ground truth\n",
        "    groundtruth = create_groundtruth_from_object_detection_dict(element)\n",
        "\n",
        "    # add ground truth to dataset\n",
        "    dataset.add_groundtruth(groundtruth)\n",
        "    \n",
        "    print(groundtruth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Image Classification GroundTruths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'datum': {'uid': 'img1', 'metadata': {'path': {'type': 'string', 'value': 'a/b/c/img1.png'}}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'dog', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'raster': None, 'embedding': None}]}\n",
            "{'datum': {'uid': 'img2', 'metadata': {'path': {'type': 'string', 'value': 'a/b/c/img2.png'}}}, 'annotations': [{'task_type': 'classification', 'labels': [{'key': 'class_label', 'value': 'cat', 'score': None}], 'metadata': {}, 'bounding_box': None, 'polygon': None, 'raster': None, 'embedding': None}]}\n"
          ]
        }
      ],
      "source": [
        "def create_groundtruth_from_image_classification_dict(element: dict):\n",
        "    \n",
        "    # create Datum using filename, save the full filepath into metadata\n",
        "    datum = Datum(\n",
        "        uid=Path(element[\"path\"]).stem,\n",
        "        metadata={\n",
        "            \"path\": element[\"path\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # create Annotation\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.CLASSIFICATION,\n",
        "            labels=[\n",
        "                Label(key=key, value=value)\n",
        "                for label in element[\"annotations\"]\n",
        "                for key, value in label.items()\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # create and return GroundTruth\n",
        "    return GroundTruth(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "image_classifications = [\n",
        "    {\"path\": \"a/b/c/img1.png\", \"annotations\": [{\"class_label\": \"dog\"}]},\n",
        "    {\"path\": \"a/b/c/img2.png\", \"annotations\": [{\"class_label\": \"cat\"}]}\n",
        "]\n",
        "\n",
        "for element in image_classifications:\n",
        "    # create ground truth\n",
        "    groundtruth = create_groundtruth_from_image_classification_dict(element)\n",
        "\n",
        "    # add ground truth to dataset\n",
        "    dataset.add_groundtruth(groundtruth)\n",
        "    \n",
        "    print(groundtruth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Image Segmentation GroundTruths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ClientException",
          "evalue": "{\"name\": \"InternalError\", \"detail\": \"(psycopg2.errors.InternalError_) rt_raster_iterator: The set of rasters provided (custom extent included, if appropriate) do not have the same alignment\\nCONTEXT:  SQL function \\\"st_mapalgebra\\\" statement 1\\n\\n[SQL: INSERT INTO annotation (datum_id, model_id, task_type, meta, created_at, box, polygon, raster, embedding_id) VALUES (%(datum_id)s, %(model_id)s, %(task_type)s, %(meta)s, now(), ST_GeomFromEWKT(%(box)s), ST_GeomFromEWKT(%(polygon)s), (SELECT ST_MapAlgebra(ST_AddBand(ST_MakeEmptyRaster(%(ST_MakeEmptyRaster_1)s, %(ST_MakeEmptyRaster_2)s, %(ST_MakeEmptyRaster_3)s, %(ST_MakeEmptyRaster_4)s, %(ST_MakeEmptyRaster_5)s, %(ST_MakeEmptyRaster_6)s, %(ST_MakeEmptyRaster_7)s, %(ST_MakeEmptyRaster_8)s, %(ST_MakeEmptyRaster_9)s), %(ST_AddBand_1)s), ST_AsRaster(ST_GeomFromText(%(ST_GeomFromText_1)s), %(ST_AsRaster_1)s, %(ST_AsRaster_2)s, %(ST_AsRaster_3)s, %(ST_AsRaster_4)s, %(ST_AsRaster_5)s), %(ST_MapAlgebra_2)s, %(ST_MapAlgebra_3)s, %(ST_MapAlgebra_4)s) AS \\\"ST_MapAlgebra_1\\\"), %(embedding_id)s) RETURNING annotation.id, annotation.created_at]\\n[parameters: {'datum_id': 6, 'model_id': None, 'task_type': 'semantic-segmentation', 'meta': '{}', 'box': None, 'polygon': None, 'ST_MakeEmptyRaster_1': 100, 'ST_MakeEmptyRaster_2': 100, 'ST_MakeEmptyRaster_3': 0, 'ST_MakeEmptyRaster_4': 0, 'ST_MakeEmptyRaster_5': 1, 'ST_MakeEmptyRaster_6': 1, 'ST_MakeEmptyRaster_7': 0, 'ST_MakeEmptyRaster_8': 0, 'ST_MakeEmptyRaster_9': 0, 'ST_AddBand_1': '8BUI', 'ST_GeomFromText_1': 'MULTIPOLYGON (((10.0 15.5, 20.9 50.2, 25.9 28.4, 10.0 15.5)))', 'ST_AsRaster_1': 1.0, 'ST_AsRaster_2': 1.0, 'ST_AsRaster_3': '8BUI', 'ST_AsRaster_4': 1, 'ST_AsRaster_5': 0, 'ST_MapAlgebra_2': '[rast2]', 'ST_MapAlgebra_3': '8BUI', 'ST_MapAlgebra_4': 'UNION', 'embedding_id': None}]\\n(Background on this error at: https://sqlalche.me/e/20/2j85)\", \"timestamp\": 1711757824.746161}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientException\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m groundtruth \u001b[39m=\u001b[39m create_groundtruth_from_image_segmentation_dict(element)\n\u001b[1;32m     52\u001b[0m \u001b[39m# add ground truth to dataset\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m dataset\u001b[39m.\u001b[39;49madd_groundtruth(groundtruth)\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(groundtruth)\n",
            "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/coretypes.py:516\u001b[0m, in \u001b[0;36mDataset.add_groundtruth\u001b[0;34m(self, groundtruth)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_groundtruth\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     groundtruth: GroundTruth,\n\u001b[1;32m    507\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m    Add a ground truth to the dataset.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39m        The ground truth to create.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     Client(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconn)\u001b[39m.\u001b[39;49mcreate_groundtruths(\n\u001b[1;32m    517\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    518\u001b[0m         groundtruths\u001b[39m=\u001b[39;49m[groundtruth],\n\u001b[1;32m    519\u001b[0m     )\n",
            "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/coretypes.py:1247\u001b[0m, in \u001b[0;36mClient.create_groundtruths\u001b[0;34m(self, dataset, groundtruths)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     groundtruth_dict[\u001b[39m\"\u001b[39m\u001b[39mdatum\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_name()\n\u001b[1;32m   1246\u001b[0m     groundtruths_json\u001b[39m.\u001b[39mappend(groundtruth_dict)\n\u001b[0;32m-> 1247\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconn\u001b[39m.\u001b[39;49mcreate_groundtruths(groundtruths_json)\n",
            "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/client.py:370\u001b[0m, in \u001b[0;36mClientConnection.create_groundtruths\u001b[0;34m(self, groundtruths, chunk_size_bytes)\u001b[0m\n\u001b[1;32m    365\u001b[0m chunked_groundtruths \u001b[39m=\u001b[39m _chunk_list(\n\u001b[1;32m    366\u001b[0m     json_list\u001b[39m=\u001b[39mgroundtruths,\n\u001b[1;32m    367\u001b[0m     chunk_size_bytes\u001b[39m=\u001b[39mchunk_size_bytes,\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunked_groundtruths:\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_post_rel_host(\n\u001b[1;32m    371\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgroundtruths\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    372\u001b[0m         json\u001b[39m=\u001b[39;49mchunk,\n\u001b[1;32m    373\u001b[0m     )\n",
            "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/client.py:320\u001b[0m, in \u001b[0;36mClientConnection._requests_post_rel_host\u001b[0;34m(self, endpoint, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_requests_post_rel_host\u001b[39m(\u001b[39mself\u001b[39m, endpoint: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    317\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m    Helper for handling POST requests.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_wrapper(\n\u001b[1;32m    321\u001b[0m         method_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, endpoint\u001b[39m=\u001b[39;49mendpoint, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    322\u001b[0m     )\n",
            "File \u001b[0;32m~/valor/.env-velour/lib/python3.10/site-packages/valor/client.py:307\u001b[0m, in \u001b[0;36mClientConnection._requests_wrapper\u001b[0;34m(self, method_name, endpoint, ignore_auth, *args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m         \u001b[39mraise\u001b[39;00m ClientException(resp)\n\u001b[1;32m    308\u001b[0m     \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mJSONDecodeError, \u001b[39mKeyError\u001b[39;00m):\n\u001b[1;32m    309\u001b[0m         resp\u001b[39m.\u001b[39mraise_for_status()\n",
            "\u001b[0;31mClientException\u001b[0m: {\"name\": \"InternalError\", \"detail\": \"(psycopg2.errors.InternalError_) rt_raster_iterator: The set of rasters provided (custom extent included, if appropriate) do not have the same alignment\\nCONTEXT:  SQL function \\\"st_mapalgebra\\\" statement 1\\n\\n[SQL: INSERT INTO annotation (datum_id, model_id, task_type, meta, created_at, box, polygon, raster, embedding_id) VALUES (%(datum_id)s, %(model_id)s, %(task_type)s, %(meta)s, now(), ST_GeomFromEWKT(%(box)s), ST_GeomFromEWKT(%(polygon)s), (SELECT ST_MapAlgebra(ST_AddBand(ST_MakeEmptyRaster(%(ST_MakeEmptyRaster_1)s, %(ST_MakeEmptyRaster_2)s, %(ST_MakeEmptyRaster_3)s, %(ST_MakeEmptyRaster_4)s, %(ST_MakeEmptyRaster_5)s, %(ST_MakeEmptyRaster_6)s, %(ST_MakeEmptyRaster_7)s, %(ST_MakeEmptyRaster_8)s, %(ST_MakeEmptyRaster_9)s), %(ST_AddBand_1)s), ST_AsRaster(ST_GeomFromText(%(ST_GeomFromText_1)s), %(ST_AsRaster_1)s, %(ST_AsRaster_2)s, %(ST_AsRaster_3)s, %(ST_AsRaster_4)s, %(ST_AsRaster_5)s), %(ST_MapAlgebra_2)s, %(ST_MapAlgebra_3)s, %(ST_MapAlgebra_4)s) AS \\\"ST_MapAlgebra_1\\\"), %(embedding_id)s) RETURNING annotation.id, annotation.created_at]\\n[parameters: {'datum_id': 6, 'model_id': None, 'task_type': 'semantic-segmentation', 'meta': '{}', 'box': None, 'polygon': None, 'ST_MakeEmptyRaster_1': 100, 'ST_MakeEmptyRaster_2': 100, 'ST_MakeEmptyRaster_3': 0, 'ST_MakeEmptyRaster_4': 0, 'ST_MakeEmptyRaster_5': 1, 'ST_MakeEmptyRaster_6': 1, 'ST_MakeEmptyRaster_7': 0, 'ST_MakeEmptyRaster_8': 0, 'ST_MakeEmptyRaster_9': 0, 'ST_AddBand_1': '8BUI', 'ST_GeomFromText_1': 'MULTIPOLYGON (((10.0 15.5, 20.9 50.2, 25.9 28.4, 10.0 15.5)))', 'ST_AsRaster_1': 1.0, 'ST_AsRaster_2': 1.0, 'ST_AsRaster_3': '8BUI', 'ST_AsRaster_4': 1, 'ST_AsRaster_5': 0, 'ST_MapAlgebra_2': '[rast2]', 'ST_MapAlgebra_3': '8BUI', 'ST_MapAlgebra_4': 'UNION', 'embedding_id': None}]\\n(Background on this error at: https://sqlalche.me/e/20/2j85)\", \"timestamp\": 1711757824.746161}"
          ]
        }
      ],
      "source": [
        "def create_groundtruth_from_image_segmentation_dict(element: dict):\n",
        "    \n",
        "    # create Datum using filename, save the full filepath into metadata\n",
        "    datum = Datum(\n",
        "        uid=Path(element[\"path\"]).stem,\n",
        "        metadata={\n",
        "            \"path\": element[\"path\"] \n",
        "        }\n",
        "    )\n",
        "\n",
        "    # create Annotations\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.SEMANTIC_SEGMENTATION,\n",
        "            labels=[Label(key=\"class_label\", value=annotation[\"class_label\"])],\n",
        "            raster=Raster.from_geometry(\n",
        "                geometry=Polygon(\n",
        "                    [\n",
        "                        [\n",
        "                            (pt['x'], pt['y'])\n",
        "                            for pt in [*subpolygon, subpolygon[0]]\n",
        "                        ]\n",
        "                        for subpolygon in annotation[\"contour\"]\n",
        "                    ]\n",
        "                ),\n",
        "                height=100,\n",
        "                width=100,\n",
        "            )\n",
        "        )\n",
        "        for annotation in element[\"annotations\"]\n",
        "        if len(annotation[\"contour\"]) > 0\n",
        "    ]\n",
        "\n",
        "    # create and return GroundTruth\n",
        "    return GroundTruth(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "image_segmentations = [\n",
        "    {\"path\": \"a/b/c/img6.png\", \"annotations\": [{\"class_label\": \"dog\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}]]}]},\n",
        "    {\"path\": \"a/b/c/img7.png\", \"annotations\": [{\"class_label\": \"cat\", \"contour\": [[{\"x\": 97.2, \"y\": 40.2}, {\"x\": 33.33, \"y\": 44.3}, {\"x\": 10.9, \"y\": 18.7}]]}]},\n",
        "    {\"path\": \"a/b/c/img8.png\", \"annotations\": [{\"class_label\": \"car\", \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}], [{\"x\": 60.0, \"y\": 15.5}, {\"x\": 70.9, \"y\": 50.2}, {\"x\": 75.9, \"y\": 28.4}]]}]}\n",
        "]\n",
        "\n",
        "for element in image_segmentations:\n",
        "    # create ground truth\n",
        "    groundtruth = create_groundtruth_from_image_segmentation_dict(element)\n",
        "\n",
        "    # add ground truth to dataset\n",
        "    dataset.add_groundtruth(groundtruth)\n",
        "    \n",
        "    print(groundtruth)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Text Classification GroundTruths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_groundtruth_from_text_classification_dict(element: dict):\n",
        "    \n",
        "    # create Datum using filename, save the full filepath into metadata\n",
        "    datum = Datum(\n",
        "        uid=Path(element[\"path\"]).stem,\n",
        "        metadata={\n",
        "            \"path\": element[\"path\"],\n",
        "            \"context\": element[\"annotations\"][0][\"sentiment\"][\"context\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # create Annotation\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.CLASSIFICATION,\n",
        "            labels=[\n",
        "                Label(\n",
        "                    key=\"label\", \n",
        "                    value=element[\"annotations\"][0][\"sentiment\"][\"label\"]\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # create and return GroundTruth\n",
        "    return GroundTruth(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "text_classifications = [\n",
        "    {\"path\": \"a/b/c/text1.txt\", \"annotations\": [{\"sentiment\": {\"context\": \"Is the content of this product review postive?\", \"label\": \"positive\"}}]}\n",
        "]\n",
        "\n",
        "for element in text_classifications:\n",
        "    # create ground truth\n",
        "    groundtruth = create_groundtruth_from_text_classification_dict(element)\n",
        "\n",
        "    # add ground truth to dataset\n",
        "    dataset.add_groundtruth(groundtruth)\n",
        "    \n",
        "    print(groundtruth)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finalizing Our Dataset\n",
        "\n",
        "Now that we've created all of our `GroundTruth` objects, we finalize our `Dataset` such that it's ready for evaluation. Valor makes finalization a requirement for traceability purposes: we want you to feel confident that a finalized `Dataset` or `Model` won't change over any length of time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.finalize()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Our Model\n",
        "\n",
        "Now that we've described our dataset, the next step is to define our model and subsequent predictions. Again, for demonstrative purposes, we'll define predictions for four separate task types in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model.create(\n",
        "    name=\"myModel\",\n",
        "    metadata={\n",
        "        \"foo\": \"bar\",\n",
        "        \"some_number\": 4321,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Object Detection Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# populate a dictionary mapping Datum UIDs to datums for all of the datums in our dataset\n",
        "datums_by_uid = {\n",
        "    datum.uid: datum\n",
        "    for datum in dataset.get_datums()\n",
        "}\n",
        "\n",
        "def create_prediction_from_object_detection_dict(element: dict, datums_by_uid:dict) -> Prediction:\n",
        "    \n",
        "    # get datum from dataset using filename\n",
        "    uid=Path(element[\"path\"]).stem\n",
        "    datum = datums_by_uid[uid]\n",
        "\n",
        "    # create Annotations\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.OBJECT_DETECTION,\n",
        "            labels=[\n",
        "                Label(key=\"class_label\", value=label[\"class_label\"], score=label[\"score\"])\n",
        "                for label in annotation[\"labels\"]\n",
        "            ],\n",
        "            bounding_box=BoundingBox.from_extrema(\n",
        "                xmin=annotation[\"bbox\"][\"xmin\"],\n",
        "                xmax=annotation[\"bbox\"][\"xmax\"],\n",
        "                ymin=annotation[\"bbox\"][\"ymin\"],\n",
        "                ymax=annotation[\"bbox\"][\"ymax\"],\n",
        "            )\n",
        "        )\n",
        "        for annotation in element[\"annotations\"]\n",
        "        if len(annotation) > 0\n",
        "    ]\n",
        "\n",
        "    # create and return Prediction\n",
        "    return Prediction(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "object_detections = [\n",
        "    {\"path\": \"a/b/c/img3.png\", \"annotations\": [\n",
        "        {\"labels\": [{\"class_label\": \"dog\", \"score\": 0.8}, {\"class_label\": \"cat\", \"score\": 0.1}, {\"class_label\": \"person\", \"score\": 0.1}], \"bbox\": {\"xmin\": 16, \"ymin\": 130, \"xmax\": 70, \"ymax\": 150}}, \n",
        "        {\"labels\": [{\"class_label\": \"dog\", \"score\": 0.05}, {\"class_label\": \"cat\", \"score\": 0.05}, {\"class_label\": \"person\", \"score\": 0.9}], \"bbox\": {\"xmin\": 89, \"ymin\": 10, \"xmax\": 97, \"ymax\": 110}}\n",
        "    ]},\n",
        "    {\"path\": \"a/b/c/img4.png\", \"annotations\": [\n",
        "        {\"labels\": [{\"class_label\": \"dog\", \"score\": 0.8}, {\"class_label\": \"cat\", \"score\": 0.1}, {\"class_label\": \"person\", \"score\": 0.1}], \"bbox\": {\"xmin\": 500, \"ymin\": 220, \"xmax\": 530, \"ymax\": 260}}\n",
        "    ]},\n",
        "    {\"path\": \"a/b/c/img5.png\", \"annotations\": []}\n",
        "]\n",
        "\n",
        "for element in object_detections:\n",
        "    # create prediction\n",
        "    prediction = create_prediction_from_object_detection_dict(element, datums_by_uid=datums_by_uid)\n",
        "\n",
        "    # add prediction to model\n",
        "    model.add_prediction(dataset, prediction)\n",
        "    \n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Image Classification Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prediction_from_image_classification_dict(element: dict, datums_by_uid:dict) -> Prediction:\n",
        "    \n",
        "    # get datum from dataset using filename\n",
        "    uid=Path(element[\"path\"]).stem\n",
        "    datum = datums_by_uid[uid]\n",
        "\n",
        "    # create Annotation\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.CLASSIFICATION,\n",
        "            labels=[\n",
        "                Label(key=\"class_label\", value=label[\"class_label\"], score=label[\"score\"])\n",
        "                for label in element[\"annotations\"]\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # create and return Prediction\n",
        "    return Prediction(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "image_classifications = [\n",
        "    {\"path\": \"a/b/c/img1.png\", \"annotations\": [{\"class_label\": \"dog\", \"score\": 0.9}, {\"class_label\": \"cat\", \"score\": 0.1}]},\n",
        "    {\"path\": \"a/b/c/img2.png\", \"annotations\": [{\"class_label\": \"dog\", \"score\": 0.1}, {\"class_label\": \"cat\", \"score\": 0.9}]}\n",
        "]\n",
        "\n",
        "for element in image_classifications:\n",
        "    # create prediction\n",
        "    prediction = create_prediction_from_image_classification_dict(element, datums_by_uid=datums_by_uid)\n",
        "\n",
        "    # add prediction to dataset\n",
        "    model.add_prediction(dataset, prediction)\n",
        "    \n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Image Segmentation Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prediction_from_image_segmentation_dict(element: dict, datums_by_uid: dict) -> Prediction:\n",
        "    \n",
        "    # get datum from dataset using filename\n",
        "    uid=Path(element[\"path\"]).stem\n",
        "    datum = datums_by_uid[uid]\n",
        "\n",
        "\n",
        "    # create Annotations\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.SEMANTIC_SEGMENTATION,\n",
        "            labels=[\n",
        "                Label(key=\"class_label\", value=annotation[\"class_label\"])\n",
        "            ],\n",
        "            raster=Raster.from_geometry(\n",
        "                geometry=Polygon(\n",
        "                    [\n",
        "                        [\n",
        "                            (pt['x'], pt['y'])\n",
        "                            for pt in [*subpolygon, subpolygon[0]]\n",
        "                        ]\n",
        "                        for subpolygon in annotation[\"contour\"]\n",
        "                    ]\n",
        "                ),\n",
        "                height=100,\n",
        "                width=100,\n",
        "            )\n",
        "        )\n",
        "        for annotation in element[\"annotations\"]\n",
        "        if len(annotation[\"contour\"]) > 0\n",
        "    ]\n",
        "\n",
        "    # create and return Prediction\n",
        "    return Prediction(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "image_segmentations = [\n",
        "    {\n",
        "        \"path\": \"a/b/c/img6.png\", \n",
        "        \"annotations\": [\n",
        "            {\n",
        "                \"class_label\": \"dog\",\n",
        "                \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}]]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"path\": \"a/b/c/img7.png\", \n",
        "        \"annotations\": [\n",
        "            {\n",
        "                \"class_label\": \"cat\",\n",
        "                \"contour\": [[{\"x\": 97.2, \"y\": 40.2}, {\"x\": 33.33, \"y\": 44.3}, {\"x\": 10.9, \"y\": 18.7}]]\n",
        "            }\n",
        "        ]   \n",
        "    },\n",
        "    {\n",
        "        \"path\": \"a/b/c/img8.png\", \n",
        "        \"annotations\": [\n",
        "            {\n",
        "                \"class_label\": \"car\",\n",
        "                \"contour\": [[{\"x\": 10.0, \"y\": 15.5}, {\"x\": 20.9, \"y\": 50.2}, {\"x\": 25.9, \"y\": 28.4}], [{\"x\": 60.0, \"y\": 15.5}, {\"x\": 70.9, \"y\": 50.2}, {\"x\": 75.9, \"y\": 28.4}]]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "for element in image_segmentations:\n",
        "    # create prediction\n",
        "    prediction = create_prediction_from_image_segmentation_dict(element, datums_by_uid=datums_by_uid)\n",
        "\n",
        "    # add prediction to model\n",
        "    model.add_prediction(dataset, prediction)\n",
        "    \n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Text Classification Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prediction_from_text_classification_dict(element: dict, datums_by_uid:dict) -> Prediction:\n",
        "    \n",
        "    # get datum from dataset using filename\n",
        "    uid=Path(element[\"path\"]).stem\n",
        "    datum = datums_by_uid[uid]\n",
        "\n",
        "    # create Annotation\n",
        "    annotations = [\n",
        "        Annotation(\n",
        "            task_type=TaskType.CLASSIFICATION,\n",
        "            labels=[\n",
        "                Label(\n",
        "                    key=\"label\", \n",
        "                    value=label[\"label\"],\n",
        "                    score=label[\"score\"],\n",
        "                )\n",
        "                for label in element[\"annotations\"][0][\"sentiment\"][\"labels\"]\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # create and return Prediction\n",
        "    return Prediction(\n",
        "        datum=datum,\n",
        "        annotations=annotations,\n",
        "    )\n",
        "\n",
        "text_classifications = [\n",
        "    {\n",
        "        \"path\": \"a/b/c/text1.txt\",\n",
        "        \"annotations\": [\n",
        "            {\"sentiment\": \n",
        "                {\n",
        "                    \"context\": \"Is the content of this product review postive?\", \n",
        "                    \"labels\": [\n",
        "                        {\"label\": \"positive\", \"score\": 0.8},\n",
        "                        {\"label\": \"negative\", \"score\": 0.2}\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "for element in text_classifications:\n",
        "    # create prediction\n",
        "    prediction = create_prediction_from_text_classification_dict(element, datums_by_uid=datums_by_uid)\n",
        "\n",
        "    # add prediction to model\n",
        "    model.add_prediction(dataset, prediction)\n",
        "    \n",
        "    print(prediction)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finalizing Our Model\n",
        "\n",
        "Now that we've created all of our `Prediction` objects, we finalize our `Model` such that it's ready for evaluation. When finalizing our `Model`, we pass in the `Dataset` object that we want to link it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.finalize_inferences(dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Our Objects\n",
        "\n",
        "Now that we've finalized our `Dataset` and `Model`, we can explore all of the objects stored in Valor before running our evaluations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Client Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.get_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.get_models()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for datum in dataset.get_datums():\n",
        "    print(datum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for datum in dataset.get_datums():\n",
        "    groundtruth = dataset.get_groundtruth(datum)\n",
        "    print(groundtruth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for label in dataset.get_labels():\n",
        "    print(label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for datum in dataset.get_datums():\n",
        "    print(model.get_prediction(dataset, datum))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for label in model.get_labels():\n",
        "    print(label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating Performance\n",
        "\n",
        "Finally, we'll use our Valor abstractions to evaluate model performance. For more detailed, task-specific examples, see our follow-up notebooks at the links below:\n",
        "- [Tabular classification](https://github.com/Striveworks/valor/blob/main/examples/classification/tabular.ipynb)\n",
        "- [Object detection](https://github.com/Striveworks/valor/blob/main/examples/object-detection/coco-yolo.ipynb)\n",
        "- [Semantic segmentation](https://github.com/Striveworks/valor/blob/main/examples/semantic-segmentation/coco-yolo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_objdet = model.evaluate_detection(dataset)\n",
        "eval_objdet.wait_for_completion()\n",
        "eval_objdet.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Classifications\n",
        "\n",
        "Note that running the code below evaluates both our text classifications as well as our image classifications. If we only wanted to evaluate one type of classification task, we could use `evaluation_classification`'s `filters` argument to specify which type of labels to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_clf = model.evaluate_classification(dataset)\n",
        "eval_clf.wait_for_completion()\n",
        "eval_clf.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Segmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_semseg = model.evaluate_segmentation(dataset)\n",
        "eval_semseg.wait_for_completion()\n",
        "eval_semseg.metrics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env-valor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
