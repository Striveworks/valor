{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for Text Generation Metric Evaluation\n",
    "\n",
    "This notebook demonstrates example use cases for the Valor text generation metrics. The Valor text generation metrics can be used across a variety of tasks which typically, but not always, involve prompting an LLM to generate some text. Use cases include Query Answering, Retrieval Augmented Generation (which can be thought of as a subcase of Q&A), Summarization and Content Generation. \n",
    "\n",
    "Some of the metrics can be applied across different use cases. For example, the BLEU metric can be used to compare predictions (generated text) to groundtruth answers in the case of Q&A/RAG, and can also be used to compare predictions (generated text) to groundtruth summaries in the case of Summarization. Conversely, some of the metrics are specific to a use case, such as the ContextRecall metric for RAG or the Summarization score for Summarization. \n",
    "\n",
    "In all three use cases below, we generate answers using GPT3.5-turbo and evaluate those answers with a variety of metrics. For the text comparison metrics, we compare GPT3.5-turbo's responses to groundtruth Huggingface answers/summaries for the RAG and Summarization datasets. For the llm guided metrics (which include the RAG metrics, Summarization metrics and general text generation metrics), we are using GPT4o to evaluate the responses of GPT3.5-turbo. \n",
    "\n",
    "The first example is RAG for Q&A. We get a RAG dataset from HuggingFace, use Llama-Index and GPT3.5-turbo to generate answers, and evaluate those answers with text comparison metrics, RAG metrics and general text generation metrics.\n",
    "\n",
    "The second example is Summarization. We download a CNN news dataset from HuggingFace which includes groundtruth summaries. We ask GPT3.5-turbo to summarize those articles. Then we evaluate those summaries with text comparison metrics, summarization metrics and general text generation metrics.\n",
    "\n",
    "The third example is content generation. We manually create a few queries, each of a different query type (creative, education, professional). Then we evaluate the generated content with general text generation metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Valor API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from valor.enums import EvaluationStatus\n",
    "from valor import Annotation, Datum, Dataset, Model, GroundTruth, Client, Prediction, connect\n",
    "\n",
    "# Connect to Valor API.\n",
    "connect(\"http://0.0.0.0:8000\")\n",
    "client = Client()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "MISTRAL_API_KEY = os.environ[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case #1: RAG for Q&A\n",
    "\n",
    "## Download and Save the Corpus for the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = load_dataset(\"rag-datasets/mini_wikipedia\", \"text-corpus\")[\"passages\"]\n",
    "print(corpus_dataset)\n",
    "\n",
    "# For each passage in corpus_dataset, save that passage to a .txt file with the passage_id as the filename.\n",
    "for passage in corpus_dataset:\n",
    "    with open(f\"./rag_corpus/{passage[\"id\"]}.txt\", \"w\") as f:\n",
    "        f.write(passage[\"passage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries and get Answers with Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the query dataset. \n",
    "qa_dataset = load_dataset(\"rag-datasets/mini_wikipedia\", \"question-answer\")[\"test\"]\n",
    "qa_dataset = qa_dataset.shuffle(seed=42)\n",
    "print(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in the rag_corpus and builds an index.\n",
    "# Initially a query_engine, which will use GPT3.5-turbo by default with calls to OpenAI's API.\n",
    "# You must specify your OpenAI API key in the environment variable OPENAI_API_KEY for the below code to function. \n",
    "documents = SimpleDirectoryReader(\"rag_corpus\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample use\n",
    "response = query_engine.query(\"What country borders Argentina and Brazil?\")\n",
    "print(response)\n",
    "print(response.source_nodes)\n",
    "\n",
    "response = query_engine.query(\"What color is a penguin?\")\n",
    "print(response)\n",
    "print(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"rag_data.csv\"):\n",
    "    os.remove(\"rag_data.csv\")\n",
    "\n",
    "NUMBER_OF_RECORDS = 50\n",
    "\n",
    "with open(\"rag_data.csv\", mode=\"w\") as data_file:\n",
    "    data_writer = csv.writer(data_file, delimiter=\",\", quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow([\"query\", \"groundtruth\", \"prediction\", \"context_list\"])\n",
    "\n",
    "    for i in range(NUMBER_OF_RECORDS):\n",
    "        query = qa_dataset[i][\"question\"]\n",
    "        groundtruth = qa_dataset[i][\"answer\"]\n",
    "        print(f\"{i}: {query}\")\n",
    "\n",
    "        response_object = query_engine.query(query)\n",
    "        response = response_object.response\n",
    "        print(f\"response: {response}\")\n",
    "        context_list = []\n",
    "        for i in range(len(response_object.source_nodes)):\n",
    "            context_list.append(response_object.source_nodes[i].text)\n",
    "        data_writer.writerow([query, groundtruth, response, context_list])\n",
    "    \n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in Valor\n",
    "\n",
    "In this example, the RAG pipeline produces answers to the given queries by retrieving context and then generating answers based on the context and query. Groundtruth answers are also known for these queries. Both the datums (which contain the queries) and the groundtruths are added to the dataset. Then, the predictions are added to the model, which includes the answer and the context used to generate that answer. \n",
    "\n",
    "The metrics requested include some text comparison metrics (BLEU, ROUGE, LDistance), which do a text comparison between the generated answer and the groundtruth answer for the same datum. If the user only desires these metrics, then they do not need to include the context_list in the prediction and they do not need to supply the llm_api_parameters. \n",
    "\n",
    "However, other metrics are requested that use llm guided evaluation (Coherence, ContextRelevance, AnswerRelevance, Hallucination, Toxicity). To get these metrics, the user needs to specify an api url, an api key and a model name, along with any other model kwargs. Each of these metrics will use API calls to the specified LLM service to get information relevant for computing the desired metrics. Some of these metrics, such as Toxicity, do not require any context, so can be used with a Q&A model that does not use context. However other metrics, such as ContextRelevance, require context to be passed in the prediction, as that context is used for computing the metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset of queries, groundtruths and predictions. \n",
    "df = pd.read_csv(\"rag_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, build and finalize the dataset and model.\n",
    "dataset = Dataset.create(\n",
    "    name=\"rag_dataset\",\n",
    "    metadata={\n",
    "        \"hf_dataset_name\": \"rag-datasets/mini_wikipedia\",\n",
    "        \"hf_dataset_subset\": \"question-answer\",\n",
    "        \"hf_dataset_split\": \"test\",\n",
    "        \"shuffle_seed\": 42,\n",
    "        \"number_of_records\": 50,\n",
    "    }\n",
    ")\n",
    "model = Model.create(\n",
    "    name=\"rag_model\",\n",
    "    metadata={\n",
    "        \"embedding_model_name\": \"text-embedding-ada-002\", # When we ran llama-index above, it defaulted to text-embedding-ada-002.\n",
    "        \"llm_model_name\": \"GPT3.5-turbo\", # When we ran llama-index above, it defaulted to GPT3.5.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a list of datums\n",
    "datum_list = []\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "\n",
    "    datum_list.append(\n",
    "        Datum(\n",
    "            uid=f\"query{i}\",\n",
    "            text=row[\"query\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Build and finalize the dataset\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = datum_list[i]\n",
    "\n",
    "    dataset.add_groundtruth(\n",
    "        GroundTruth(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                # Perhaps you have multiple correct or good groundtruth answers to the query.\n",
    "                # The labels below are a trivial example, but you could have less trivial examples.\n",
    "                # For example, to the query \"When was the United States of America founded?\", you might \n",
    "                # consider both \"During the American Revolution\" or \"July 4th, 1776\" to be good answers.\n",
    "                Annotation(\n",
    "                    text=row[\"groundtruth\"],\n",
    "                    metadata={\"annotator\": \"Alice\"},\n",
    "                ),\n",
    "                Annotation(\n",
    "                    text=\"The answer is \" + row[\"groundtruth\"],\n",
    "                    metadata={\"annotator\": \"Bob\"},\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "dataset.finalize()\n",
    "\n",
    "# Build and finalize the model\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = datum_list[i]\n",
    "\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    text=row[\"prediction\"],\n",
    "                    context=ast.literal_eval(row[\"context_list\"]),\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "model.finalize_inferences(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT4o to evaluate GPT3.5-turbo's predictions across a variety of metrics. \n",
    "eval_job = model.evaluate_text_generation(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"AnswerRelevance\", \"BLEU\", \"Coherence\", \"ROUGE\"],\n",
    "    llm_api_params = {\n",
    "        \"client\":\"openai\",\n",
    "        \"api_key\":OPENAI_API_KEY,\n",
    "        \"data\":{\n",
    "            \"model\":\"gpt-4o\",\n",
    "            \"seed\":2024,\n",
    "        },\n",
    "    },    \n",
    "    metric_params={\n",
    "        \"BLEU\": {\n",
    "            \"weights\": [1, 0, 0, 0],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion() == EvaluationStatus.DONE\n",
    "\n",
    "# These are the computed metrics.\n",
    "eval_job.metrics\n",
    "\n",
    "# Here are some example metrics. These are all for query49 and were evaluated by GPT-4o.\n",
    "example_expected_metrics = [\n",
    "    {\n",
    "        \"type\": \"AnswerRelevance\", \n",
    "        \"value\": 1.0,\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\", \n",
    "            \"datum_uid\": \"query49\", \n",
    "            \"prediction\": \"The government is seeking to increase the media industry's GDP contribution to 3% by 2012.\"\n",
    "        }, \n",
    "    },\n",
    "    {\n",
    "        \"type\": \"BLEU\", \n",
    "        \"value\": 0.7058823529411765,\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\", \n",
    "            \"weights\": [1.0, 0.0, 0.0, 0.0], \n",
    "            \"datum_uid\": \"query49\", \n",
    "            \"prediction\": \"The government is seeking to increase the media industry's GDP contribution to 3% by 2012.\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"Coherence\", \n",
    "        \"value\": 3.0,\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\", \n",
    "            \"datum_uid\": \"query49\", \n",
    "            \"prediction\": \"The government is seeking to increase the media industry's GDP contribution to 3% by 2012.\"\n",
    "        }, \n",
    "    },\n",
    "    {\n",
    "        \"type\": \"ROUGE\", \n",
    "        \"value\": {\n",
    "            \"rouge1\": 0.7741935483870969, \n",
    "            \"rouge2\": 0.5384615384615385, \n",
    "            \"rougeL\": 0.7142857142857143, \n",
    "            \"rougeLsum\": 0.7142857142857143\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\", \n",
    "            \"datum_uid\": \"query49\", \n",
    "            \"prediction\": \"The government is seeking to increase the media industry's GDP contribution to 3% by 2012.\", \n",
    "            \"rouge_types\": [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], \n",
    "            \"use_stemmer\": False\n",
    "        }, \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case #2: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CNN Articles and get Summaries with GPT3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "\n",
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cnn dataset. \n",
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")[\"test\"]\n",
    "cnn_dataset = cnn_dataset.shuffle(seed=42)\n",
    "print(cnn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"summarization_data.csv\"):\n",
    "    os.remove(\"summarization_data.csv\")\n",
    "\n",
    "NUMBER_OF_RECORDS = 50\n",
    "\n",
    "instruction=\"You are a helpful assistant. Please summarize the following article in a few sentences.\"\n",
    "\n",
    "with open(\"summarization_data.csv\", mode=\"w\") as data_file:\n",
    "    data_writer = csv.writer(data_file, delimiter=\",\", quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow([\"text\", \"groundtruth\", \"prediction\"])\n",
    "\n",
    "    for i in range(NUMBER_OF_RECORDS):\n",
    "        article = cnn_dataset[i][\"article\"]\n",
    "        groundtruth = cnn_dataset[i][\"highlights\"]\n",
    "\n",
    "        print(f\"{i}: {groundtruth}\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "        ]\n",
    "\n",
    "        response_object = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages, seed=42\n",
    "        )\n",
    "        prediction = response_object.choices[0].message.content\n",
    "\n",
    "        print(f\"prediction: {prediction}\")\n",
    "        data_writer.writerow([article, groundtruth, prediction])\n",
    "    \n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in Valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset of queries, groundtruths and predictions. \n",
    "df = pd.read_csv(\"summarization_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, build and finalize the dataset and model.\n",
    "dataset = Dataset.create(\"summarization_dataset\")\n",
    "model = Model.create(\"summarization_model\")\n",
    "\n",
    "# Create a list of datums\n",
    "datum_list = []\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "\n",
    "    datum_list.append(\n",
    "        Datum(\n",
    "            uid=f\"article{i}\",\n",
    "            text=row[\"text\"],\n",
    "            metadata={\n",
    "                \"query\": \"Summarize this article in a few sentences.\", \n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# Build and finalize the dataset\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = datum_list[i]\n",
    "\n",
    "    dataset.add_groundtruth(\n",
    "        GroundTruth(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    text=row[\"groundtruth\"],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "dataset.finalize()\n",
    "\n",
    "# Build and finalize the model\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = datum_list[i]\n",
    "\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    text=row[\"prediction\"],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "model.finalize_inferences(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT4o to evaluate GPT3.5-turbo's predictions across a variety of metrics. \n",
    "eval_job = model.evaluate_text_generation(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE\", \"Coherence\"],\n",
    "    llm_api_params = {\n",
    "        \"client\":\"openai\",\n",
    "        \"api_key\":OPENAI_API_KEY,\n",
    "        \"data\":{\n",
    "            \"model\":\"gpt-4o\",\n",
    "            \"seed\":2024,\n",
    "        },\n",
    "    },   \n",
    "    metric_params={\n",
    "        \"BLEU\": {\n",
    "            \"weights\": [1, 0, 0, 0],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion() == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics\n",
    "\n",
    "example_expected_metrics = [\n",
    "    {\n",
    "        \"type\": \"BLEU\", \n",
    "        \"value\": 0.10000000000000002,\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\",  \n",
    "            \"datum_uid\": \"query40\", \n",
    "            \"prediction\": \"Diving ducks are heavier than dabbling ducks, which makes it more difficult for them to take off and fly.\",\n",
    "            \"weights\": [1.0, 0.0, 0.0, 0.0],\n",
    "        }, \n",
    "    },\n",
    "    {\n",
    "        \"type\": \"Coherence\", \n",
    "        \"value\": 5.0,\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\", \n",
    "            \"datum_uid\": \"query40\", \n",
    "            \"prediction\": \"Diving ducks are heavier than dabbling ducks, which makes it more difficult for them to take off and fly.\",\n",
    "        }, \n",
    "    },\n",
    "    {\n",
    "        \"type\": \"ROUGE\", \n",
    "        \"value\": {\n",
    "            \"rouge1\": 0.18181818181818182, \n",
    "            \"rouge2\": 0.09999999999999999, \n",
    "            \"rougeL\": 0.18181818181818182, \n",
    "            \"rougeLsum\": 0.18181818181818182\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"rag_dataset\", \n",
    "            \"datum_uid\": \"query40\", \n",
    "            \"prediction\": \"Diving ducks are heavier than dabbling ducks, which makes it more difficult for them to take off and fly.\", \n",
    "            \"rouge_types\": [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], \n",
    "            \"use_stemmer\": False\n",
    "            }, \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case #3: Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Example Content Generation Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Write about a haunted house from the perspective of the ghost.\",\n",
    "    \"Explain to an elementary school student how to do long multiplication with the example 43 times 22. The resulting answer should be 946.\",\n",
    "    \"Draft an email to a coworker explaining a project delay. Explain that the delay is due to funding cuts, which resulted in multiple employees being moved to different projects. Inform the coworker that the project deadline will have to be pushed back. Be apologetic and professional. Express eagerness to still complete the project as efficiently as possible.\",\n",
    "]\n",
    "\n",
    "query_metadata = [\n",
    "    {\n",
    "        \"request_type\": \"creative\",\n",
    "    },\n",
    "    {\n",
    "        \"request_type\": \"educational\",\n",
    "    },\n",
    "    {\n",
    "        \"request_type\": \"professional\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"content_generation_data.csv\"):\n",
    "    os.remove(\"content_generation_data.csv\")\n",
    "\n",
    "instruction=\"You are a helpful assistant.\"\n",
    "\n",
    "with open(\"content_generation_data.csv\", mode=\"w\") as data_file:\n",
    "    data_writer = csv.writer(data_file, delimiter=\",\", quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow([\"query\", \"prediction\"])\n",
    "\n",
    "    for i in range(len(queries)):\n",
    "        query = queries[i]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "        response_object = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages, seed=42\n",
    "        )\n",
    "        prediction = response_object.choices[0].message.content\n",
    "\n",
    "        print(f\"prediction: {prediction}\")\n",
    "        data_writer.writerow([query, prediction])\n",
    "    \n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in Valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset of queries and predictions.\n",
    "df = pd.read_csv(\"content_generation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, build and finalize the dataset and model.\n",
    "dataset = Dataset.create(\"content_generation_dataset\")\n",
    "model = Model.create(\"content_generation_model\")\n",
    "\n",
    "# Create a list of datums\n",
    "datum_list = []\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "\n",
    "    datum_list.append(\n",
    "        Datum(\n",
    "            uid=f\"query{i}\",\n",
    "            text=row[\"query\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Build and finalize the dataset\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = datum_list[i]\n",
    "\n",
    "    # There are no groundtruth annotations for content generation.\n",
    "    dataset.add_groundtruth(\n",
    "        GroundTruth(\n",
    "            datum=datum,\n",
    "            annotations=[],\n",
    "        )\n",
    "    )\n",
    "dataset.finalize()\n",
    "\n",
    "# Build and finalize the model\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = datum_list[i]\n",
    "\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    text=row[\"prediction\"],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "model.finalize_inferences(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT4o to evaluate GPT3.5-turbo's predictions across a variety of metrics. \n",
    "eval_job = model.evaluate_text_generation(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"Coherence\"],\n",
    "    llm_api_params = {\n",
    "        \"client\":\"openai\",\n",
    "        \"api_key\":OPENAI_API_KEY,\n",
    "        \"data\":{\n",
    "            \"model\":\"gpt-4o\",\n",
    "            \"seed\":2024,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion() == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics\n",
    "\n",
    "example_expected_metrics = [\n",
    "    {\n",
    "        \"value\": 5.0,\n",
    "        \"type\": \"Coherence\",\n",
    "        \"parameters\": {\n",
    "            \"dataset\": \"content_generation_dataset\",\n",
    "            \"datum_uid\": \"query2\",\n",
    "            \"prediction\": \"\"\"Subject: Project Delay Due to Funding Cuts\n",
    "\n",
    "Dear [Coworker's Name],\n",
    "\n",
    "I hope this message finds you well. I am writing to update you on the status of our project and unfortunately, convey some disappointing news regarding a delay in its completion.\n",
    "\n",
    "Due to recent funding cuts within our department, our project team has been significantly affected. Several team members, including myself, have been relocated to work on other projects to address the shifting priorities resulting from the budget constraints.\n",
    "\n",
    "As a consequence of these unexpected changes, it is with regret that I must inform you that the original deadline for our project will need to be extended. I understand the inconvenience that this may cause, and I sincerely apologize for any inconvenience this delay may bring to you and your plans.\n",
    "\n",
    "Rest assured that despite this setback, I am fully committed to ensuring that we still deliver the project with utmost efficiency and quality. I am exploring all possible avenues to mitigate the delay and work towards completing our project in a timely manner.\n",
    "\n",
    "I appreciate your understanding and patience during this challenging time. Your ongoing support and collaboration are invaluable as we navigate through this situation together. If you have any concerns or questions, please do not hesitate to reach out to me.\n",
    "\n",
    "Thank you for your understanding, and I look forward to working with you to successfully finalize our project.\n",
    "\n",
    "Warm regards,\n",
    "\n",
    "[Your Name]\"\"\",\n",
    "        },\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velour_api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
