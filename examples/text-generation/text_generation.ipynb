{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for Text Generation Metric Evaluation\n",
    "\n",
    "This notebook demonstrates examples use cases for the Valor text generation task type. The Valor text generation task type can be used across a variety of tasks which typically, but not always, involve prompting an LLM to generate some text. Use cases include Query Answering, Retrieval Augmented Generation (which can be thought of as a subcase of Q&A), Summarization and Content Generation. Not all of the text generation metrics make sense for all of these use cases, however these use cases share many common metrics. For example, the BLEU metric can be used to compare to groundtruth answers in the case of Q&A/RAG, and can also be used to compare to groundtruth summaries in the case of Summarization. For another example, both the Coherence and Naturalness metrics can be used to evaluate the quality of generated text across all of these use cases. Some of these use cases also have use case specific metrics, such as ContextRecall for RAG or the Summarization score for Summarization. \n",
    "\n",
    "The first example is RAG for Q&A. We get a RAG dataset from HuggingFace, use Llama-Index and GPT3.5 to generate answers, and evaluate those answers with text comparison metrics, RAG metrics and general text generation metrics.\n",
    "\n",
    "The second example summarization. (TODO get some actual data). Then we evaluate those summaries with text comparison metrics, summarization metrics and general text generation metrics.\n",
    "\n",
    "The third example is content generation. (TODO get some actual content generation data). Then we evaluate the generated content with general text generation metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case #1: RAG for Q&A\n",
    "\n",
    "## Download and Save the Corpus for the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = load_dataset(\"rag-datasets/mini_wikipedia\", \"text-corpus\")[\"passages\"]\n",
    "print(corpus_dataset)\n",
    "\n",
    "# For each passage in corpus_dataset, save that passage to a .txt file with the passage_id as the filename.\n",
    "for passage in corpus_dataset:\n",
    "    with open(f'./rag_corpus/{passage[\"id\"]}.txt', 'w') as f:\n",
    "        f.write(passage['passage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Queries and get Answers with Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the query dataset. \n",
    "qa_dataset = load_dataset(\"rag-datasets/mini_wikipedia\", \"question-answer\")[\"test\"]\n",
    "qa_dataset = qa_dataset.shuffle(seed=42)\n",
    "print(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in the rag_corpus and builds an index.\n",
    "# Initially a query_engine, which will use GPT3.5 by default with calls to OpenAI's API.\n",
    "# You must specify your OpenAI API key in the environment variable OPENAI_API_KEY for the below code to function. \n",
    "documents = SimpleDirectoryReader(\"rag_corpus\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample use\n",
    "response = query_engine.query(\"What country borders Argentina and Brazil?\")\n",
    "print(response)\n",
    "print(response.source_nodes)\n",
    "\n",
    "response = query_engine.query(\"What color is a penguin?\")\n",
    "print(response)\n",
    "print(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('rag_data.csv'):\n",
    "    os.remove('rag_data.csv')\n",
    "\n",
    "NUMBER_OF_RECORDS = 50\n",
    "\n",
    "with open('rag_data.csv', mode='w') as data_file:\n",
    "    data_writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['query', 'groundtruth', 'prediction', 'context_list'])\n",
    "\n",
    "    for i in range(NUMBER_OF_RECORDS):\n",
    "        query = qa_dataset[i]['question']\n",
    "        groundtruth = qa_dataset[i]['answer']\n",
    "        print(f\"{i}: {query}\")\n",
    "\n",
    "        response_object = query_engine.query(query)\n",
    "        response = response_object.response\n",
    "        print(f\"response: {response}\")\n",
    "        context_list = []\n",
    "        for i in range(len(response_object.source_nodes)):\n",
    "            context_list.append(response_object.source_nodes[i].text)\n",
    "        data_writer.writerow([query, groundtruth, response, context_list])\n",
    "    \n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in Valor\n",
    "\n",
    "In this example, the RAG pipeline produces answers to the given queries by retrieving context and then generating answers based on the context and query. Groundtruth answers are also known for these queries. Both the datums (which contain the queries) and the groundtruths are added to the dataset. Then, the predictions are added to the model, which includes the answer and the context used to generate that answer. \n",
    "\n",
    "The metrics requested include some text comparison metrics (BLEU, ROUGE1, LDistance), which do a text comparison between the generated answer and the groundtruth answer for the same datum. If the user only desires these metrics, then they do not need to include the context_list in the prediction and they do not need to supply the llm related parameters. \n",
    "\n",
    "However, other metrics are requested that use llm guided evaluation (Coherence, ContextRelevance, AnswerRelevance, Hallucination, Toxicity). To get these metrics, the user needs to specify an api url, an api key and a model name, along with any other model kwargs. Each of these metrics will use API calls to the specified LLM service to get information relevant for computing the desired metrics. Some of these metrics, such as Toxicity, do not require any context, so can be used with a Q&A model that does not use context. However other metrics, such as ContextRelevance, require context to be passed in the prediction, as that context is used for computing the metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from valor.enums import TaskType, EvaluationStatus\n",
    "from valor import Annotation, Datum, Dataset, Model, GroundTruth, Label, Client, Prediction, viz, connect\n",
    "\n",
    "# Connect to Valor API.\n",
    "connect(\"http://0.0.0.0:8000\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset of queries, groundtruths and predictions. \n",
    "df = pd.read_csv('rag_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version with Labels and TEXT_GENERATION task type - my current preferred implementation\n",
    "It seems undesirable to use labels when the labels aren't really doing anything, however the label_ids are useful as identifiers for the resulting metrics. The labels also make it so we don't have to add a bunch of use case specific attributes for each use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create('rag_dataset')\n",
    "model = Model.create('rag_model')\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "\n",
    "    # All queries are added to the dataset as Datum objects. \n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['query'],\n",
    "    )\n",
    "    dataset.add_datum(datum)\n",
    "\n",
    "    # Suppose that only the first half of the queries have groundtruth answers. \n",
    "    # The text comparison metrics can only be computed for queries that have groundtruths. \n",
    "    if i < len(df)/2:\n",
    "        dataset.add_groundtruth(\n",
    "            GroundTruth(\n",
    "                datum=datum,\n",
    "                annotations=[\n",
    "                    # Perhaps you have multiple correct or good groundtruth answers to the query.\n",
    "                    # The labels below are a trivial example, but you could have less trivial examples.\n",
    "                    # For example, to the query \"When was the United States of America founded?\", you might \n",
    "                    # consider both \"During the American Revolution\" or \"July 4th, 1776\" to be good answers.\n",
    "                    Annotation(\n",
    "                        task_type=TaskType.TEXT_GENERATION,\n",
    "                        labels=[\n",
    "                            Label(key=\"answer\", value=row['groundtruth']),\n",
    "                            Label(key=\"answer\", value=\"The answer is \" + row['groundtruth']),\n",
    "                        ],\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['query'],\n",
    "    )\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.TEXT_GENERATION,\n",
    "                    labels=[Label(key=\"answer\", value=row['prediction'])],\n",
    "                    context_list=row['context_list'],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_text_generation(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE1\", 'LDistance', 'Coherence', 'ContextRelevance', 'AnswerRelevance', 'Hallucination', 'Toxicity'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics\n",
    "\n",
    "# What might the expected metrics look like? \n",
    "# BLEU metric:\n",
    "# {\n",
    "#    \"value\": <BLEU_SCORE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"BLEU\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# }\n",
    "# Coherence metric:\n",
    "# {\n",
    "#    \"value\": <COHERENCE_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Coherence\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } \n",
    "# ContextRelevance metric:\n",
    "# {\n",
    "#    \"value\": <CONTEXT_RELEVANCE_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"ContextRelevance\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it would look if we had use case specific evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_job_text_comp = model.evaluate_text_comparison(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE1\", 'LDistance'],\n",
    "    # metric_kwargs=None,\n",
    ")\n",
    "assert eval_job_text_comp.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "eval_job_text_comp.metrics\n",
    "# BLEU metric:\n",
    "# {\n",
    "#    \"value\": <BLEU_SCORE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"BLEU\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# }\n",
    "\n",
    "\n",
    "eval_job_text_gen = model.evaluate_text_generation( # or evaluate_llm_guided_metrics?\n",
    "    dataset,\n",
    "    metrics_to_return=['Coherence', 'Hallucination', 'Toxicity'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "assert eval_job_text_gen.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "eval_job_text_gen.metrics\n",
    "# Coherence metric:\n",
    "# {\n",
    "#    \"value\": <COHERENCE_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Coherence\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } \n",
    "\n",
    "\n",
    "eval_job_llm_guided = model.evaluate_rag(\n",
    "    dataset,\n",
    "    metrics_to_return=['ContextRelevance', 'AnswerRelevance'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "assert eval_job_llm_guided.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "eval_job_llm_guided.metrics\n",
    "# ContextRelevance metric:\n",
    "# {\n",
    "#    \"value\": <CONTEXT_RELEVANCE_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"ContextRelevance\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version without Labels and QUERY_ANSWERING task type\n",
    "Potentially looks better to the user but it's awkward to put the query and the answer in the metric. In this implementation, each of the use cases QUERY_ANSWERING, SUMMARIZATION, CONTENT_GENERATION would have their own task type, as each will use different datum and annotation attributes. I don't think this is strictly necessary, but thought that it might make more sense this way if we are adding a bunch of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create('rag_dataset')\n",
    "model = Model.create('rag_model')\n",
    "\n",
    "for i in range(len(df)/2):\n",
    "    row = df.iloc[i]\n",
    "\n",
    "    # All queries are added to the dataset as Datum objects. \n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        query=row['query'],\n",
    "    )\n",
    "    dataset.add_datum(datum)\n",
    "\n",
    "    # Suppose that only the first half of the queries have groundtruth answers. \n",
    "    # Some of the below metrics can only be computed for datums that have groundtruths.\n",
    "    if i < len(df)/2:\n",
    "        dataset.add_groundtruth(\n",
    "            GroundTruth(\n",
    "                datum=datum,\n",
    "                annotations=[\n",
    "                    # Perhaps you have multiple correct or good groundtruth answers to the query.\n",
    "                    # The labels below are a trivial example, but you could have less trivial examples.\n",
    "                    # For example, to the query \"When was the United States of America founded?\", you might \n",
    "                    # consider both \"During the American Revolution\" or \"July 4th, 1776\" to be good answers.\n",
    "                    # For another example, suppose we have a RAG pipeline\n",
    "                    Annotation(\n",
    "                        task_type=TaskType.QUERY_ANSWERING,\n",
    "                        answer=row['groundtruth'],\n",
    "                    ),\n",
    "                    Annotation(\n",
    "                        task_type=TaskType.QUERY_ANSWERING,\n",
    "                        answer=\"The answer is \" + row['groundtruth'],\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        query=row['query'],\n",
    "    )\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.QUERY_ANSWERING,\n",
    "                    answer=row['prediction'],\n",
    "                    context_list=row['context_list'],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_query_answering(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE1\", 'LDistance', 'Coherence', 'ContextRelevance', 'AnswerRelevance', 'Hallucination', 'Toxicity'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics\n",
    "\n",
    "# What might the expected metrics look like? \n",
    "# BLEU metric:\n",
    "# {\n",
    "#    \"value\": <BLEU_SCORE>,\n",
    "#    \"type\": \"BLEU\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "#    \"params\": {\n",
    "#         \"query\": <QUERY>,\n",
    "#         \"answer\": <ANSWER>,\n",
    "#    }\n",
    "# }\n",
    "# Coherence metric:\n",
    "# {\n",
    "#    \"value\": <COHERENCE_VALUE>,\n",
    "#    \"type\": \"Coherence\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "#    \"params\": {\n",
    "#        \"query\": <QUERY>,\n",
    "#        \"answer\": <ANSWER>,\n",
    "#    }\n",
    "# } \n",
    "# ContextRelevance metric:\n",
    "# {\n",
    "#    \"value\": <CONTEXT_RELEVANCE_VALUE>,\n",
    "#    \"type\": \"ContextRelevance\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "#    \"params\": {\n",
    "#        \"query\": <QUERY>,\n",
    "#        \"answer\": <ANSWER>,\n",
    "#    }\n",
    "# } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case #2: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version with Labels and TEXT_GENERATION task type - my current preferred implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create('summarization_dataset')\n",
    "model = Model.create('summarization_model')\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['article'],\n",
    "    )\n",
    "    dataset.add_datum(datum)\n",
    "    dataset.add_groundtruth(\n",
    "        GroundTruth(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.TEXT_GENERATION,\n",
    "                    labels=[Label(key=\"summary\", value=row['groundtruth'])],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['article'],\n",
    "    )\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.TEXT_GENERATION,\n",
    "                    labels=[Label(key=\"summary\", value=row['prediction'])],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_text_generation(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE1\", 'LDistance', 'Toxicity', 'Summarization'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics\n",
    "\n",
    "# What might the expected metrics look like? \n",
    "# BLEU metric:\n",
    "# {\n",
    "#    \"value\": <BLEU_SCORE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"BLEU\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# }\n",
    "# Toxicity metric:\n",
    "# {\n",
    "#    \"value\": <TOXICITY_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Toxicity\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } \n",
    "# Summarization metric:\n",
    "# {\n",
    "#    \"value\": <SUMMARIZATION_SCORE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Summarization\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it would look if we had use case specific evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_job_text_comp = model.evaluate_text_comparison(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE1\", 'LDistance'],\n",
    ")\n",
    "assert eval_job_text_comp.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "eval_job_text_comp.metrics\n",
    "# BLEU metric:\n",
    "# {\n",
    "#    \"value\": <BLEU_SCORE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"BLEU\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# }\n",
    "\n",
    "\n",
    "eval_job_text_gen = model.evaluate_text_generation( # or evaluate_llm_guided_metrics?\n",
    "    dataset,\n",
    "    metrics_to_return=['Toxicity'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "assert eval_job_text_gen.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "eval_job_text_gen.metrics\n",
    "# Toxicity metric:\n",
    "# {\n",
    "#    \"value\": <TOXICITY_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Toxicity\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } \n",
    "\n",
    "\n",
    "eval_job_summ = model.evaluate_summarization(\n",
    "    dataset,\n",
    "    metrics_to_return=['Summarization'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "assert eval_job_summ.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "eval_job_summ.metrics\n",
    "# Summarization metric:\n",
    "# {\n",
    "#    \"value\": <SUMMARIZATION_SCORE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Summarization\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version without Labels and SUMMARIZATION task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create('summarization_dataset')\n",
    "model = Model.create('summarization_model')\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['article'],\n",
    "    )\n",
    "    dataset.add_datum(datum)\n",
    "    dataset.add_groundtruth(\n",
    "        GroundTruth(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.SUMMARIZATION,\n",
    "                    summary=row['groundtruth'],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['article'],\n",
    "    )\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.SUMMARIZATION,\n",
    "                    summary=row['summary'],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_summarization(\n",
    "    dataset,\n",
    "    metrics_to_return=[\"BLEU\", \"ROUGE1\", 'LDistance', 'Toxicity', 'Summarization'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case #3: Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in Valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create('content_generation_dataset')\n",
    "model = Model.create('content_generation_model')\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['prompt'],\n",
    "    )\n",
    "    dataset.add_datum(datum)\n",
    "    # There are no groundtruths for content generation.\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    datum = Datum(\n",
    "        uid=i,\n",
    "        text=row['prompt'],\n",
    "    )\n",
    "    model.add_prediction(\n",
    "        dataset, \n",
    "        Prediction(\n",
    "            datum=datum,\n",
    "            annotations=[\n",
    "                Annotation(\n",
    "                    task_type=TaskType.TEXT_GENERATION,\n",
    "                    labels=[Label(key=\"generated_content\", value=row['output'])],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.finalize_inferences(dataset)\n",
    "\n",
    "eval_job = model.evaluate_text_generation(\n",
    "    dataset,\n",
    "    metrics_to_return=['Coherence', 'Toxicity', 'Bias'],\n",
    "    llm_model='gpt-3.5-turbo',\n",
    "    api_url=\"https://api.openai.com/v1/chat/completions\", \n",
    "    # api_key=None, # If no key is specified, uses OPENAI_API_KEY or LLM_API_KEY environment variable\n",
    "    # llm_model_kwargs=None,\n",
    ")\n",
    "\n",
    "assert eval_job.wait_for_completion(timeout=30) == EvaluationStatus.DONE\n",
    "\n",
    "eval_job.metrics\n",
    "\n",
    "# What might the expected metrics look like? \n",
    "# Toxicity metric:\n",
    "# {\n",
    "#    \"value\": <TOXICITY_VALUE>,\n",
    "#    \"label_id\": <PREDICTION_LABEL_ID>,\n",
    "#    \"type\": \"Toxicity\",\n",
    "#    \"evaluation_id\": <EVALUATION_ID>,\n",
    "# } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to break things into different task types, we might still consider content generation as part of Q&A. Content Generation fits the same format as Q&A, except that a user should not use text comparison metrics to compare generated content to a groundtruth. The only way to prevent the user from uploading groundtruths and using the text comparison method would be to have a separate task type for content generation. But if the user actually has groundtruths that they want to compare to, then why should we bother restricting them in this way? Maybe there is a content generation use case that we are not thinking about right now where a user might actually want to compare generated content to some other text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velour_api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
