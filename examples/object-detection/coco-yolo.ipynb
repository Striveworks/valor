{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "38ec8ecc",
      "metadata": {},
      "source": [
        "# Object Detection Example\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we'll walk-through a detailed example of how you can use Velour to evaluate object detections made on [the COCO Panoptic dataset](https://cocodataset.org/#home). We'll use Ultralytics' `YOLOv8` model to predict what objects exist in various COCO photographs.\n",
        "\n",
        "For a conceptual introduction to Velour, [check out our project overview](https://striveworks.github.io/velour/). For a higher-level example notebook, [check out our \"Getting Started\" notebook](https://github.com/Striveworks/velour/blob/main/examples/getting_started.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9b26ec",
      "metadata": {},
      "source": [
        "## Defining Our Datasets\n",
        "\n",
        "We start by fetching our dataset and uploading it to Velour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d0a509-7500-44ba-b951-3566d4a4fac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from velour import Client, Model, Annotation, Prediction, Label\n",
        "from velour.enums import TaskType\n",
        "from velour.viz import create_combined_segmentation_mask\n",
        "\n",
        "import integrations.coco_integration as coco\n",
        "import integrations.yolo_integration as yolo\n",
        "\n",
        "import ultralytics\n",
        "\n",
        "# connect to Velour API\n",
        "client = Client(\"http://localhost:8000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ddd815",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the dataset in Velour; see the scripts in `integrations/` for code\n",
        "velour_dataset = coco.create_dataset_from_coco_panoptic(\n",
        "    client, \n",
        "    destination=Path(\"./\").absolute().parent / Path(\"coco\"),\n",
        "    limit=2, \n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "40af5eec",
      "metadata": {},
      "source": [
        "## Defining Our Model\n",
        "\n",
        "With our `Dataset` in Velour, we're ready to create our `Model` object and add `Predictions` to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2750a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the model in Velour. note that we can use any name we'd like\n",
        "velour_model = Model(client, \"yolov8n-seg\")\n",
        "\n",
        "inference_engine = ultralytics.YOLO(f\"{velour_model.name}.pt\")\n",
        "\n",
        "for datum in tqdm(velour_dataset.get_datums()):\n",
        "\n",
        "    image = coco.download_image(datum)\n",
        "\n",
        "    results = inference_engine(image, verbose=False)\n",
        "\n",
        "    # convert result into Velour Bounding Box prediction\n",
        "    bbox_prediction = yolo.parse_detection_into_bounding_box(\n",
        "        results,            # raw inference\n",
        "        datum=datum,        # velour datum\n",
        "        label_key='name',   # label_key override\n",
        "    )\n",
        "\n",
        "    # convert result into Velour Raster prediction\n",
        "    raster_prediction = yolo.parse_detection_into_raster(\n",
        "        results,            # raw inference\n",
        "        datum=datum,        # velour datum\n",
        "        label_key='name',   # label_key override\n",
        "    )\n",
        "\n",
        "    # add predictions to the model\n",
        "    velour_model.add_prediction(bbox_prediction)\n",
        "    velour_model.add_prediction(raster_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a21ced7",
      "metadata": {},
      "source": [
        "Lastly, we finalize our `Model` to get it ready for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f19430",
      "metadata": {},
      "outputs": [],
      "source": [
        "velour_model.finalize_inferences(velour_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "159693f4",
      "metadata": {},
      "source": [
        "## Exploring Our Dataset\n",
        "\n",
        "Before we evaluate our results, let's check out the metadata stored in Velour. Below, we show an example of a COCO image (in this case, the image we added using UID '139')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14939a3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "groundtruth_139 = velour_dataset.get_groundtruth('139')\n",
        "coco.download_image(groundtruth_139.datum)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87516a4",
      "metadata": {},
      "source": [
        "Next, we overlay a segmentation mask over the image to show all of the objects we want to be able to detect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737e3e25-aa4a-4934-ad5f-da770bffa44a",
      "metadata": {},
      "outputs": [],
      "source": [
        "instance_mask, instance_legend = create_combined_segmentation_mask(\n",
        "    [groundtruth_139], \n",
        "    label_key=\"name\",\n",
        "    task_type=TaskType.DETECTION,\n",
        ")\n",
        "\n",
        "instance_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd651c08-c554-4fb2-9dab-4b44679c500d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print the color code for the above segmentations\n",
        "for k, v in instance_legend.items():\n",
        "    print(k)\n",
        "    display(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c33822",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3e8e7aab",
      "metadata": {},
      "source": [
        "## Evaluating Performance\n",
        "\n",
        "With our `Dataset` and `Model` defined, we're ready to evaluate our performance and display the results. Note that we use the `wait_for_completion` method since all evaluations run as a postgres `BackgroundTask`; this method ensures that the evaluation finishes before we display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f5d932",
      "metadata": {},
      "outputs": [],
      "source": [
        "eval1 = velour_model.evaluate_detection(velour_dataset)\n",
        "eval1.wait_for_completion()\n",
        "\n",
        "eval1.results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c63f4491",
      "metadata": {},
      "source": [
        "### Evaluating with Filters\n",
        "\n",
        "Sometimes, we may only want to calculate metrics for a subset of our data (i.e., we may only want to see how well our model performed at a specific type of detection). To accomplish this task, we can use the `filters` param of `evaluation_detection` to specify what types of data to evaluate performance for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aebd5f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# see how well we did at detecting people in images that have segmentation areas <=2000 pixels\n",
        "eval2 = velour_model.evaluate_detection(\n",
        "    velour_dataset,\n",
        "    filters=[\n",
        "        Label.label == Label(key='name', value='person'),\n",
        "        Annotation.geometric_area <= 2000\n",
        "    ]\n",
        ")\n",
        "eval2.wait_for_completion()\n",
        "eval2.results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env-velour",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
