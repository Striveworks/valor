{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "38ec8ecc",
      "metadata": {},
      "source": [
        "# Object Detection Example\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we'll walk through a detailed example of how you can use Valor to evaluate object detections made on [the COCO Panoptic dataset](https://cocodataset.org/#home). We'll use Ultralytics' `YOLOv8` model to predict what objects exist in various COCO photographs and compare performance between bounding box and image segmentation results.\n",
        "\n",
        "For a conceptual introduction to Valor, [check out our project overview](https://striveworks.github.io/valor/). For a higher-level example notebook, [check out our \"Getting Started\" notebook](https://github.com/Striveworks/valor/blob/main/examples/getting_started.ipynb).\n",
        "\n",
        "Before using this notebook, please ensure that the Valor service is running on your machine (for start-up instructions, [click here](https://striveworks.github.io/valor/getting_started/)). To connect to a non-local instance of Valor, update `client = Client(\"http://0.0.0.0:8000\")` in the first code block to point to the correct URL."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9b26ec",
      "metadata": {},
      "source": [
        "## Defining Our Datasets\n",
        "\n",
        "We start by fetching our dataset and uploading it to Valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a4d0a509-7500-44ba-b951-3566d4a4fac1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully connected to host at http://localhost:8000/\n"
          ]
        }
      ],
      "source": [
        "import ultralytics\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from valor import Client, Dataset, Model, Annotation, Label, Filter, connect\n",
        "from valor.enums import TaskType, AnnotationType\n",
        "from valor.schemas import And, Eq\n",
        "from valor.viz import create_combined_segmentation_mask\n",
        "\n",
        "# connect to Valor API\n",
        "connect(\"http://localhost:8000\")\n",
        "client = Client()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a28f5e66",
      "metadata": {},
      "source": [
        "The modules included in `./integrations` are helper modules that demonstrate how to ingest datasets and model inferences into Valor. The depth of each integration varies depending on the use case. \n",
        "\n",
        "The `coco_integration` is designed to download, extract, and upload all in one command as you are starting off with all the the data. \n",
        "\n",
        "The `yolo_integration` is much simpler; it is a collection of parser functions that convert YOLO model results into Valor types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "907e3e54",
      "metadata": {},
      "outputs": [],
      "source": [
        "import integrations.coco_integration as coco\n",
        "import integrations.yolo_integration as yolo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "db64b6c6",
      "metadata": {},
      "source": [
        "# Defining Our Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "94798123",
      "metadata": {},
      "source": [
        "This block utilizes `get_instance_groundtruths` from `integrations/coco_integration.py` to download, extract, and upload the COCO Panoptic validation dataset to Valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "89ddd815",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gt_objdet_coco_bbox.jsonl already exists locally.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create the dataset in Valor\n",
        "valor_dataset_bbox = Dataset.create(\"coco-box\")\n",
        "\n",
        "# retrieve chunks containing valor.Groundtruth objects and upload them.\n",
        "for chunk in coco.get_instance_groundtruths(\n",
        "    dtype=AnnotationType.BOX,\n",
        "    chunk_size=100,\n",
        "    limit=5000,\n",
        "    from_cache=True,\n",
        "):\n",
        "    valor_dataset_bbox.add_groundtruths(chunk)\n",
        "\n",
        "# finalize the data\n",
        "valor_dataset_bbox.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b525b611",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the dataset in Valor\n",
        "valor_dataset_raster = Dataset.create(\"coco-raster\")\n",
        "\n",
        "# retrieve chunks containing valor.Groundtruth objects and upload them.\n",
        "for chunk in coco.get_instance_groundtruths(\n",
        "    dtype=AnnotationType.RASTER,\n",
        "    chunk_size=100,\n",
        "    limit=100,\n",
        "    from_cache=True,\n",
        "):\n",
        "    valor_dataset_raster.add_groundtruths(chunk)\n",
        "\n",
        "# finalize the data\n",
        "valor_dataset_raster.finalize()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "40af5eec",
      "metadata": {},
      "source": [
        "## Defining Our Model\n",
        "\n",
        "With our `Dataset` in Valor, we're ready to create our `Model` object and add `Predictions` to it. This block utilizes `get_instance_predictions` from `integrations/yolo_integration.py` to run inferences over the COCO Panoptic validation dataset. To save on time, the default behavior of this function is to draw from a cache of precomputed inferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2750a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the model in Valor. note that we can use any name we'd like.\n",
        "valor_model = Model.create(\"yolov8n\")\n",
        "\n",
        "# retrieve chunks containing bounding box predictions and upload them.\n",
        "for chunk in yolo.get_instance_predictions(\n",
        "    dtype=AnnotationType.BOX,\n",
        "    coco_uids=[datum.uid for datum in valor_dataset_bbox.get_datums()],\n",
        "    chunk_size=200,\n",
        "    from_cache=True,\n",
        "):\n",
        "    valor_model.add_predictions(valor_dataset_bbox, chunk)\n",
        "\n",
        "# retrieve chunks containing bitmask predictions and upload them.\n",
        "for chunk in yolo.get_instance_predictions(\n",
        "    dtype=AnnotationType.RASTER,\n",
        "    coco_uids=[datum.uid for datum in valor_dataset_raster.get_datums()],\n",
        "    chunk_size=5,\n",
        "    limit=5,\n",
        "    from_cache=True,\n",
        "):\n",
        "    valor_model.add_predictions(valor_dataset_raster, chunk)\n",
        "\n",
        "# finalize the inferences for a dataset\n",
        "valor_model.finalize_inferences(valor_dataset_bbox)\n",
        "valor_model.finalize_inferences(valor_dataset_raster)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "159693f4",
      "metadata": {},
      "source": [
        "## Exploring Our Dataset\n",
        "\n",
        "Before we evaluate our results, let's check out what's stored in Valor. Below, we show an example of a COCO image (in this case, the image we added using UID '139')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14939a3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "groundtruth_139 = valor_dataset_raster.get_groundtruth('139')\n",
        "assert groundtruth_139\n",
        "coco.download_image(groundtruth_139.datum)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c87516a4",
      "metadata": {},
      "source": [
        "Next, we visualize multiple segmentation masks to show all of the objects we want to be able to detect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737e3e25-aa4a-4934-ad5f-da770bffa44a",
      "metadata": {},
      "outputs": [],
      "source": [
        "instance_mask, instance_legend = create_combined_segmentation_mask(\n",
        "    groundtruth_139, \n",
        "    label_key=\"name\",\n",
        "    filter_on_instance_segmentations=True,\n",
        ")\n",
        "\n",
        "instance_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd651c08-c554-4fb2-9dab-4b44679c500d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print the color code for the above segmentations\n",
        "for k, v in instance_legend.items():\n",
        "    print(k)\n",
        "    display(v)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3e8e7aab",
      "metadata": {},
      "source": [
        "## Evaluating Performance\n",
        "\n",
        "With our `Dataset` and `Model` defined, we're ready to evaluate our performance and display the results. Note that we use the `wait_for_completion` method since all evaluations run as background tasks; this method ensures that the evaluation finishes before we display the results.\n",
        "\n",
        "Sometimes, we may only want to calculate metrics for a subset of our data (i.e., we may only want to see how well our model performed at a specific type of detection). To accomplish this task, we can use the `filters` parameter of `evaluation_detection` to specify what types of data to evaluate performance for.\n",
        "\n",
        "We will be running and comparing two different evaluations investigating the performance difference of YOLOv8's bounding box and raster outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f5d932",
      "metadata": {},
      "outputs": [],
      "source": [
        "# bounding box evaluation\n",
        "eval_bbox = valor_model.evaluate_detection(\n",
        "    valor_dataset_bbox,\n",
        "    filters=Filter(\n",
        "        labels=(\n",
        "            Label.key == \"name\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "eval_bbox.wait_for_completion()\n",
        "eval_bbox.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e55b1f1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# raster evaluation\n",
        "eval_raster = valor_model.evaluate_detection(\n",
        "    valor_dataset_raster,\n",
        "    filters=Filter(\n",
        "        labels=(\n",
        "            Label.key == \"name\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "eval_raster.wait_for_completion()\n",
        "eval_raster.metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "904e221b",
      "metadata": {},
      "source": [
        "We can compare performance by comparing our results in pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4212ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bdf = eval_bbox.to_dataframe((\"annotation type\", \"bbox\"))\n",
        "rdf = eval_raster.to_dataframe((\"annotation type\", \"raster\"))\n",
        "pd.concat([bdf, rdf], axis=1, names=[\"bbox\", \"raster\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "12373159",
      "metadata": {},
      "source": [
        "## Evaluating based on object size.\n",
        "\n",
        "Filters are not limited to annotation type and label keys as shown above. We can also define filters for a pixel-wise geometric area that will help us test the performance of objects that fall within certain size ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc78dd1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "lower_bound = 30000\n",
        "upper_bound = 100000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d51f103c",
      "metadata": {},
      "source": [
        "### Small Object Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1453301",
      "metadata": {},
      "outputs": [],
      "source": [
        "# bounding box evaluation\n",
        "eval_bbox_small = valor_model.evaluate_detection(\n",
        "    valor_dataset_bbox,\n",
        "    filters=Filter(\n",
        "        annotations=And(\n",
        "            Label.key == \"name\",\n",
        "            Annotation.bounding_box.area < lower_bound,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "eval_bbox_small.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef904d0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# raster evaluation\n",
        "eval_raster_small = valor_model.evaluate_detection(\n",
        "    valor_dataset_raster,\n",
        "    filters=Filter(\n",
        "        annotations=And(\n",
        "            Label.key == \"name\",\n",
        "            Annotation.raster.area < lower_bound,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "eval_raster_small.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3dc40d",
      "metadata": {},
      "outputs": [],
      "source": [
        "bbox_df = eval_bbox_small.to_dataframe((\"annotation type\", \"bbox\"))\n",
        "raster_df = eval_raster_small.to_dataframe((\"annotation type\", \"raster\"))\n",
        "pd.concat([bbox_df, raster_df], axis=1, names=[\"bbox\", \"raster\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ece3d955",
      "metadata": {},
      "source": [
        "### Mid-sized Object Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472fa53b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# bounding box evaluation\n",
        "eval_bbox_mid = valor_model.evaluate_detection(\n",
        "    valor_dataset_bbox,\n",
        "    filters=Filter(\n",
        "        annotations=And(\n",
        "            Label.key == \"name\",\n",
        "            Annotation.bounding_box.area >= lower_bound,\n",
        "            Annotation.bounding_box.area <= upper_bound,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "eval_bbox_mid.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28aecb68",
      "metadata": {},
      "outputs": [],
      "source": [
        "# raster evaluation\n",
        "eval_raster_mid = valor_model.evaluate_detection(\n",
        "    valor_dataset_raster,\n",
        "    filters=Filter(\n",
        "        annotations=And(\n",
        "            Label.key == \"name\",\n",
        "            Annotation.raster.area >= lower_bound,\n",
        "            Annotation.raster.area <= upper_bound,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "eval_raster_mid.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1834644a",
      "metadata": {},
      "outputs": [],
      "source": [
        "bbox_df = eval_bbox_mid.to_dataframe((\"annotation type\", \"bbox\"))\n",
        "raster_df = eval_raster_mid.to_dataframe((\"annotation type\", \"raster\"))\n",
        "pd.concat([bbox_df, raster_df], axis=1, names=[\"bbox\", \"raster\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ab8fa1ad",
      "metadata": {},
      "source": [
        "### Large Object Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63247d38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# bounding box evaluation\n",
        "eval_bbox_large = valor_model.evaluate_detection(\n",
        "    valor_dataset_bbox,\n",
        "    filters=Filter(\n",
        "        annotations=And(\n",
        "            Label.key == \"name\",\n",
        "            Annotation.bounding_box.area > upper_bound,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "eval_bbox_large.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818f8147",
      "metadata": {},
      "outputs": [],
      "source": [
        "# raster evaluation\n",
        "eval_raster_large = valor_model.evaluate_detection(\n",
        "    valor_dataset_raster,\n",
        "    filters=Filter(\n",
        "        annotations=And(\n",
        "            Label.key == \"name\",\n",
        "            Annotation.raster.area > upper_bound,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "eval_raster_large.wait_for_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e20df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "bbox_df = eval_bbox_large.to_dataframe((\"annotation type\", \"bbox\"))\n",
        "raster_df = eval_raster_large.to_dataframe((\"annotation type\", \"raster\"))\n",
        "pd.concat([bbox_df, raster_df], axis=1, names=[\"bbox\", \"raster\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env-valor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
