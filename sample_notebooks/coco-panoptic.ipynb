{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d0a509-7500-44ba-b951-3566d4a4fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czaloom/velour/.env-velour/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from velour.client import Client, ClientException\n",
    "from velour.client import Dataset as VelourDataset, Model as VelourModel\n",
    "from velour.schemas import Image\n",
    "from velour.viz import combined_segmentation_mask\n",
    "from velour.integrations.coco import upload_coco_panoptic\n",
    "from velour.integrations.yolo import parse_yolo_object_detection, parse_yolo_image_segmentation\n",
    "from velour.metrics import Task\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acb1bd2-238e-4f7e-b246-dd881f020813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chariot.datasets import Dataset as ChariotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32c5d65-a49b-4440-9cd3-241501b89cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully connected to http://localhost:8000/.\n"
     ]
    }
   ],
   "source": [
    "client = Client(\"http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdc5dc-94e4-42c4-8c3c-4f75b6ce50db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset ingestion\n",
    "\n",
    "We assume the COCO panoptic evaluations from\n",
    "http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip have been downloaded and unzipped (including unzipping the file `panoptic_val2017.zip`) to `./coco`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03075a85-d075-4eff-928d-69369d042287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./coco/annotations/panoptic_val2017.json\") as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "annotations[\"annotations\"] = annotations[\"annotations\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9af91c2-f88d-4722-b988-a2da13eb042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to download underlying images from COCO\n",
    "image_id_to_coco_url = {str(img_dict[\"id\"]): img_dict[\"coco_url\"] for img_dict in annotations[\"images\"]}\n",
    "def download_image(img: Image) -> PIL.Image:\n",
    "    url = image_id_to_coco_url[img.uid]\n",
    "    img_data = BytesIO(requests.get(url).content)\n",
    "    return PIL.Image.open(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb100fb9-5cde-47a3-acb5-961b7c9bfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dataset object in velour\n",
    "try:\n",
    "    dataset = VelourDataset.create(client, \"coco2017-panoptic\")\n",
    "    upload_coco_panoptic(\n",
    "        dataset,\n",
    "        annotations=annotations,\n",
    "        masks_path=\"./coco/annotations/panoptic_val2017/\"\n",
    "    )\n",
    "except ClientException:\n",
    "    dataset = VelourDataset.get(client, \"coco2017-panoptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6e03d2-72fe-49b5-907d-cd67ba4e1dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f26aef-b5bb-4c6c-9c28-707ad5e40c76",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m download_image(dataset\u001b[39m.\u001b[39;49mget_images()[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "download_image(dataset.get_images()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d5384-14fc-4093-8fea-040d4782e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segs = dataset.get_groundtruth_instance_segmentations(\"139\")\n",
    "semantic_segs = dataset.get_groundtruth_semantic_segmentations(\"139\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e3e25-aa4a-4934-ad5f-da770bffa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_mask, instance_legend = combined_segmentation_mask(instance_segs, label_key=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febaa28-6cfd-426b-b637-437bc5594687",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd651c08-c554-4fb2-9dab-4b44679c500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in instance_legend.items():\n",
    "    print(k)\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1592a5-375a-4472-98d6-01f519ce2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_mask, semantic_legend = combined_segmentation_mask(semantic_segs, label_key=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e5396-03b8-45ad-bed6-9c9c06315da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e6751-68bb-482f-99b6-e0ee8b28c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in semantic_legend.items():\n",
    "    print(k)\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ea8d6-9916-4c9d-b913-b7e47d971f2d",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a65438-5e69-4577-9a7d-6d796fb164e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chariot.client import connect\n",
    "from chariot.models import Model\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from velour.integrations.chariot import create_model_from_chariot, parse_chariot_object_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928fc11-f025-4785-8439-a3c2fb1c69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect(\"https://production.chariot.striveworks.us/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac2e65-1277-4098-b196-0adb71ac3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.get_dataset(\"coco2017-panoptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43969bb6-102c-45fa-b6a0-0f01d840da2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"fasterrcnnresnet-50fpn\"\n",
    "chariot_model = Model(name=model_name, project_name=\"Global\")\n",
    "\n",
    "try:\n",
    "    velour_chariot_model = create_model_from_chariot(client, chariot_model)\n",
    "except ClientException:\n",
    "    velour_chariot_model = client.get_model('chariot-'+model_name+'-v'+str(chariot_model.version))\n",
    "\n",
    "\n",
    "try:\n",
    "    for image_metadata in tqdm(dataset.get_images()):\n",
    "        image = download_image(image_metadata)    \n",
    "        \n",
    "        # Chariot Inference\n",
    "        detections = chariot_model.detect(image)\n",
    "        velour_dets = parse_chariot_object_detections(detections, image_metadata, label_key=\"name\") \n",
    "\n",
    "        velour_chariot_model.add_predictions(dataset=dataset, predictions=velour_dets, show_progress_bar=False)\n",
    "except ClientException:\n",
    "    print(\"Velour Chariot model already finalized.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae341dc-44b8-47e3-b99c-486c475b4c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"yolov8n-seg\"\n",
    "yolo_model = YOLO(f\"{model_name}.pt\")\n",
    "\n",
    "try:\n",
    "    velour_yolo_model = client.create_image_model(model_name + \"\")\n",
    "except ClientException:\n",
    "    velour_yolo_model = client.get_model(model_name + '')\n",
    "\n",
    "try:\n",
    "    for image_metadata in tqdm(dataset.get_images()):\n",
    "        \n",
    "        # YOLO Inference (before)\n",
    "        image = download_image(image_metadata)\n",
    "        results = yolo_model(image, verbose=False)\n",
    "\n",
    "        velour_dets = parse_yolo_object_detection(results[0], uid=image_metadata.uid, label_key='name')\n",
    "        velour_yolo_model.add_predictions(dataset=dataset, predictions=velour_dets, show_progress_bar=False)\n",
    "\n",
    "except ClientException:\n",
    "    print(\"Velour YOLO model already finalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92993647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(velour_dets[0].scored_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datums = []\n",
    "def bulk_inference():\n",
    "    pass\n",
    "def parse_bulk_inference():\n",
    "    pass\n",
    "def convert_to_bulk_inference_format():\n",
    "    pass\n",
    "model = None\n",
    "class Datum:\n",
    "    pass\n",
    "yolo: callable = None\n",
    "parse_coco_panoptic = bulk_inference\n",
    "coco_to_velour = None\n",
    "yolo_to_velour = None\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./coco/annotations/panoptic_val2017.json\") as f:\n",
    "    datums = json.load(f)\n",
    "\n",
    "## Dataset Creation\n",
    "dataset = client.create_dataset(\"coco2017-panoptic\")        # NOTE: Dataset/Model will determine type based on the data uploaded to it. \n",
    "dataset.metadata[\"href\"] = \"some url\"\n",
    "dataset.metadata[\"description\"] = \"the coco dataset\"\n",
    "dataset.metadata[\"project\"] = \"some project id\"\n",
    "dataset.metadata[\"can be anything\"] = 123\n",
    "\n",
    "## Ground Truth\n",
    "for image in annotations:\n",
    "\n",
    "    # Create Datum\n",
    "    datum = dataset.create_datum(image.uid)\n",
    "    datum.metadata[\"type\"] = \"image\"\n",
    "    datum.metadata[\"lens_zoom\"] = 0.4\n",
    "\n",
    "    # GroundTruths (Custom)\n",
    "    gts = [                                                 # This for-loop is what a parsing integration abstracts\n",
    "        coco_to_velour(detection)                           # NOTE: Still need some form of object conversion\n",
    "        for detection in image\n",
    "    ]\n",
    "    dataset.add_groundtruths(datum, gts)\n",
    "\n",
    "dataset.finalize()\n",
    "\n",
    "## Model Creation\n",
    "model = client.create_model(\"yolov8n-seg\")\n",
    "model.metadata[\"href\"] = \"some url\"\n",
    "model.metadata[\"description\"] = \"yolo\"\n",
    "\n",
    "## Inference\n",
    "for datum in tqdm(dataset.get_images()):\n",
    "\n",
    "        # 3rd-Party Inference\n",
    "        image = download_image(datum)\n",
    "        results = yolo(image, verbose=False)\n",
    " \n",
    "        # Predictions (Custom)\n",
    "        preds = [                                           # This for-loop is what a parsing integration abstracts\n",
    "            yolo_to_velour(detection)                       # NOTE: Still need some form of object conversion\n",
    "            for detection in results                \n",
    "        ]\n",
    "        model.add_predictions(datum, preds)\n",
    "\n",
    "model.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f9282-70b7-4d5e-911a-810d7c3502e6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb60f0-516b-4a89-b159-0359285b35a6",
   "metadata": {},
   "source": [
    "for provenance, the dataset and inferences on the dataset must be finalized before evaluation can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4179d-0314-4d79-81f1-b9b309452250",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.finalize()\n",
    "velour_chariot_model.finalize_inferences(dataset)\n",
    "velour_yolo_model.finalize_inferences(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85404d81-b422-41c3-b7fc-4c6ba727eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_ap(dataset, model, max_area=None):\n",
    "    eval_job = model.evaluate_ap(\n",
    "        dataset=dataset,\n",
    "        model_pred_task_type=Task.BBOX_OBJECT_DETECTION,\n",
    "        dataset_gt_task_type=Task.INSTANCE_SEGMENTATION,\n",
    "        label_key=\"name\",\n",
    "        max_area=max_area,\n",
    "    )\n",
    "\n",
    "    while eval_job.status() == 'Processing':\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return eval_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b78220-a479-4f2f-8134-a24fc2e8b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "job1 = evaluate_ap(dataset, velour_chariot_model)\n",
    "job2 = evaluate_ap(dataset, velour_chariot_model, max_area=30*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adefc71-c2c8-42f0-b0a9-2381263ae3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job3 = evaluate_ap(dataset, velour_yolo_model)\n",
    "job4 = evaluate_ap(dataset, velour_yolo_model, max_area=30*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b0cc1-93b6-4807-a63c-9cd9e0435a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job1.metrics()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job2.metrics()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(job3.metrics()[:2])\n",
    "for m in job3.metrics():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job4.metrics()[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ade3e294",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from velour.client import Client\n",
    "from velour.data_types import Label\n",
    "client = Client(\"http://localhost:8000\")\n",
    "\n",
    "velour_coco = client.get_dataset(\"coco2017-panoptic\")\n",
    "\n",
    "model_name = \"yolov8n-seg\"\n",
    "velour_yolo = client.get_model(model_name + '')\n",
    "\n",
    "model_name = \"fasterrcnnresnet-50fpn\"\n",
    "velour_chariot = client.get_model('chariot-'+model_name+'-v'+str('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99183b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(velour_coco.name)\n",
    "print(velour_coco.description)\n",
    "print(velour_coco.href)\n",
    "info = velour_coco.get_info()\n",
    "print(info.number_of_classifications)\n",
    "print(info.number_of_bounding_boxes)\n",
    "print(info.number_of_bounding_polygons)\n",
    "print(info.number_of_segmentations)\n",
    "print(info.annotation_type)\n",
    "print(info.associated_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(velour_yolo.name)\n",
    "print(velour_yolo.description)\n",
    "print(velour_yolo.href)\n",
    "info = velour_yolo.get_info()\n",
    "print(info.number_of_classifications)\n",
    "print(info.number_of_bounding_boxes)\n",
    "print(info.number_of_bounding_polygons)\n",
    "print(info.number_of_segmentations)\n",
    "print(info.annotation_type)\n",
    "print(info.associated_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(velour_chariot.name)\n",
    "print(velour_chariot.description)\n",
    "print(velour_chariot.href)\n",
    "info = velour_chariot.get_info()\n",
    "print(info.number_of_classifications)\n",
    "print(info.number_of_bounding_boxes)\n",
    "print(info.number_of_bounding_polygons)\n",
    "print(info.number_of_segmentations)\n",
    "print(info.annotation_type)\n",
    "print(info.associated_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = velour_coco.get_label_distribution()\n",
    "\n",
    "label_person = Label(key='name', value='person')\n",
    "label = Label(key='supercategory', value='furniture')\n",
    "\n",
    "count = distribution[label_person]\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ed238",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = velour_yolo.get_label_distribution()\n",
    "\n",
    "label = Label(key='name', value='tv')\n",
    "\n",
    "info = distribution[label]\n",
    "\n",
    "print(info['count'])\n",
    "print(info['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = velour_coco.get_labels()\n",
    "\n",
    "for label in labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = velour_yolo.get_labels()\n",
    "\n",
    "for label in labels:\n",
    "    print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-velour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
