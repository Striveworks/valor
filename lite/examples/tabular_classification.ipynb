{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0644bdbe-38da-478e-8673-802a5cb59da0",
   "metadata": {},
   "source": [
    "# Evaluating Tabular Classifications\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll walk-through a detailed example of how you can use Valor to evaluate classifications made on a tabular dataset. This example uses `sklearn`'s breast cancer dataset to make a binary prediction about whether a woman has breast cancer, based on a table of descriptive features, such as mean radius and mean texture.\n",
    "\n",
    "For a conceptual introduction to Valor, [check out our project overview](https://striveworks.github.io/valor/). For a higher-level example notebook, [check out our \"Getting Started\" notebook](https://github.com/Striveworks/valor/blob/main/examples/getting_started.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18d9c3",
   "metadata": {},
   "source": [
    "## Defining Our Datasets\n",
    "\n",
    "We start by fetching our dataset, dividing it into test/train splits, and uploading both sets to Valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9daebe8-0bb4-41eb-8359-9cadaa4a7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from valor_lite.classification import DataLoader, Classification, MetricType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c72cd1-50f7-4e85-9e25-d0ed35b1d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from sklearn\n",
    "dset = load_breast_cancer()\n",
    "dset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a8d343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), array([1, 1, 0, 1]), array(['malignant', 'benign'], dtype='<U9'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split datasets\n",
    "X, y, target_names = dset[\"data\"], dset[\"target\"], dset[\"target_names\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# show an example input\n",
    "X_train.shape, y_train[:4], target_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e85ac3c",
   "metadata": {},
   "source": [
    "### Create GroundTruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5311dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format training groundtruths\n",
    "training_groundtruths = [\n",
    "    (f\"train{i}\", (\"class\", target_names[t]))\n",
    "    for i, t in enumerate(y_train)\n",
    "]\n",
    "\n",
    "# format testing groundtruths\n",
    "testing_groundtruths = [\n",
    "    (f\"test{i}\", (\"class\", target_names[t]))\n",
    "    for i, t in enumerate(y_test)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea0e42",
   "metadata": {},
   "source": [
    "## Defining Our Model\n",
    "\n",
    "Now that our `Datasets` have been defined, we can describe our model in Valor using the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f43e61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03939074, 0.96060926],\n",
       "       [0.00546861, 0.99453139],\n",
       "       [0.88274567, 0.11725433],\n",
       "       [0.079975  , 0.920025  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit an sklearn model to our data\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# get predictions on both of our datasets\n",
    "y_train_probs = pipe.predict_proba(X_train)\n",
    "y_test_probs = pipe.predict_proba(X_test)\n",
    "\n",
    "# show an example output\n",
    "y_train_probs[:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61de269b",
   "metadata": {},
   "source": [
    "### Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a224345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define our predictions\n",
    "training_predictions = [\n",
    "    (\n",
    "        f\"train{i}\",\n",
    "        [\n",
    "            ((\"class\", target_names[j]), p)\n",
    "            for j, p in enumerate(prob)\n",
    "        ],\n",
    "    )\n",
    "    for i, prob in enumerate(y_train_probs)\n",
    "]\n",
    "testing_predictions = [\n",
    "    (\n",
    "        f\"test{i}\",\n",
    "        [\n",
    "            ((\"class\", target_names[j]), p)\n",
    "            for j, p in enumerate(prob)\n",
    "        ],\n",
    "    )\n",
    "    for i, prob in enumerate(y_test_probs)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f3c2f5",
   "metadata": {},
   "source": [
    "## Create Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22eed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_clfs = [\n",
    "    Classification(\n",
    "        uid=gt[0],\n",
    "        groundtruths=[gt[1]],\n",
    "        predictions=[\n",
    "            plabel\n",
    "            for plabel, _ in pd[1]\n",
    "        ],\n",
    "        scores=[\n",
    "            score\n",
    "            for _, score in pd[1]\n",
    "        ]\n",
    "    )\n",
    "    for gt, pd in zip(training_groundtruths, training_predictions)\n",
    "    if gt[0] == pd[0]\n",
    "]\n",
    "\n",
    "testing_clfs = [\n",
    "    Classification(\n",
    "        uid=gt[0],\n",
    "        groundtruths=[gt[1]],\n",
    "        predictions=[\n",
    "            plabel\n",
    "            for plabel, _ in pd[1]\n",
    "        ],\n",
    "        scores=[\n",
    "            score\n",
    "            for _, score in pd[1]\n",
    "        ]\n",
    "    )\n",
    "    for gt, pd in zip(testing_groundtruths, testing_predictions)\n",
    "    if gt[0] == pd[0]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b78f442",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "With our `Dataset` and `Model` defined, we're ready to evaluate our performance and display the results. Note that we use the `wait_for_completion` method since all evaluations run as background tasks; this method ensures that the evaluation finishes before we display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d34560",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader()\n",
    "training_loader.add_data(training_clfs)\n",
    "training_evaluator = training_loader.finalize()\n",
    "\n",
    "testing_loader = DataLoader()\n",
    "testing_loader.add_data(testing_clfs)\n",
    "testing_evaluator = testing_loader.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba0e545-4eaa-4f6b-8d62-f3a63018e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = testing_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e447e7-0da4-49ae-af0a-8baa1446b4e7",
   "metadata": {},
   "source": [
    "As a brief sanity check, we can check Valor's outputs against `sklearn's` own classification report. We see that the two results are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17eeea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Precision(value=[0.0], score_thresholds=[0.5], label=('class', 'malignant')),\n",
       " Precision(value=[0.0], score_thresholds=[0.5], label=('class', 'benign'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[MetricType.Precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880aa6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Recall(value=[1.0], score_thresholds=[0.5], label=('class', 'malignant')),\n",
       " Recall(value=[0.9789473684210527], score_thresholds=[0.5], label=('class', 'benign'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[MetricType.Recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7015afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[F1(value=[0.0], score_thresholds=[0.5], label=('class', 'malignant')),\n",
       " F1(value=[0.0], score_thresholds=[0.5], label=('class', 'benign'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[MetricType.F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347c180e-9913-4aa4-994e-de507da32d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant   0.993711  0.975309  0.984424       162\n",
      "      benign   0.985019  0.996212  0.990584       264\n",
      "\n",
      "    accuracy                       0.988263       426\n",
      "   macro avg   0.989365  0.985760  0.987504       426\n",
      "weighted avg   0.988324  0.988263  0.988241       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_preds = pipe.predict(X_train)\n",
    "print(classification_report(y_train, y_train_preds, digits=6, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-valor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
